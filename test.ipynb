{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Cargando el archivo CSV\n",
    "file_path = '/home/gero/Downloads/Sonar.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Separando las características y las etiquetas\n",
    "X = data.iloc[:, :-1]  # Todas las filas, todas las columnas excepto la última\n",
    "y = data.iloc[:, -1]  # Todas las filas, solo la última columna\n",
    "\n",
    "# Normalizando los datos\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Creando y entrenando el modelo\n",
    "clf = MLPClassifier(solver='sgd', learning_rate_init=0.1, hidden_layer_sizes=(10,),\n",
    "                    max_iter=4000, verbose=False, tol=1e-06, activation='tanh', random_state=42)\n",
    "clf.fit(X_normalized, y)\n",
    "\n",
    "# Evaluando el modelo\n",
    "y_pred = clf.predict(X_normalized)\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 15:51:24.948551: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-03 15:51:25.111930: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-03 15:51:25.727349: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de parámetros: 4301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 15:51:26.533135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-03 15:51:26.624190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-03 15:51:26.624289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-03 15:51:26.625886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-03 15:51:26.625961: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-03 15:51:26.626015: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-03 15:51:27.064902: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-03 15:51:27.065008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-03 15:51:27.065068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-03 15:51:27.065124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3245 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Carga el modelo\n",
    "model = load_model('/home/gero/Downloads/mrPrecio.h5')\n",
    "\n",
    "# Obtiene el número total de parámetros\n",
    "total_parameters = model.count_params()\n",
    "print(\"Número total de parámetros:\", total_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "NAs en las características: 2\n",
      "NAs en las etiquetas: 4\n",
      "NAs en las características: 0\n",
      "NAs en las etiquetas: 0\n",
      "7/7 [==============================] - 0s 857us/step\n",
      "Coeficiente de determinación R^2: -2.831077986119392\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Cargar el modelo\n",
    "model = load_model('/home/gero/Downloads/mrPrecio.h5')\n",
    "\n",
    "# Cargar los datos\n",
    "datos = pd.read_csv('/home/gero/Downloads/automobile-simple.csv')\n",
    "\n",
    "features = [\"curb-weight\", \"engine-size\", \"horsepower\", \"city-mpg\", \"highway-mpg\", \"volume\", \"eco-rating\"]\n",
    "# Separar las características y las etiquetas\n",
    "X = datos[features].copy()  # Ajusta el nombre de la columna según tus datos\n",
    "y_real = datos['price']\n",
    "\n",
    "print(f\"NAs en las características: {X.isna().sum().sum()}\")\n",
    "print(f\"NAs en las etiquetas: {y_real.isna().sum().sum()}\")\n",
    "\n",
    "# Completar valores faltantes con el valor promedio\n",
    "X = X.fillna(X.mean())\n",
    "y_real = y_real.fillna(y_real.mean())\n",
    "\n",
    "print(f\"NAs en las características: {X.isna().sum().sum()}\")\n",
    "print(f\"NAs en las etiquetas: {y_real.isna().sum().sum()}\")\n",
    "\n",
    "# Normalizar los datos con MinMaxScaler para que estén en el rango [-1, 1]\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Realizar las predicciones\n",
    "y_pred = model.predict(X_normalized)\n",
    "\n",
    "# Calcular el coeficiente R^2\n",
    "r2 = r2_score(y_real, y_pred)\n",
    "print(\"Coeficiente de determinación R^2:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multi_class must be in ('ovo', 'ovr')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/gero/Documents/Austral/Laboratorio de Implementación 3/consultoria-multinacional/test.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gero/Documents/Austral/Laboratorio%20de%20Implementaci%C3%B3n%203/consultoria-multinacional/test.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m)  \u001b[39m# El clasificador siempre predice la clase mayoritaria\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gero/Documents/Austral/Laboratorio%20de%20Implementaci%C3%B3n%203/consultoria-multinacional/test.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Calculando el AUC-ROC\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gero/Documents/Austral/Laboratorio%20de%20Implementaci%C3%B3n%203/consultoria-multinacional/test.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m auc_roc \u001b[39m=\u001b[39m roc_auc_score(y_real, y_pred)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gero/Documents/Austral/Laboratorio%20de%20Implementaci%C3%B3n%203/consultoria-multinacional/test.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m auc_roc\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    215\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:620\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    614\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPartial AUC computation not available in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    615\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmulticlass setting, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_fpr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    616\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m set to `None`, received `max_fpr=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    617\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minstead\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(max_fpr)\n\u001b[1;32m    618\u001b[0m         )\n\u001b[1;32m    619\u001b[0m     \u001b[39mif\u001b[39;00m multi_class \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 620\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmulti_class must be in (\u001b[39m\u001b[39m'\u001b[39m\u001b[39movo\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39movr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    621\u001b[0m     \u001b[39mreturn\u001b[39;00m _multiclass_roc_auc_score(\n\u001b[1;32m    622\u001b[0m         y_true, y_score, labels, multi_class, average, sample_weight\n\u001b[1;32m    623\u001b[0m     )\n\u001b[1;32m    624\u001b[0m \u001b[39melif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: multi_class must be in ('ovo', 'ovr')"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Ejemplo de datos\n",
    "# 75 filas de la clase mayoritaria (1) y 25 de la clase minoritaria (0)\n",
    "y_real = np.array([1]*50 + [0]*25 + [2]*25)\n",
    "y_pred = np.array([1]*100)  # El clasificador siempre predice la clase mayoritaria\n",
    "\n",
    "# Calculando el AUC-ROC\n",
    "auc_roc = roc_auc_score(y_real, y_pred)\n",
    "auc_roc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{}\n",
      "()\n",
      "{'axis': None}\n",
      "(array([41,  0,  0,  0,  0,  0,  0,  4,  4,  0,  2,  2,  0,  0,  4,  0]),)\n",
      "{}\n",
      "()\n",
      "{'axis': None}\n",
      "(array([0, 0, 0, 2, 0, 0, 0, 0, 0, 0]),)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LeakyReLU\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "data = pd.read_csv('/home/gero/Downloads/AUTOS.csv')\n",
    "\n",
    "numeric_cols = [\"normalized-losses\", \"wheel-base\", \"length\", \"width\", \"height\", \"curb-weight\", \"engine-size\",\n",
    "                \"bore\", \"stroke\", \"compression-ratio\", \"horsepower\", \"peak-rpm\", \"city-mpg\", \"highway-mpg\", \"price\", \"symboling\"]\n",
    "non_numeric_cols = [\"make\", \"fuel-type\", \"aspiration\", \"num-of-doors\", \"body-style\", \"drive-wheels\", \"engine-location\", \"engine-type\", \"num-of-cylinders\", \"fuel-system\"]\n",
    "\n",
    "# Completar los valores faltantes de las columnas numéricas con el valor promedio\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data[numeric_cols] = imputer.fit_transform(data[numeric_cols])\n",
    "\n",
    "# Completar los valores faltantes de las columnas no numéricas con el valor más frecuente\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "data[non_numeric_cols] = imputer.fit_transform(data[non_numeric_cols])\n",
    "\n",
    "# Convertir las columnas no numéricas a valores numéricos\n",
    "data = pd.get_dummies(data, columns=non_numeric_cols, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dividir el dataset en entrenamiento y validación\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Separar las características (X) y las etiquetas (y)\n",
    "X_train = train_data.drop(['price', 'highway-mpg'], axis=1)\n",
    "y_train_price = train_data['price']\n",
    "y_train_mpg = train_data['highway-mpg']\n",
    "\n",
    "X_val = val_data.drop(['price', 'highway-mpg'], axis=1)\n",
    "y_val_price = val_data['price']\n",
    "y_val_mpg = val_data['highway-mpg']\n",
    "\n",
    "# Normalización de las características\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Configuraciones de modelos\n",
    "optimizers = [SGD, RMSprop, Adam]\n",
    "activations = ['tanh', 'sigmoid', 'relu', LeakyReLU(alpha=0.1)]\n",
    "n_runs = 20\n",
    "\n",
    "r2_scores_price = []\n",
    "r2_scores_mpg = []\n",
    "epoch_counts = []\n",
    "\n",
    "for optimizer_class in optimizers:\n",
    "    for activation in activations:\n",
    "        r2_price_sum = 0\n",
    "        r2_mpg_sum = 0\n",
    "        epoch_sum = 0\n",
    "\n",
    "        for _ in range(n_runs):\n",
    "            # Crear el modelo MLP\n",
    "            model = Sequential()\n",
    "            model.add(Dense(64, input_dim=X_train.shape[1], activation=activation))\n",
    "            model.add(Dense(32, activation=activation))\n",
    "            model.add(Dense(1))  # Una neurona para la regresión\n",
    "            optimizer = optimizer_class()\n",
    "            model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "            # Entrenar el modelo\n",
    "            X_train_shuffled, y_train_price_shuffled, y_train_mpg_shuffled = shuffle(X_train, y_train_price, y_train_mpg, random_state=42)\n",
    "            history = model.fit(X_train_shuffled, y_train_price_shuffled, epochs=1000, validation_data=(X_val, y_val_price), verbose=0)\n",
    "\n",
    "            # Realizar predicciones en el conjunto de validación\n",
    "            y_val_price_pred = model.predict(X_val)\n",
    "\n",
    "            # Calcular R^2 para el atributo 'price'\n",
    "            r2_price = r2_score(y_val_price, y_val_price_pred)\n",
    "            r2_price_sum += r2_price\n",
    "\n",
    "            # Obtener el número de épocas utilizadas\n",
    "            epoch_sum += len(history.history['loss'])\n",
    "\n",
    "        # Calcular el promedio de R^2 y épocas para esta configuración\n",
    "        avg_r2_price = r2_price_sum / n_runs\n",
    "        avg_epochs = epoch_sum / n_runs\n",
    "\n",
    "        r2_scores_price.append(avg_r2_price)\n",
    "        epoch_counts.append(avg_epochs)\n",
    "\n",
    "# Calcular el promedio de R^2 y épocas para todas las configuraciones\n",
    "avg_r2_price_all = sum(r2_scores_price) / len(r2_scores_price)\n",
    "avg_epochs_all = sum(epoch_counts) / len(epoch_counts)\n",
    "\n",
    "print(f\"R^2 promedio para 'price': {avg_r2_price_all}\")\n",
    "print(f\"Promedio de épocas: {avg_epochs_all}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "Sin padding:\n",
      " [[ 0  1  0 -1  0]\n",
      " [-1  0  0  0  1]\n",
      " [-2 -1  0  1  2]\n",
      " [-1  0  0  0  1]\n",
      " [ 0  1  0 -1  0]]\n",
      "\n",
      "Con padding:\n",
      " [[-1  1  1  0 -1 -1  1]\n",
      " [-1  0  1  0 -1  0  1]\n",
      " [-1 -1  0  0  0  1  1]\n",
      " [ 0 -2 -1  0  1  2  0]\n",
      " [-1 -1  0  0  0  1  1]\n",
      " [-1  0  1  0 -1  0  1]\n",
      " [-1  1  1  0 -1 -1  1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "# Definiendo la matriz de entrada y el kernel\n",
    "ENTRADA = np.array(\n",
    "    [\n",
    "        [1, 0, 0, 0, 0, 0, 1],\n",
    "        [0, 1, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 1, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 1, 0, 1, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 1, 0],\n",
    "        [1, 0, 0, 0, 0, 0, 1],\n",
    "    ]\n",
    ")\n",
    "KERNEL = np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n",
    "\n",
    "# Realizando la convolución sin padding\n",
    "output_without_padding = convolve2d(ENTRADA, KERNEL, mode=\"valid\")\n",
    "\n",
    "# Realizando la convolución con padding\n",
    "padded_input = np.pad(ENTRADA, pad_width=1, mode=\"constant\", constant_values=0)\n",
    "output_with_padding = convolve2d(padded_input, KERNEL, mode=\"valid\")\n",
    "\n",
    "print(\"Sin padding:\\n\", output_without_padding)\n",
    "print(\"\\nCon padding:\\n\", output_with_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: (array([[ 1., -2.,  0.,  0.,  0., -2.,  1.],\n",
       "         [-2.,  1., -2.,  0., -2.,  1., -2.],\n",
       "         [ 0., -2.,  1., -3.,  1., -2.,  0.],\n",
       "         [ 0.,  0., -3.,  1., -3.,  0.,  0.],\n",
       "         [ 0., -2.,  1., -3.,  1., -2.,  0.],\n",
       "         [-2.,  1., -2.,  0., -2.,  1., -2.],\n",
       "         [ 1., -2.,  0.,  0.,  0., -2.,  1.]]),\n",
       "  array([[1., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 1.]])),\n",
       " 2: (array([[1., 0., 0., 1.],\n",
       "         [0., 1., 1., 0.],\n",
       "         [0., 1., 1., 0.],\n",
       "         [1., 0., 0., 1.]]),\n",
       "  array([[1., 0., 0., 1.],\n",
       "         [0., 1., 1., 0.],\n",
       "         [0., 1., 1., 0.],\n",
       "         [1., 0., 0., 1.]])),\n",
       " 3: (array([[1., 0., 1.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 0., 1.]]),\n",
       "  array([[1., 0., 1.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 0., 1.]]))}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convolve_with_stride_and_padding(input_matrix, kernel, stride, padding):\n",
    "    # Aplicando padding\n",
    "    padded_matrix = np.pad(input_matrix, padding, mode=\"constant\", constant_values=0)\n",
    "\n",
    "    # Dimensiones de la matriz de entrada y del kernel\n",
    "    input_shape = padded_matrix.shape\n",
    "    kernel_shape = kernel.shape\n",
    "\n",
    "    # Dimensiones de la salida\n",
    "    output_shape = (\n",
    "        (input_shape[0] - kernel_shape[0]) // stride + 1,\n",
    "        (input_shape[1] - kernel_shape[1]) // stride + 1,\n",
    "    )\n",
    "\n",
    "    # Matriz de salida\n",
    "    output = np.zeros(output_shape)\n",
    "\n",
    "    # Realizando la convolución\n",
    "    for x in range(0, input_shape[0] - kernel_shape[0] + 1, stride):\n",
    "        for y in range(0, input_shape[1] - kernel_shape[1] + 1, stride):\n",
    "            output[x // stride, y // stride] = np.sum(\n",
    "                padded_matrix[x : x + kernel_shape[0], y : y + kernel_shape[1]] * kernel\n",
    "            )\n",
    "    return output\n",
    "\n",
    "\n",
    "# Definiendo la matriz de entrada y los kernels\n",
    "ENTRADA = np.array(\n",
    "    [\n",
    "        [1, 0, 0, 0, 0, 0, 1],\n",
    "        [0, 1, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 1, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 1, 0, 1, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 1, 0],\n",
    "        [1, 0, 0, 0, 0, 0, 1],\n",
    "    ]\n",
    ")\n",
    "KERNEL1 = np.array([[0, -1, 0], [-1, 1, -1], [0, -1, 0]])\n",
    "KERNEL2 = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]])\n",
    "\n",
    "# Realizando la convolución para diferentes valores de stride\n",
    "results = {}\n",
    "for stride in range(1, 4):\n",
    "    result_kernel1 = convolve_with_stride_and_padding(ENTRADA, KERNEL1, stride, 1)\n",
    "    result_kernel2 = convolve_with_stride_and_padding(ENTRADA, KERNEL2, stride, 1)\n",
    "    results[stride] = (result_kernel1, result_kernel2)\n",
    "\n",
    "# Compara los resultados de results para ver si son iguales para algún stride\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 16:31:40.995327: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-04 16:31:41.021282: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-04 16:31:41.498121: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros en Modelo 1: 16133\n",
      "Parámetros en Modelo 2: 16133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 16:31:41.992017: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2023-12-04 16:31:41.992034: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: Legion-Gero\n",
      "2023-12-04 16:31:41.992037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: Legion-Gero\n",
      "2023-12-04 16:31:41.992077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 525.147.5\n",
      "2023-12-04 16:31:41.992084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 525.147.5\n",
      "2023-12-04 16:31:41.992085: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 525.147.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPooling2D\n",
    "\n",
    "# Configuración 1\n",
    "modelo1 = Sequential([\n",
    "    Conv2D(10, kernel_size=(5, 5), strides=(2, 2), activation='relu', input_shape=(50, 50, 1)),\n",
    "    Flatten(),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Configuración 2\n",
    "modelo2 = Sequential([\n",
    "    Conv2D(10, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=(50, 50, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Contando el número de parámetros en cada modelo\n",
    "parametros_modelo1 = modelo1.count_params()\n",
    "parametros_modelo2 = modelo2.count_params()\n",
    "\n",
    "print(\"Parámetros en Modelo 1:\", parametros_modelo1)\n",
    "print(\"Parámetros en Modelo 2:\", parametros_modelo2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "consultoria-multinacional-yH41UiuE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
