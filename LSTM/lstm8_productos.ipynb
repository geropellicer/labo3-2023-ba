{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usandor Tensorflow version 2.13.1\n",
      "Usando CPU.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Usandor Tensorflow version \" + tf.__version__)\n",
    "\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "  print('Usando GPU: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "  print(\"Usando CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_LAYERS = [1]\n",
    "NUMBER_OF_NEURONS = [[50,50,50,50]]\n",
    "NUMBER_OF_STEPS = [1,4,6,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "import pandas as pd\n",
    "from empresa4.datasets import get_dataset, nombres_datasets\n",
    "from keras.callbacks import EarlyStopping\n",
    "from datetime import datetime\n",
    "from empresa4.core import calculate_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split a single time series into overlapping sequences\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix - 1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>product_id</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>product_category</th>\n",
       "      <th>cat2</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>tn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201702</td>\n",
       "      <td>20001</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201703</td>\n",
       "      <td>20001</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201704</td>\n",
       "      <td>20001</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201705</td>\n",
       "      <td>20001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>201908</td>\n",
       "      <td>20005</td>\n",
       "      <td>98.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>201909</td>\n",
       "      <td>20005</td>\n",
       "      <td>101.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>111.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>201910</td>\n",
       "      <td>20005</td>\n",
       "      <td>104.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>201911</td>\n",
       "      <td>20005</td>\n",
       "      <td>107.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>117.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>201912</td>\n",
       "      <td>20005</td>\n",
       "      <td>110.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     periodo  product_id  cust_request_qty  cust_request_tn product_category  \\\n",
       "0     201701       20001               1.0              2.0            FOODS   \n",
       "1     201702       20001               2.0              3.0            FOODS   \n",
       "2     201703       20001               3.0              4.0            FOODS   \n",
       "3     201704       20001               4.0              5.0            FOODS   \n",
       "4     201705       20001               5.0              6.0            FOODS   \n",
       "..       ...         ...               ...              ...              ...   \n",
       "175   201908       20005              98.0            103.0               HC   \n",
       "176   201909       20005             101.0            106.0               HC   \n",
       "177   201910       20005             104.0            109.0               HC   \n",
       "178   201911       20005             107.0            112.0               HC   \n",
       "179   201912       20005             110.0            115.0               HC   \n",
       "\n",
       "            cat2  sku_size  plan_precios_cuidados      tn  \n",
       "0       ADEREZOS       3.0                      1     6.0  \n",
       "1       ADEREZOS       4.0                      1     8.0  \n",
       "2       ADEREZOS       5.0                      1    10.0  \n",
       "3       ADEREZOS       6.0                      1    12.0  \n",
       "4       ADEREZOS       7.0                      1    14.0  \n",
       "..           ...       ...                    ...     ...  \n",
       "175  ROPA LAVADO     108.0                      1  1080.0  \n",
       "176  ROPA LAVADO     111.0                      1  1110.0  \n",
       "177  ROPA LAVADO     114.0                      1  1140.0  \n",
       "178  ROPA LAVADO     117.0                      1  1170.0  \n",
       "179  ROPA LAVADO     120.0                      1  1200.0  \n",
       "\n",
       "[180 rows x 9 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "# df = get_dataset(\"02_productos_todos_anti_leak\")\n",
    "df = pd.read_csv(\"./fake.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>product_id</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>product_category</th>\n",
       "      <th>cat2</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>tn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201702</td>\n",
       "      <td>20001</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201703</td>\n",
       "      <td>20001</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201704</td>\n",
       "      <td>20001</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201705</td>\n",
       "      <td>20001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>201706</td>\n",
       "      <td>20001</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>201707</td>\n",
       "      <td>20001</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>201708</td>\n",
       "      <td>20001</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>201709</td>\n",
       "      <td>20001</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>201710</td>\n",
       "      <td>20001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>201711</td>\n",
       "      <td>20001</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>201712</td>\n",
       "      <td>20001</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>201801</td>\n",
       "      <td>20001</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>201802</td>\n",
       "      <td>20001</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>201803</td>\n",
       "      <td>20001</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>201804</td>\n",
       "      <td>20001</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>201805</td>\n",
       "      <td>20001</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>201806</td>\n",
       "      <td>20001</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>201807</td>\n",
       "      <td>20001</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>201808</td>\n",
       "      <td>20001</td>\n",
       "      <td>20.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>201809</td>\n",
       "      <td>20001</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>201810</td>\n",
       "      <td>20001</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>201811</td>\n",
       "      <td>20001</td>\n",
       "      <td>23.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>201812</td>\n",
       "      <td>20001</td>\n",
       "      <td>24.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>201901</td>\n",
       "      <td>20001</td>\n",
       "      <td>25.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>201902</td>\n",
       "      <td>20001</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>201903</td>\n",
       "      <td>20001</td>\n",
       "      <td>27.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>201904</td>\n",
       "      <td>20001</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>201905</td>\n",
       "      <td>20001</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>201906</td>\n",
       "      <td>20001</td>\n",
       "      <td>30.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>201907</td>\n",
       "      <td>20001</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>201908</td>\n",
       "      <td>20001</td>\n",
       "      <td>32.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>201909</td>\n",
       "      <td>20001</td>\n",
       "      <td>33.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>201910</td>\n",
       "      <td>20001</td>\n",
       "      <td>34.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>201911</td>\n",
       "      <td>20001</td>\n",
       "      <td>35.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>201912</td>\n",
       "      <td>20001</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    periodo  product_id  cust_request_qty  cust_request_tn product_category  \\\n",
       "0    201701       20001               1.0              2.0            FOODS   \n",
       "1    201702       20001               2.0              3.0            FOODS   \n",
       "2    201703       20001               3.0              4.0            FOODS   \n",
       "3    201704       20001               4.0              5.0            FOODS   \n",
       "4    201705       20001               5.0              6.0            FOODS   \n",
       "5    201706       20001               6.0              7.0            FOODS   \n",
       "6    201707       20001               7.0              8.0            FOODS   \n",
       "7    201708       20001               8.0              9.0            FOODS   \n",
       "8    201709       20001               9.0             10.0            FOODS   \n",
       "9    201710       20001              10.0             11.0            FOODS   \n",
       "10   201711       20001              11.0             12.0            FOODS   \n",
       "11   201712       20001              12.0             13.0            FOODS   \n",
       "12   201801       20001              13.0             14.0            FOODS   \n",
       "13   201802       20001              14.0             15.0            FOODS   \n",
       "14   201803       20001              15.0             16.0            FOODS   \n",
       "15   201804       20001              16.0             17.0            FOODS   \n",
       "16   201805       20001              17.0             18.0            FOODS   \n",
       "17   201806       20001              18.0             19.0            FOODS   \n",
       "18   201807       20001              19.0             20.0            FOODS   \n",
       "19   201808       20001              20.0             21.0            FOODS   \n",
       "20   201809       20001              21.0             22.0            FOODS   \n",
       "21   201810       20001              22.0             23.0            FOODS   \n",
       "22   201811       20001              23.0             24.0            FOODS   \n",
       "23   201812       20001              24.0             25.0            FOODS   \n",
       "24   201901       20001              25.0             26.0            FOODS   \n",
       "25   201902       20001              26.0             27.0            FOODS   \n",
       "26   201903       20001              27.0             28.0            FOODS   \n",
       "27   201904       20001              28.0             29.0            FOODS   \n",
       "28   201905       20001              29.0             30.0            FOODS   \n",
       "29   201906       20001              30.0             31.0            FOODS   \n",
       "30   201907       20001              31.0             32.0            FOODS   \n",
       "31   201908       20001              32.0             33.0            FOODS   \n",
       "32   201909       20001              33.0             34.0            FOODS   \n",
       "33   201910       20001              34.0             35.0            FOODS   \n",
       "34   201911       20001              35.0             36.0            FOODS   \n",
       "35   201912       20001              36.0             37.0            FOODS   \n",
       "\n",
       "        cat2  sku_size  plan_precios_cuidados    tn  \n",
       "0   ADEREZOS       3.0                      1   6.0  \n",
       "1   ADEREZOS       4.0                      1   8.0  \n",
       "2   ADEREZOS       5.0                      1  10.0  \n",
       "3   ADEREZOS       6.0                      1  12.0  \n",
       "4   ADEREZOS       7.0                      1  14.0  \n",
       "5   ADEREZOS       8.0                      1  16.0  \n",
       "6   ADEREZOS       9.0                      1  18.0  \n",
       "7   ADEREZOS      10.0                      1  20.0  \n",
       "8   ADEREZOS      11.0                      1  22.0  \n",
       "9   ADEREZOS      12.0                      1  24.0  \n",
       "10  ADEREZOS      13.0                      1  26.0  \n",
       "11  ADEREZOS      14.0                      1  28.0  \n",
       "12  ADEREZOS      15.0                      1  30.0  \n",
       "13  ADEREZOS      16.0                      1  32.0  \n",
       "14  ADEREZOS      17.0                      1  34.0  \n",
       "15  ADEREZOS      18.0                      1  36.0  \n",
       "16  ADEREZOS      19.0                      1  38.0  \n",
       "17  ADEREZOS      20.0                      1  40.0  \n",
       "18  ADEREZOS      21.0                      1  42.0  \n",
       "19  ADEREZOS      22.0                      1  44.0  \n",
       "20  ADEREZOS      23.0                      1  46.0  \n",
       "21  ADEREZOS      24.0                      1  48.0  \n",
       "22  ADEREZOS      25.0                      1  50.0  \n",
       "23  ADEREZOS      26.0                      1  52.0  \n",
       "24  ADEREZOS      27.0                      1  54.0  \n",
       "25  ADEREZOS      28.0                      1  56.0  \n",
       "26  ADEREZOS      29.0                      1  58.0  \n",
       "27  ADEREZOS      30.0                      1  60.0  \n",
       "28  ADEREZOS      31.0                      1  62.0  \n",
       "29  ADEREZOS      32.0                      1  64.0  \n",
       "30  ADEREZOS      33.0                      1  66.0  \n",
       "31  ADEREZOS      34.0                      1  68.0  \n",
       "32  ADEREZOS      35.0                      1  70.0  \n",
       "33  ADEREZOS      36.0                      1  72.0  \n",
       "34  ADEREZOS      37.0                      1  74.0  \n",
       "35  ADEREZOS      38.0                      1  76.0  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"product_id\"] == 20001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "equiv = {\n",
    "    201701: 201703,\n",
    "    201702: 201704,\n",
    "    201703: 201705,\n",
    "    201704: 201706,\n",
    "    201705: 201707,\n",
    "    201706: 201708,\n",
    "    201707: 201709,\n",
    "    201708: 201710,\n",
    "    201709: 201711,\n",
    "    201710: 201712,\n",
    "    201711: 201801,\n",
    "    201712: 201802,\n",
    "    201801: 201803,\n",
    "    201802: 201804,\n",
    "    201803: 201805,\n",
    "    201804: 201806,\n",
    "    201805: 201807,\n",
    "    201806: 201808,\n",
    "    201807: 201809,\n",
    "    201808: 201810,\n",
    "    201809: 201811,\n",
    "    201810: 201812,\n",
    "    201811: 201901,\n",
    "    201812: 201902,\n",
    "    201901: 201903,\n",
    "    201902: 201904,\n",
    "    201903: 201905,\n",
    "    201904: 201906,\n",
    "    201905: 201907,\n",
    "    201906: 201908,\n",
    "    201907: 201909,\n",
    "    201908: 201910,\n",
    "    201909: 201911,\n",
    "    201910: 201912,\n",
    "    201911: 202001,\n",
    "    201912: 202002,\n",
    "    202001: 202003,\n",
    "    202002: 202004,\n",
    "}\n",
    "target_df = get_dataset(\"02_productos_todos\")\n",
    "\n",
    "\n",
    "def lag_target_class(row):\n",
    "    # from the column \"periodo\" and \"product_id\" of this row, get the equivalen periodo in equiv and get the tn column from \"target\" df for this product_id and the equiv periodo\n",
    "    product_id = row[\"product_id\"]\n",
    "    periodo = row[\"periodo\"]\n",
    "    periodo_equiv = equiv.get(periodo)\n",
    "    if periodo_equiv is None:\n",
    "        return None\n",
    "    value = target_df[(target_df[\"product_id\"] == product_id) & (target_df[\"periodo\"] == periodo_equiv)][\"tn\"].values[0]\n",
    "    return value\n",
    "\n",
    "\n",
    "def lag_target_class_2(row):\n",
    "    # from the column \"periodo\" and \"product_id\" of this row, get the equivalen periodo in equiv and get the tn column from \"target\" df for this product_id and the equiv periodo\n",
    "    product_id = row[\"product_id\"]\n",
    "    periodo = row[\"periodo\"]\n",
    "    periodo_equiv = equiv.get(periodo)\n",
    "    if periodo_equiv is None:\n",
    "        return None\n",
    "    if periodo not in [201911, 201912]:\n",
    "        value = df[(df[\"product_id\"] == product_id) & (df[\"periodo\"] == periodo_equiv)][\"tn\"].values[0]\n",
    "    else:\n",
    "        value = None\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>product_id</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>product_category</th>\n",
       "      <th>cat2</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>tn</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201702</td>\n",
       "      <td>20001</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201703</td>\n",
       "      <td>20001</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201704</td>\n",
       "      <td>20001</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201705</td>\n",
       "      <td>20001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>201908</td>\n",
       "      <td>20005</td>\n",
       "      <td>98.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>201909</td>\n",
       "      <td>20005</td>\n",
       "      <td>101.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>111.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>1170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>201910</td>\n",
       "      <td>20005</td>\n",
       "      <td>104.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>201911</td>\n",
       "      <td>20005</td>\n",
       "      <td>107.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>117.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>201912</td>\n",
       "      <td>20005</td>\n",
       "      <td>110.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     periodo  product_id  cust_request_qty  cust_request_tn product_category  \\\n",
       "0     201701       20001               1.0              2.0            FOODS   \n",
       "1     201702       20001               2.0              3.0            FOODS   \n",
       "2     201703       20001               3.0              4.0            FOODS   \n",
       "3     201704       20001               4.0              5.0            FOODS   \n",
       "4     201705       20001               5.0              6.0            FOODS   \n",
       "..       ...         ...               ...              ...              ...   \n",
       "175   201908       20005              98.0            103.0               HC   \n",
       "176   201909       20005             101.0            106.0               HC   \n",
       "177   201910       20005             104.0            109.0               HC   \n",
       "178   201911       20005             107.0            112.0               HC   \n",
       "179   201912       20005             110.0            115.0               HC   \n",
       "\n",
       "            cat2  sku_size  plan_precios_cuidados      tn  target  \n",
       "0       ADEREZOS       3.0                      1     6.0    10.0  \n",
       "1       ADEREZOS       4.0                      1     8.0    12.0  \n",
       "2       ADEREZOS       5.0                      1    10.0    14.0  \n",
       "3       ADEREZOS       6.0                      1    12.0    16.0  \n",
       "4       ADEREZOS       7.0                      1    14.0    18.0  \n",
       "..           ...       ...                    ...     ...     ...  \n",
       "175  ROPA LAVADO     108.0                      1  1080.0  1140.0  \n",
       "176  ROPA LAVADO     111.0                      1  1110.0  1170.0  \n",
       "177  ROPA LAVADO     114.0                      1  1140.0  1200.0  \n",
       "178  ROPA LAVADO     117.0                      1  1170.0     NaN  \n",
       "179  ROPA LAVADO     120.0                      1  1200.0     NaN  \n",
       "\n",
       "[180 rows x 10 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"target\"] = df.apply(lag_target_class_2, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>product_id</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>product_category</th>\n",
       "      <th>cat2</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>tn</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201702</td>\n",
       "      <td>20001</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201703</td>\n",
       "      <td>20001</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201704</td>\n",
       "      <td>20001</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201705</td>\n",
       "      <td>20001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>201706</td>\n",
       "      <td>20001</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>201707</td>\n",
       "      <td>20001</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>201708</td>\n",
       "      <td>20001</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>201709</td>\n",
       "      <td>20001</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>201710</td>\n",
       "      <td>20001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>201711</td>\n",
       "      <td>20001</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>201712</td>\n",
       "      <td>20001</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>201801</td>\n",
       "      <td>20001</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>201802</td>\n",
       "      <td>20001</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>201803</td>\n",
       "      <td>20001</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>34.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>201804</td>\n",
       "      <td>20001</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>201805</td>\n",
       "      <td>20001</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>201806</td>\n",
       "      <td>20001</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>201807</td>\n",
       "      <td>20001</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>42.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>201808</td>\n",
       "      <td>20001</td>\n",
       "      <td>20.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>201809</td>\n",
       "      <td>20001</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>46.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>201810</td>\n",
       "      <td>20001</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>201811</td>\n",
       "      <td>20001</td>\n",
       "      <td>23.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>201812</td>\n",
       "      <td>20001</td>\n",
       "      <td>24.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>52.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>201901</td>\n",
       "      <td>20001</td>\n",
       "      <td>25.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>201902</td>\n",
       "      <td>20001</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>56.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>201903</td>\n",
       "      <td>20001</td>\n",
       "      <td>27.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>201904</td>\n",
       "      <td>20001</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>201905</td>\n",
       "      <td>20001</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>201906</td>\n",
       "      <td>20001</td>\n",
       "      <td>30.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>201907</td>\n",
       "      <td>20001</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "      <td>66.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>201908</td>\n",
       "      <td>20001</td>\n",
       "      <td>32.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1</td>\n",
       "      <td>68.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>201909</td>\n",
       "      <td>20001</td>\n",
       "      <td>33.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>201910</td>\n",
       "      <td>20001</td>\n",
       "      <td>34.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>72.0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>201911</td>\n",
       "      <td>20001</td>\n",
       "      <td>35.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>201912</td>\n",
       "      <td>20001</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>76.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    periodo  product_id  cust_request_qty  cust_request_tn product_category  \\\n",
       "0    201701       20001               1.0              2.0            FOODS   \n",
       "1    201702       20001               2.0              3.0            FOODS   \n",
       "2    201703       20001               3.0              4.0            FOODS   \n",
       "3    201704       20001               4.0              5.0            FOODS   \n",
       "4    201705       20001               5.0              6.0            FOODS   \n",
       "5    201706       20001               6.0              7.0            FOODS   \n",
       "6    201707       20001               7.0              8.0            FOODS   \n",
       "7    201708       20001               8.0              9.0            FOODS   \n",
       "8    201709       20001               9.0             10.0            FOODS   \n",
       "9    201710       20001              10.0             11.0            FOODS   \n",
       "10   201711       20001              11.0             12.0            FOODS   \n",
       "11   201712       20001              12.0             13.0            FOODS   \n",
       "12   201801       20001              13.0             14.0            FOODS   \n",
       "13   201802       20001              14.0             15.0            FOODS   \n",
       "14   201803       20001              15.0             16.0            FOODS   \n",
       "15   201804       20001              16.0             17.0            FOODS   \n",
       "16   201805       20001              17.0             18.0            FOODS   \n",
       "17   201806       20001              18.0             19.0            FOODS   \n",
       "18   201807       20001              19.0             20.0            FOODS   \n",
       "19   201808       20001              20.0             21.0            FOODS   \n",
       "20   201809       20001              21.0             22.0            FOODS   \n",
       "21   201810       20001              22.0             23.0            FOODS   \n",
       "22   201811       20001              23.0             24.0            FOODS   \n",
       "23   201812       20001              24.0             25.0            FOODS   \n",
       "24   201901       20001              25.0             26.0            FOODS   \n",
       "25   201902       20001              26.0             27.0            FOODS   \n",
       "26   201903       20001              27.0             28.0            FOODS   \n",
       "27   201904       20001              28.0             29.0            FOODS   \n",
       "28   201905       20001              29.0             30.0            FOODS   \n",
       "29   201906       20001              30.0             31.0            FOODS   \n",
       "30   201907       20001              31.0             32.0            FOODS   \n",
       "31   201908       20001              32.0             33.0            FOODS   \n",
       "32   201909       20001              33.0             34.0            FOODS   \n",
       "33   201910       20001              34.0             35.0            FOODS   \n",
       "34   201911       20001              35.0             36.0            FOODS   \n",
       "35   201912       20001              36.0             37.0            FOODS   \n",
       "\n",
       "        cat2  sku_size  plan_precios_cuidados    tn  target  \n",
       "0   ADEREZOS       3.0                      1   6.0    10.0  \n",
       "1   ADEREZOS       4.0                      1   8.0    12.0  \n",
       "2   ADEREZOS       5.0                      1  10.0    14.0  \n",
       "3   ADEREZOS       6.0                      1  12.0    16.0  \n",
       "4   ADEREZOS       7.0                      1  14.0    18.0  \n",
       "5   ADEREZOS       8.0                      1  16.0    20.0  \n",
       "6   ADEREZOS       9.0                      1  18.0    22.0  \n",
       "7   ADEREZOS      10.0                      1  20.0    24.0  \n",
       "8   ADEREZOS      11.0                      1  22.0    26.0  \n",
       "9   ADEREZOS      12.0                      1  24.0    28.0  \n",
       "10  ADEREZOS      13.0                      1  26.0    30.0  \n",
       "11  ADEREZOS      14.0                      1  28.0    32.0  \n",
       "12  ADEREZOS      15.0                      1  30.0    34.0  \n",
       "13  ADEREZOS      16.0                      1  32.0    36.0  \n",
       "14  ADEREZOS      17.0                      1  34.0    38.0  \n",
       "15  ADEREZOS      18.0                      1  36.0    40.0  \n",
       "16  ADEREZOS      19.0                      1  38.0    42.0  \n",
       "17  ADEREZOS      20.0                      1  40.0    44.0  \n",
       "18  ADEREZOS      21.0                      1  42.0    46.0  \n",
       "19  ADEREZOS      22.0                      1  44.0    48.0  \n",
       "20  ADEREZOS      23.0                      1  46.0    50.0  \n",
       "21  ADEREZOS      24.0                      1  48.0    52.0  \n",
       "22  ADEREZOS      25.0                      1  50.0    54.0  \n",
       "23  ADEREZOS      26.0                      1  52.0    56.0  \n",
       "24  ADEREZOS      27.0                      1  54.0    58.0  \n",
       "25  ADEREZOS      28.0                      1  56.0    60.0  \n",
       "26  ADEREZOS      29.0                      1  58.0    62.0  \n",
       "27  ADEREZOS      30.0                      1  60.0    64.0  \n",
       "28  ADEREZOS      31.0                      1  62.0    66.0  \n",
       "29  ADEREZOS      32.0                      1  64.0    68.0  \n",
       "30  ADEREZOS      33.0                      1  66.0    70.0  \n",
       "31  ADEREZOS      34.0                      1  68.0    72.0  \n",
       "32  ADEREZOS      35.0                      1  70.0    74.0  \n",
       "33  ADEREZOS      36.0                      1  72.0    76.0  \n",
       "34  ADEREZOS      37.0                      1  74.0     NaN  \n",
       "35  ADEREZOS      38.0                      1  76.0     NaN  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"product_id\"] == 20001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"product_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "periodo\n",
       "201701    5\n",
       "201702    5\n",
       "201809    5\n",
       "201810    5\n",
       "201811    5\n",
       "201812    5\n",
       "201901    5\n",
       "201902    5\n",
       "201903    5\n",
       "201904    5\n",
       "201905    5\n",
       "201906    5\n",
       "201907    5\n",
       "201908    5\n",
       "201909    5\n",
       "201910    5\n",
       "201911    5\n",
       "201808    5\n",
       "201807    5\n",
       "201806    5\n",
       "201709    5\n",
       "201703    5\n",
       "201704    5\n",
       "201705    5\n",
       "201706    5\n",
       "201707    5\n",
       "201708    5\n",
       "201710    5\n",
       "201805    5\n",
       "201711    5\n",
       "201712    5\n",
       "201801    5\n",
       "201802    5\n",
       "201803    5\n",
       "201804    5\n",
       "201912    5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"periodo\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "# Sort by `periodo` just to be sure\n",
    "product_data = df.sort_values([\"product_id\", \"periodo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((180, 10),\n",
       " Index(['periodo', 'product_id', 'cust_request_qty', 'cust_request_tn',\n",
       "        'product_category', 'cat2', 'sku_size', 'plan_precios_cuidados', 'tn',\n",
       "        'target'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_data.shape, product_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using scikit-learn, ONE HOT ENCODE the categorical variables: product_category\n",
    "product_data = pd.get_dummies(product_data, columns=[\"product_category\", \"cat2\"])\n",
    "\n",
    "# sort columns so that 'lag_tn' is the last column in the dataframe\n",
    "product_data = product_data[[col for col in product_data.columns if col != 'target'] + ['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert boolean columns to int\n",
    "product_data = product_data.astype({\"product_category_FOODS\": int, \"product_category_HC\": int, \"product_category_PC\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "periodo                     int64\n",
       "product_id                  int64\n",
       "plan_precios_cuidados       int64\n",
       "product_category_FOODS      int64\n",
       "product_category_HC         int64\n",
       "product_category_PC         int64\n",
       "cat2_ADEREZOS               int64\n",
       "cat2_PIEL1                  int64\n",
       "cat2_ROPA LAVADO            int64\n",
       "cust_request_qty          float64\n",
       "cust_request_tn           float64\n",
       "sku_size                  float64\n",
       "tn                        float64\n",
       "target                    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert bool columns to int\n",
    "values = [\n",
    "    \"ROPA LAVADO\",\n",
    "    \"PIEL1\",\n",
    "    \"ADEREZOS\",\n",
    "]\n",
    "\n",
    "for value in values:\n",
    "    product_data = product_data.astype({f\"cat2_{value}\": int})\n",
    "product_data.dtypes.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_data.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "train_df = product_data[product_data[\"periodo\"] <= 201909]\n",
    "test_df = product_data[product_data[\"periodo\"] <= 201910]\n",
    "predict_df = product_data[product_data[\"periodo\"] <= 201912]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standarize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler_train = StandardScaler()\n",
    "# scaler_test = StandardScaler()\n",
    "# scaler_predict_product_id = StandardScaler()\n",
    "# scaler_predict_target = StandardScaler()\n",
    "# scaler_predict_rest = StandardScaler()\n",
    "\n",
    "rest = [col for col in predict_df.columns if col not in [\"product_id\", \"target\"]]\n",
    "\n",
    "# train_df[train_df.columns] = scaler_train.fit_transform(train_df[train_df.columns])\n",
    "# test_df[test_df.columns] = scaler_test.fit_transform(test_df[test_df.columns])\n",
    "# predict_df[rest] = scaler_predict_rest.fit_transform(predict_df[rest])\n",
    "# predict_df[\"target\"] = scaler_predict_target.fit_transform(predict_df[[\"target\"]])\n",
    "# predict_df[\"product_id\"] = scaler_predict_product_id.fit_transform(predict_df[[\"product_id\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_layers, n_neurons, dropout_threshold, n_steps, X_train, y_train, X_test, y_test):\n",
    "    # Train a single model where product_id is part of the input\n",
    "    print(f\"Training model with {n_layers} layers, {n_neurons} neurons, {n_steps} steps.\")\n",
    "\n",
    "    # Number of features \n",
    "    n_features = X_train.shape[2]\n",
    "\n",
    "    # Define the LSTM model\n",
    "    if n_layers == 1:\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(n_neurons, activation=\"relu\", input_shape=(n_steps, n_features)))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    elif n_layers == 2:\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(n_neurons, activation=\"relu\", return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "        model.add(Dropout(dropout_threshold))\n",
    "        model.add(LSTM(n_neurons, activation=\"relu\"))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    elif n_layers == 3:\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(n_neurons, activation=\"relu\", return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "        model.add(Dropout(dropout_threshold))\n",
    "        model.add(LSTM(n_neurons, activation=\"relu\", return_sequences=True))\n",
    "        model.add(Dropout(dropout_threshold))\n",
    "        model.add(LSTM(n_neurons, activation=\"relu\"))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    elif n_layers == 4:\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(n_neurons, activation=\"relu\", return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "        model.add(Dropout(dropout_threshold))\n",
    "        model.add(LSTM(n_neurons, activation=\"relu\", return_sequences=True))\n",
    "        model.add(Dropout(dropout_threshold))\n",
    "        model.add(LSTM(n_neurons, activation=\"relu\", return_sequences=True))\n",
    "        model.add(Dropout(dropout_threshold))\n",
    "        model.add(LSTM(n_neurons, activation=\"relu\"))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    else:\n",
    "        raise ValueError(\"n_layers must be 1, 2, 3 or 4\")\n",
    "\n",
    "    # Define EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, epochs=200, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(n_steps, model, X_predict, y_predict):\n",
    "    rows = []\n",
    "    # for each product_id, generate a prediction for 201904\n",
    "    for i, product in enumerate(predict_df[\"product_id\"].unique()):\n",
    "        print(\n",
    "            f\"Predicting 201904 for product {product} ({i+1}/{len(predict_df['product_id'].unique())}))\"\n",
    "        )\n",
    "\n",
    "        this_predict_df = predict_df[predict_df[\"product_id\"] == product]\n",
    "\n",
    "        this_predict_df_array = this_predict_df.values\n",
    "\n",
    "        n_features = X_predict.shape[2]\n",
    "\n",
    "        # Prepare the input for prediction\n",
    "        x_input = this_predict_df_array[-n_steps:, :-1]\n",
    "        x_input = x_input.reshape((1, n_steps, n_features))\n",
    "\n",
    "        print(\"/////////// INPUT ///////////////////////////\")\n",
    "        print(\"/////////// INPUT ///////////////////////////\")\n",
    "        print(\"/////////// INPUT ///////////////////////////\")\n",
    "        print(x_input)\n",
    "        print(\"/////////// INPUT ///////////////////////////\")\n",
    "        print(\"/////////// INPUT ///////////////////////////\")\n",
    "        print(\"/////////// INPUT ///////////////////////////\")\n",
    "\n",
    "\n",
    "        # Make prediction\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        if yhat[0][0] < 1:\n",
    "            yhat[0][0] = 0\n",
    "\n",
    "        # Append to final output DataFrame\n",
    "        rows.append(\n",
    "            {\n",
    "                \"product_id\": product,\n",
    "                \"target\": yhat[0][0],\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    final_output = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\"product_id\", \"target\"],\n",
    "    )\n",
    "    final_output = final_output.sort_values(\"product_id\", ascending=True)\n",
    "    timestamp = datetime.now().timestamp()\n",
    "    final_output.to_csv(f\"./output/output_lstm8_BO_{timestamp}.csv\", index=False)\n",
    "    print(\"//////////////////////////////////////\")\n",
    "    print(X_predict.shape, y_predict.shape, final_output.shape)\n",
    "    print(\"//////////////////////////////////////\")\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(predictions):\n",
    "    # revert the standarization for the output\n",
    "    # predictions[\"target\"] = scaler_predict_target.inverse_transform(predictions[[\"target\"]])\n",
    "    # predictions[\"product_id\"] = scaler_predict_product_id.inverse_transform(predictions[[\"product_id\"]])\n",
    "    \n",
    "    predictions[\"product_id\"] = predictions[\"product_id\"].astype(int)\n",
    "    predictions.rename(columns={\"target\": \"prediction\"}, inplace=True)\n",
    "\n",
    "    print(\"*********\"*10)\n",
    "    print(\"*********\"*10)\n",
    "    print(\"*********\"*10)\n",
    "    print(predictions)\n",
    "    print(\"*********\"*10)\n",
    "    print(\"*********\"*10)\n",
    "    print(\"*********\"*10)\n",
    "\n",
    "    \n",
    "    return calculate_error(predictions[\"prediction\"].tolist(), [76, 118.5, 75, 190, 1200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_s_rows = []\n",
    "# for n_layers in NUMBER_OF_LAYERS:\n",
    "#     for n_neurons in NUMBER_OF_NEURONS:\n",
    "#         for n_steps in NUMBER_OF_STEPS:\n",
    "\n",
    "#             X_train, y_train = split_sequences(train_df.values, n_steps)\n",
    "#             X_test, y_test = split_sequences(test_df.values, n_steps)\n",
    "#             X_predict, y_predict = split_sequences(predict_df.values, n_steps)\n",
    "\n",
    "#             model = train_model(n_layers, n_neurons, n_steps, X_train, y_train, X_test, y_test)\n",
    "#             predictions = make_predictions(n_steps, model, X_predict, y_predict)\n",
    "#             score = calculate_score(predictions)\n",
    "#             print(\"SCORE: \", score)\n",
    "\n",
    "#             g_s_rows.append(\n",
    "#                 {\n",
    "#                     \"n_layers\": n_layers,\n",
    "#                     \"n_neurons\": n_neurons,\n",
    "#                     \"n_steps\": n_steps,\n",
    "#                     \"score\": score,\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#             g_s_df = pd.DataFrame(g_s_rows)\n",
    "#             g_s_df.to_csv(f\"./output/test_grid_search_lstm8_productos_{n_layers}_{n_neurons[0]}_{n_steps}_{score}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define the search space using Optuna\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 4)\n",
    "    n_neurons = trial.suggest_int('n_neurons', 20, 500)\n",
    "    n_steps = trial.suggest_int('n_steps', 1, 20)\n",
    "    embudo = trial.suggest_float('embudo', 1.1, 2.5)\n",
    "    dropout_threshold = trial.suggest_float('dropout_threshold', 0.1, 0.66)\n",
    "\n",
    "    # Your existing logic to train and evaluate the model\n",
    "    X_train, y_train = split_sequences(train_df.values, n_steps)\n",
    "    X_test, y_test = split_sequences(test_df.values, n_steps)\n",
    "    X_predict, y_predict = split_sequences(predict_df.values, n_steps)\n",
    "\n",
    "    model = train_model(n_layers, n_neurons, dropout_threshold, n_steps, X_train, y_train, X_test, y_test)\n",
    "    predictions = make_predictions(n_steps, model, X_predict, y_predict)\n",
    "    score = calculate_score(predictions)\n",
    "\n",
    "    # Save the results\n",
    "    result = {\n",
    "        \"n_layers\": n_layers,\n",
    "        \"n_neurons\": n_neurons,\n",
    "        \"n_steps\": n_steps,\n",
    "        \"dropout_threshold\": dropout_threshold,\n",
    "        \"score\": score\n",
    "    }\n",
    "    g_s_df = pd.DataFrame([result])\n",
    "    g_s_df.to_csv(f\"./output/lstm_8_optuna_{n_layers}_{n_neurons}_{n_steps}_{dropout_threshold}_{score}.csv\", mode='a', index=False)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:46:26,159] A new study created in memory with name: lstm_8_BO_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "Training model with 1 layers, 498 neurons, 18 steps.\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 47ms/step - loss: 764821248.0000 - val_loss: 3197652736.0000\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1306525312.0000 - val_loss: 196674448.0000\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 793546112.0000 - val_loss: 422885984.0000\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 822433024.0000 - val_loss: 217925296.0000\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 191734160.0000 - val_loss: 289749472.0000\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 530551008.0000 - val_loss: 434444096.0000\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 750432896.0000 - val_loss: 2191541504.0000\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 690891648.0000 - val_loss: 68135264.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 131686536.0000 - val_loss: 218347728.0000\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 936198336.0000 - val_loss: 325680832.0000\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 414694400.0000 - val_loss: 69696264.0000\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 720031040.0000 - val_loss: 1401031040.0000\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 803830400.0000 - val_loss: 2050857.6250\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 71283072.0000 - val_loss: 626720576.0000\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 439814112.0000 - val_loss: 4621035008.0000\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1746164864.0000 - val_loss: 135009120.0000\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 861546752.0000 - val_loss: 3685544960.0000\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1823049728.0000 - val_loss: 13645547.0000\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3632543488.0000 - val_loss: 1814469504.0000\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1798709632.0000 - val_loss: 10592100352.0000\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7934643712.0000 - val_loss: 128490264.0000\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 114123944.0000 - val_loss: 1081434368.0000\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1042659712.0000 - val_loss: 1042900608.0000\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01807e+05 2.00010e+04 1.90000e+01 2.00000e+01 2.10000e+01\n",
      "   1.00000e+00 4.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01808e+05 2.00010e+04 2.00000e+01 2.10000e+01 2.20000e+01\n",
      "   1.00000e+00 4.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00010e+04 2.10000e+01 2.20000e+01 2.30000e+01\n",
      "   1.00000e+00 4.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00010e+04 2.20000e+01 2.30000e+01 2.40000e+01\n",
      "   1.00000e+00 4.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00010e+04 2.30000e+01 2.40000e+01 2.50000e+01\n",
      "   1.00000e+00 5.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00010e+04 2.40000e+01 2.50000e+01 2.60000e+01\n",
      "   1.00000e+00 5.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00010e+04 2.50000e+01 2.60000e+01 2.70000e+01\n",
      "   1.00000e+00 5.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00010e+04 2.60000e+01 2.70000e+01 2.80000e+01\n",
      "   1.00000e+00 5.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00010e+04 2.70000e+01 2.80000e+01 2.90000e+01\n",
      "   1.00000e+00 5.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00010e+04 2.80000e+01 2.90000e+01 3.00000e+01\n",
      "   1.00000e+00 6.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00010e+04 2.90000e+01 3.00000e+01 3.10000e+01\n",
      "   1.00000e+00 6.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00010e+04 3.00000e+01 3.10000e+01 3.20000e+01\n",
      "   1.00000e+00 6.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01807e+05 2.00020e+04 2.05000e+01 2.15000e+01 2.25000e+01\n",
      "   0.00000e+00 6.75000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01808e+05 2.00020e+04 2.15000e+01 2.25000e+01 2.35000e+01\n",
      "   0.00000e+00 7.05000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00020e+04 2.25000e+01 2.35000e+01 2.45000e+01\n",
      "   0.00000e+00 7.35000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00020e+04 2.35000e+01 2.45000e+01 2.55000e+01\n",
      "   0.00000e+00 7.65000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00020e+04 2.45000e+01 2.55000e+01 2.65000e+01\n",
      "   0.00000e+00 7.95000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00020e+04 2.55000e+01 2.65000e+01 2.75000e+01\n",
      "   0.00000e+00 8.25000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00020e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   0.00000e+00 8.55000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00020e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   0.00000e+00 8.85000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00020e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   0.00000e+00 9.15000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00020e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   0.00000e+00 9.45000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00020e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   0.00000e+00 9.75000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00020e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   0.00000e+00 1.00500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:46:29,421] Trial 0 finished with value: 5.000160504623004 and parameters: {'n_layers': 1, 'n_neurons': 498, 'n_steps': 18, 'dropout_threshold': 0.6147287077041598}. Best is trial 0 with value: 5.000160504623004.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01807e+05 2.00030e+04 1.85000e+01 1.95000e+01 2.05000e+01\n",
      "   1.00000e+00 4.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01808e+05 2.00030e+04 1.95000e+01 2.05000e+01 2.15000e+01\n",
      "   1.00000e+00 4.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00030e+04 2.05000e+01 2.15000e+01 2.25000e+01\n",
      "   1.00000e+00 4.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00030e+04 2.15000e+01 2.25000e+01 2.35000e+01\n",
      "   1.00000e+00 4.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00030e+04 2.25000e+01 2.35000e+01 2.45000e+01\n",
      "   1.00000e+00 4.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00030e+04 2.35000e+01 2.45000e+01 2.55000e+01\n",
      "   1.00000e+00 5.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00030e+04 2.45000e+01 2.55000e+01 2.65000e+01\n",
      "   1.00000e+00 5.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00030e+04 2.55000e+01 2.65000e+01 2.75000e+01\n",
      "   1.00000e+00 5.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00030e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   1.00000e+00 5.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00030e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   1.00000e+00 5.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00030e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   1.00000e+00 6.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00030e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   1.00000e+00 6.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01807e+05 2.00040e+04 1.90000e+02 2.00000e+02 2.10000e+02\n",
      "   0.00000e+00 1.05000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01808e+05 2.00040e+04 2.00000e+02 2.10000e+02 2.20000e+02\n",
      "   0.00000e+00 1.10000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00040e+04 2.10000e+02 2.20000e+02 2.30000e+02\n",
      "   0.00000e+00 1.15000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00040e+04 2.20000e+02 2.30000e+02 2.40000e+02\n",
      "   0.00000e+00 1.20000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00040e+04 2.30000e+02 2.40000e+02 2.50000e+02\n",
      "   0.00000e+00 1.25000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00040e+04 2.40000e+02 2.50000e+02 2.60000e+02\n",
      "   0.00000e+00 1.30000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00040e+04 2.50000e+02 2.60000e+02 2.70000e+02\n",
      "   0.00000e+00 1.35000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00040e+04 2.60000e+02 2.70000e+02 2.80000e+02\n",
      "   0.00000e+00 1.40000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00040e+04 2.70000e+02 2.80000e+02 2.90000e+02\n",
      "   0.00000e+00 1.45000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00040e+04 2.80000e+02 2.90000e+02 3.00000e+02\n",
      "   0.00000e+00 1.50000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00040e+04 2.90000e+02 3.00000e+02 3.10000e+02\n",
      "   0.00000e+00 1.55000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00040e+04 3.00000e+02 3.10000e+02 3.20000e+02\n",
      "   0.00000e+00 1.60000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01807e+05 2.00050e+04 5.90000e+01 6.40000e+01 6.90000e+01\n",
      "   1.00000e+00 6.90000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01808e+05 2.00050e+04 6.20000e+01 6.70000e+01 7.20000e+01\n",
      "   1.00000e+00 7.20000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01809e+05 2.00050e+04 6.50000e+01 7.00000e+01 7.50000e+01\n",
      "   1.00000e+00 7.50000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01810e+05 2.00050e+04 6.80000e+01 7.30000e+01 7.80000e+01\n",
      "   1.00000e+00 7.80000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01811e+05 2.00050e+04 7.10000e+01 7.60000e+01 8.10000e+01\n",
      "   1.00000e+00 8.10000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01812e+05 2.00050e+04 7.40000e+01 7.90000e+01 8.40000e+01\n",
      "   1.00000e+00 8.40000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01901e+05 2.00050e+04 7.70000e+01 8.20000e+01 8.70000e+01\n",
      "   1.00000e+00 8.70000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01902e+05 2.00050e+04 8.00000e+01 8.50000e+01 9.00000e+01\n",
      "   1.00000e+00 9.00000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01903e+05 2.00050e+04 8.30000e+01 8.80000e+01 9.30000e+01\n",
      "   1.00000e+00 9.30000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01904e+05 2.00050e+04 8.60000e+01 9.10000e+01 9.60000e+01\n",
      "   1.00000e+00 9.60000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01905e+05 2.00050e+04 8.90000e+01 9.40000e+01 9.90000e+01\n",
      "   1.00000e+00 9.90000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01906e+05 2.00050e+04 9.20000e+01 9.70000e+01 1.02000e+02\n",
      "   1.00000e+00 1.02000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(163, 18, 13) (163,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001  1568.733887\n",
      "1       20002  1570.347412\n",
      "2       20003  1568.808838\n",
      "3       20004  1619.037598\n",
      "4       20005  3630.338623\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "Training model with 1 layers, 178 neurons, 12 steps.\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 38ms/step - loss: 824528128.0000 - val_loss: 3308306432.0000\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1083325568.0000 - val_loss: 736210176.0000\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 483207360.0000 - val_loss: 175758992.0000\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 163845792.0000 - val_loss: 250248112.0000\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 194202000.0000 - val_loss: 51878836.0000\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 241983136.0000 - val_loss: 513174144.0000\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 306620256.0000 - val_loss: 15606296.0000\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 63358232.0000 - val_loss: 110290672.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 61061328.0000 - val_loss: 2019514.0000\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 10973045.0000 - val_loss: 3925591.2500\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5131046.0000 - val_loss: 5999714.0000\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 448180000.0000 - val_loss: 1070546240.0000\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 790188160.0000 - val_loss: 525896320.0000\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 357094080.0000 - val_loss: 185054272.0000\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 735303104.0000 - val_loss: 145249552.0000\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 82105808.0000 - val_loss: 186620208.0000\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 243382560.0000 - val_loss: 144719088.0000\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 57980396.0000 - val_loss: 129868176.0000\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 50680312.0000 - val_loss: 8288294.0000\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01901e+05 2.00010e+04 2.50000e+01 2.60000e+01 2.70000e+01\n",
      "   1.00000e+00 5.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00010e+04 2.60000e+01 2.70000e+01 2.80000e+01\n",
      "   1.00000e+00 5.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00010e+04 2.70000e+01 2.80000e+01 2.90000e+01\n",
      "   1.00000e+00 5.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00010e+04 2.80000e+01 2.90000e+01 3.00000e+01\n",
      "   1.00000e+00 6.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00010e+04 2.90000e+01 3.00000e+01 3.10000e+01\n",
      "   1.00000e+00 6.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00010e+04 3.00000e+01 3.10000e+01 3.20000e+01\n",
      "   1.00000e+00 6.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01901e+05 2.00020e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   0.00000e+00 8.55000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00020e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   0.00000e+00 8.85000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00020e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   0.00000e+00 9.15000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00020e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   0.00000e+00 9.45000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00020e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   0.00000e+00 9.75000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00020e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   0.00000e+00 1.00500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01901e+05 2.00030e+04 2.45000e+01 2.55000e+01 2.65000e+01\n",
      "   1.00000e+00 5.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00030e+04 2.55000e+01 2.65000e+01 2.75000e+01\n",
      "   1.00000e+00 5.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00030e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   1.00000e+00 5.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00030e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   1.00000e+00 5.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00030e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   1.00000e+00 6.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00030e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   1.00000e+00 6.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01901e+05 2.00040e+04 2.50000e+02 2.60000e+02 2.70000e+02\n",
      "   0.00000e+00 1.35000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00040e+04 2.60000e+02 2.70000e+02 2.80000e+02\n",
      "   0.00000e+00 1.40000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00040e+04 2.70000e+02 2.80000e+02 2.90000e+02\n",
      "   0.00000e+00 1.45000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00040e+04 2.80000e+02 2.90000e+02 3.00000e+02\n",
      "   0.00000e+00 1.50000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00040e+04 2.90000e+02 3.00000e+02 3.10000e+02\n",
      "   0.00000e+00 1.55000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00040e+04 3.00000e+02 3.10000e+02 3.20000e+02\n",
      "   0.00000e+00 1.60000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:46:31,407] Trial 1 finished with value: 1.0 and parameters: {'n_layers': 1, 'n_neurons': 178, 'n_steps': 12, 'dropout_threshold': 0.35606905739772776}. Best is trial 1 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01901e+05 2.00050e+04 7.70000e+01 8.20000e+01 8.70000e+01\n",
      "   1.00000e+00 8.70000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01902e+05 2.00050e+04 8.00000e+01 8.50000e+01 9.00000e+01\n",
      "   1.00000e+00 9.00000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01903e+05 2.00050e+04 8.30000e+01 8.80000e+01 9.30000e+01\n",
      "   1.00000e+00 9.30000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01904e+05 2.00050e+04 8.60000e+01 9.10000e+01 9.60000e+01\n",
      "   1.00000e+00 9.60000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01905e+05 2.00050e+04 8.90000e+01 9.40000e+01 9.90000e+01\n",
      "   1.00000e+00 9.90000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01906e+05 2.00050e+04 9.20000e+01 9.70000e+01 1.02000e+02\n",
      "   1.00000e+00 1.02000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(169, 12, 13) (169,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001         0.0\n",
      "1       20002         0.0\n",
      "2       20003         0.0\n",
      "3       20004         0.0\n",
      "4       20005         0.0\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "Training model with 1 layers, 441 neurons, 7 steps.\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 37ms/step - loss: 67265120.0000 - val_loss: 44174216.0000\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 15668528.0000 - val_loss: 77418400.0000\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 19868784.0000 - val_loss: 8501918.0000\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 11996466.0000 - val_loss: 218964.6562\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1408531.8750 - val_loss: 238136.1719\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 799430.5625 - val_loss: 142588.7969\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 369427.7188 - val_loss: 230051.7812\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 485032.0625 - val_loss: 655818.7500\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 811384.9375 - val_loss: 950965.0625\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 920420.0000 - val_loss: 643679.1875\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 564618.6250 - val_loss: 402069.9062\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 585674.9375 - val_loss: 1155710.2500\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 718054.0000 - val_loss: 756360.5625\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 542986.1875 - val_loss: 81320.9531\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 164373.2188 - val_loss: 97351.4844\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 117569.1719 - val_loss: 94162.0703\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 97026.4375 - val_loss: 116994.8438\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 104623.2031 - val_loss: 114205.6562\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 96799.4531 - val_loss: 107785.0703\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 89992.2891 - val_loss: 81538.3438\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 80644.2422 - val_loss: 102770.1484\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 86726.8672 - val_loss: 80432.0156\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 76172.2500 - val_loss: 77594.0469\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 75859.6250 - val_loss: 75561.3672\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 73080.3594 - val_loss: 75004.9609\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 73210.1797 - val_loss: 73968.1562\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 70515.1484 - val_loss: 73230.4141\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 72287.9375 - val_loss: 88062.8438\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 79408.6172 - val_loss: 80142.0000\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 76510.5156 - val_loss: 71571.2500\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 73977.8594 - val_loss: 72173.8047\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 67003.1484 - val_loss: 78675.1719\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 71085.9297 - val_loss: 71181.8828\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 66297.3672 - val_loss: 68652.3281\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 66916.9219 - val_loss: 74839.0859\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 68152.3750 - val_loss: 67204.8516\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 66541.6641 - val_loss: 70430.7109\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 66968.7188 - val_loss: 66553.7891\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 68659.5000 - val_loss: 65809.3672\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 75666.5625 - val_loss: 64679.0195\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 65900.7734 - val_loss: 63762.7383\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 62309.1836 - val_loss: 83253.6016\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 80014.8906 - val_loss: 98150.4688\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 81621.5469 - val_loss: 70961.6328\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 73685.7188 - val_loss: 78614.3672\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 78274.5000 - val_loss: 81984.3750\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 72348.0469 - val_loss: 76869.7734\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 65962.6641 - val_loss: 58788.3906\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 73930.5312 - val_loss: 73905.6484\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 66117.5391 - val_loss: 57392.2930\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 63411.9375 - val_loss: 60479.9336\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 56332.7734 - val_loss: 56147.7695\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 55985.9180 - val_loss: 56082.7422\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 55279.9883 - val_loss: 84516.2188\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 76512.6328 - val_loss: 68150.2344\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 77200.3672 - val_loss: 70231.6172\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 82113.2969 - val_loss: 93432.5312\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 65501.8555 - val_loss: 52712.5430\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 63747.7305 - val_loss: 107407.5000\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 80407.4688 - val_loss: 58697.2695\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 58423.0430 - val_loss: 55650.7930\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 56069.8750 - val_loss: 64223.7578\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 59721.9492 - val_loss: 61694.6406\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 62619.8984 - val_loss: 88038.2969\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 60666.1523 - val_loss: 75632.2656\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 64500.6289 - val_loss: 80520.7109\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 63650.8320 - val_loss: 52133.9883\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 56807.2148 - val_loss: 53757.4531\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 48019.7305 - val_loss: 55621.3242\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 49944.5156 - val_loss: 49871.8125\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 55254.7852 - val_loss: 43378.0430\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 53458.3828 - val_loss: 55035.7695\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 44354.9492 - val_loss: 42482.8672\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 44220.0234 - val_loss: 56626.7852\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 57455.5469 - val_loss: 47137.5117\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 50782.4492 - val_loss: 58582.6953\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 42473.7656 - val_loss: 41020.9922\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 41804.0391 - val_loss: 47312.8008\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 45348.4805 - val_loss: 39910.4727\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 42562.2383 - val_loss: 40590.8672\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 41574.8320 - val_loss: 41281.5352\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 49758.4844 - val_loss: 38266.5039\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 45515.9453 - val_loss: 61791.7695\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 41371.6367 - val_loss: 36611.8906\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 38246.6953 - val_loss: 35401.8984\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 34062.8555 - val_loss: 34286.5781\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 33675.0547 - val_loss: 34759.3789\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 35055.6680 - val_loss: 32998.4102\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 39934.9102 - val_loss: 52066.2422\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 50360.0117 - val_loss: 64982.2734\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 40258.9492 - val_loss: 36779.2070\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 35340.8008 - val_loss: 30810.8965\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 30656.8047 - val_loss: 31170.5801\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 31026.5977 - val_loss: 33425.0352\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 31185.8105 - val_loss: 29812.6289\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 56553.1016 - val_loss: 31682.7988\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 58008.5781 - val_loss: 30613.5000\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 37788.9766 - val_loss: 37832.9766\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 34018.5273 - val_loss: 27223.2188\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 34001.3320 - val_loss: 29254.0547\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 31069.9492 - val_loss: 26219.9355\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 27678.9551 - val_loss: 29731.8477\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 26407.7207 - val_loss: 28557.4355\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 25155.8262 - val_loss: 25404.0000\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 24393.1191 - val_loss: 24381.8848\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 29346.6602 - val_loss: 36862.9648\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 32751.9941 - val_loss: 29689.5801\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 25972.2617 - val_loss: 23140.1309\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 24437.5039 - val_loss: 23043.5449\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 27849.0879 - val_loss: 21716.3945\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 21565.8867 - val_loss: 21374.5312\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 21678.4883 - val_loss: 21077.3086\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 24177.2871 - val_loss: 31471.1973\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 28052.4473 - val_loss: 27789.4141\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 24474.6484 - val_loss: 30884.3418\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 29793.9688 - val_loss: 24314.6328\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 20222.6016 - val_loss: 22350.2207\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 20740.3281 - val_loss: 20359.4121\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 18724.0039 - val_loss: 17920.6348\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 18172.1367 - val_loss: 18056.2168\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 17670.8066 - val_loss: 17306.9961\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 22455.7500 - val_loss: 23664.1953\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 18850.2695 - val_loss: 20551.5605\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 20031.5430 - val_loss: 32402.4766\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 24255.1094 - val_loss: 30821.6895\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 24270.4961 - val_loss: 26564.8906\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 21872.7676 - val_loss: 17295.3652\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 16316.9053 - val_loss: 15093.2168\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 15394.3838 - val_loss: 14257.1680\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 13840.5156 - val_loss: 19386.5938\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 19300.1816 - val_loss: 33914.3867\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 33340.9922 - val_loss: 21249.4434\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 28017.2734 - val_loss: 13937.0732\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 22586.9961 - val_loss: 25512.1230\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 21208.1230 - val_loss: 19602.5645\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 19683.9805 - val_loss: 13912.8672\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 17219.5352 - val_loss: 15036.6201\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 13227.9375 - val_loss: 11434.1611\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 11084.3975 - val_loss: 11284.6123\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 11653.9980 - val_loss: 13035.1826\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 18332.2695 - val_loss: 12382.9082\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 15761.9404 - val_loss: 12021.9258\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 15834.3535 - val_loss: 52098.9453\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 30924.2578 - val_loss: 12211.9434\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 14492.7676 - val_loss: 12430.3818\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 12372.0977 - val_loss: 17675.9766\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 12350.4375 - val_loss: 9661.1758\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 10277.7148 - val_loss: 11579.3838\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 9980.4844 - val_loss: 14888.1826\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 10511.6006 - val_loss: 8542.4707\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 8900.3887 - val_loss: 9183.5781\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 9067.6797 - val_loss: 8520.2852\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 8876.4014 - val_loss: 8646.0264\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9822.7959 - val_loss: 9254.8018\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 9959.4434 - val_loss: 9154.6309\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 8397.1699 - val_loss: 19293.9629\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 11808.3047 - val_loss: 7870.9062\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 9575.1475 - val_loss: 7953.3315\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7260.0142 - val_loss: 6572.8896\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6367.0205 - val_loss: 13147.4805\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 11833.4658 - val_loss: 11303.4590\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 10314.6406 - val_loss: 10972.6924\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 9087.4951 - val_loss: 10780.2441\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 8735.8340 - val_loss: 5775.6660\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 10063.4902 - val_loss: 11729.4131\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 8044.4932 - val_loss: 5708.3184\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5731.1685 - val_loss: 6460.8750\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6613.9741 - val_loss: 6860.9917\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 9622.8154 - val_loss: 5345.2559\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 8466.1201 - val_loss: 10043.8945\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7643.3296 - val_loss: 4785.1172\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5009.9248 - val_loss: 4825.0513\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4594.7729 - val_loss: 5062.1245\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5674.4277 - val_loss: 6538.7368\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4965.2148 - val_loss: 4480.2798\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4208.9790 - val_loss: 6379.1899\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5964.3569 - val_loss: 4275.1938\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5802.7427 - val_loss: 9427.8408\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 8757.6152 - val_loss: 4327.0703\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 7785.7344 - val_loss: 5451.1724\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4570.4790 - val_loss: 3910.7202\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4647.3633 - val_loss: 4769.7769\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4539.1689 - val_loss: 5562.1577\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6695.5361 - val_loss: 7735.1782\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5870.3374 - val_loss: 5764.4751\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4913.9746 - val_loss: 5236.8057\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5792.3799 - val_loss: 3315.2627\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3351.0427 - val_loss: 3105.5186\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3046.9895 - val_loss: 3105.3225\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3162.2898 - val_loss: 3146.9922\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3164.6841 - val_loss: 3619.0591\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6790.3970 - val_loss: 2775.3237\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5336.1694 - val_loss: 9167.3447\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6167.5874 - val_loss: 5317.6943\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4821.9048 - val_loss: 2555.8508\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5848.5811 - val_loss: 9248.6846\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 8284.6436 - val_loss: 8658.4111\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 7570.4497 - val_loss: 3294.9189\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5479.7627 - val_loss: 4314.8306\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5901.8555 - val_loss: 3858.9138\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01906e+05 2.00010e+04 3.00000e+01 3.10000e+01 3.20000e+01\n",
      "   1.00000e+00 6.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01906e+05 2.00020e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   0.00000e+00 1.00500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01906e+05 2.00030e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   1.00000e+00 6.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01906e+05 2.00040e+04 3.00000e+02 3.10000e+02 3.20000e+02\n",
      "   0.00000e+00 1.60000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:46:42,827] Trial 2 finished with value: 0.09382065982479545 and parameters: {'n_layers': 1, 'n_neurons': 441, 'n_steps': 7, 'dropout_threshold': 0.12519259144042516}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01906e+05 2.00050e+04 9.20000e+01 9.70000e+01 1.02000e+02\n",
      "   1.00000e+00 1.02000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(174, 7, 13) (174,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001    56.271980\n",
      "1       20002    97.253792\n",
      "2       20003    54.957432\n",
      "3       20004   236.941528\n",
      "4       20005  1152.262939\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "Training model with 4 layers, 360 neurons, 17 steps.\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 3s 136ms/step - loss: 77782936.0000 - val_loss: 262900064.0000\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 127645480.0000 - val_loss: 90528808.0000\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 171342560.0000 - val_loss: 149399408.0000\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 61ms/step - loss: 163971184.0000 - val_loss: 248684144.0000\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 241935632.0000 - val_loss: 355772544.0000\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 63ms/step - loss: 209243424.0000 - val_loss: 517170656.0000\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 284333376.0000 - val_loss: 1106079744.0000\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 576777920.0000 - val_loss: 164880512.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 415713664.0000 - val_loss: 365047264.0000\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 479054752.0000 - val_loss: 583089088.0000\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 378739488.0000 - val_loss: 554536384.0000\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 300797888.0000 - val_loss: 126911248.0000\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01808e+05 2.00010e+04 2.00000e+01 2.10000e+01 2.20000e+01\n",
      "   1.00000e+00 4.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00010e+04 2.10000e+01 2.20000e+01 2.30000e+01\n",
      "   1.00000e+00 4.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00010e+04 2.20000e+01 2.30000e+01 2.40000e+01\n",
      "   1.00000e+00 4.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00010e+04 2.30000e+01 2.40000e+01 2.50000e+01\n",
      "   1.00000e+00 5.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00010e+04 2.40000e+01 2.50000e+01 2.60000e+01\n",
      "   1.00000e+00 5.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00010e+04 2.50000e+01 2.60000e+01 2.70000e+01\n",
      "   1.00000e+00 5.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00010e+04 2.60000e+01 2.70000e+01 2.80000e+01\n",
      "   1.00000e+00 5.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00010e+04 2.70000e+01 2.80000e+01 2.90000e+01\n",
      "   1.00000e+00 5.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00010e+04 2.80000e+01 2.90000e+01 3.00000e+01\n",
      "   1.00000e+00 6.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00010e+04 2.90000e+01 3.00000e+01 3.10000e+01\n",
      "   1.00000e+00 6.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00010e+04 3.00000e+01 3.10000e+01 3.20000e+01\n",
      "   1.00000e+00 6.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:46:49,719] Trial 3 finished with value: 15.154943601235312 and parameters: {'n_layers': 4, 'n_neurons': 360, 'n_steps': 17, 'dropout_threshold': 0.17323124353767033}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01808e+05 2.00020e+04 2.15000e+01 2.25000e+01 2.35000e+01\n",
      "   0.00000e+00 7.05000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00020e+04 2.25000e+01 2.35000e+01 2.45000e+01\n",
      "   0.00000e+00 7.35000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00020e+04 2.35000e+01 2.45000e+01 2.55000e+01\n",
      "   0.00000e+00 7.65000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00020e+04 2.45000e+01 2.55000e+01 2.65000e+01\n",
      "   0.00000e+00 7.95000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00020e+04 2.55000e+01 2.65000e+01 2.75000e+01\n",
      "   0.00000e+00 8.25000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00020e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   0.00000e+00 8.55000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00020e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   0.00000e+00 8.85000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00020e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   0.00000e+00 9.15000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00020e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   0.00000e+00 9.45000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00020e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   0.00000e+00 9.75000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00020e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   0.00000e+00 1.00500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01808e+05 2.00030e+04 1.95000e+01 2.05000e+01 2.15000e+01\n",
      "   1.00000e+00 4.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00030e+04 2.05000e+01 2.15000e+01 2.25000e+01\n",
      "   1.00000e+00 4.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00030e+04 2.15000e+01 2.25000e+01 2.35000e+01\n",
      "   1.00000e+00 4.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00030e+04 2.25000e+01 2.35000e+01 2.45000e+01\n",
      "   1.00000e+00 4.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00030e+04 2.35000e+01 2.45000e+01 2.55000e+01\n",
      "   1.00000e+00 5.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00030e+04 2.45000e+01 2.55000e+01 2.65000e+01\n",
      "   1.00000e+00 5.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00030e+04 2.55000e+01 2.65000e+01 2.75000e+01\n",
      "   1.00000e+00 5.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00030e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   1.00000e+00 5.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00030e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   1.00000e+00 5.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00030e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   1.00000e+00 6.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00030e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   1.00000e+00 6.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01808e+05 2.00040e+04 2.00000e+02 2.10000e+02 2.20000e+02\n",
      "   0.00000e+00 1.10000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00040e+04 2.10000e+02 2.20000e+02 2.30000e+02\n",
      "   0.00000e+00 1.15000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00040e+04 2.20000e+02 2.30000e+02 2.40000e+02\n",
      "   0.00000e+00 1.20000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00040e+04 2.30000e+02 2.40000e+02 2.50000e+02\n",
      "   0.00000e+00 1.25000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00040e+04 2.40000e+02 2.50000e+02 2.60000e+02\n",
      "   0.00000e+00 1.30000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00040e+04 2.50000e+02 2.60000e+02 2.70000e+02\n",
      "   0.00000e+00 1.35000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00040e+04 2.60000e+02 2.70000e+02 2.80000e+02\n",
      "   0.00000e+00 1.40000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00040e+04 2.70000e+02 2.80000e+02 2.90000e+02\n",
      "   0.00000e+00 1.45000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00040e+04 2.80000e+02 2.90000e+02 3.00000e+02\n",
      "   0.00000e+00 1.50000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00040e+04 2.90000e+02 3.00000e+02 3.10000e+02\n",
      "   0.00000e+00 1.55000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00040e+04 3.00000e+02 3.10000e+02 3.20000e+02\n",
      "   0.00000e+00 1.60000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01808e+05 2.00050e+04 6.20000e+01 6.70000e+01 7.20000e+01\n",
      "   1.00000e+00 7.20000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01809e+05 2.00050e+04 6.50000e+01 7.00000e+01 7.50000e+01\n",
      "   1.00000e+00 7.50000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01810e+05 2.00050e+04 6.80000e+01 7.30000e+01 7.80000e+01\n",
      "   1.00000e+00 7.80000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01811e+05 2.00050e+04 7.10000e+01 7.60000e+01 8.10000e+01\n",
      "   1.00000e+00 8.10000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01812e+05 2.00050e+04 7.40000e+01 7.90000e+01 8.40000e+01\n",
      "   1.00000e+00 8.40000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01901e+05 2.00050e+04 7.70000e+01 8.20000e+01 8.70000e+01\n",
      "   1.00000e+00 8.70000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01902e+05 2.00050e+04 8.00000e+01 8.50000e+01 9.00000e+01\n",
      "   1.00000e+00 9.00000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01903e+05 2.00050e+04 8.30000e+01 8.80000e+01 9.30000e+01\n",
      "   1.00000e+00 9.30000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01904e+05 2.00050e+04 8.60000e+01 9.10000e+01 9.60000e+01\n",
      "   1.00000e+00 9.60000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01905e+05 2.00050e+04 8.90000e+01 9.40000e+01 9.90000e+01\n",
      "   1.00000e+00 9.90000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01906e+05 2.00050e+04 9.20000e+01 9.70000e+01 1.02000e+02\n",
      "   1.00000e+00 1.02000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(164, 17, 13) (164,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id    prediction\n",
      "0       20001   4946.114258\n",
      "1       20002   4931.047363\n",
      "2       20003   4927.149902\n",
      "3       20004      0.000000\n",
      "4       20005  11624.817383\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "Training model with 4 layers, 226 neurons, 6 steps.\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 2s 90ms/step - loss: 5820353.0000 - val_loss: 941322.0625\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 4962554.5000 - val_loss: 365172.5625\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 3727820.0000 - val_loss: 254892.3594\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 4041999.2500 - val_loss: 1564846.3750\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 4066055.2500 - val_loss: 503732.5938\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 3293813.5000 - val_loss: 1791688.3750\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 3059304.0000 - val_loss: 131650.2031\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 2648593.7500 - val_loss: 271381.2500\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2777999.2500 - val_loss: 246342.4219\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 3499814.0000 - val_loss: 209343.7344\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 2107257.5000 - val_loss: 1872820.5000\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 3123879.5000 - val_loss: 37943.3867\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 2495506.7500 - val_loss: 180569.7031\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 1963676.0000 - val_loss: 351832.2812\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 2395379.2500 - val_loss: 111180.4844\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2032245.3750 - val_loss: 374474.6250\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 1818132.6250 - val_loss: 159872.4531\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 1931789.0000 - val_loss: 740427.0625\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 2146831.5000 - val_loss: 120181.6953\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 1831101.6250 - val_loss: 119863.8750\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 1975675.0000 - val_loss: 332994.3125\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 1342766.3750 - val_loss: 893899.2500\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:46:54,751] Trial 4 finished with value: 0.3860861929686323 and parameters: {'n_layers': 4, 'n_neurons': 226, 'n_steps': 6, 'dropout_threshold': 0.6033691176250559}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(175, 6, 13) (175,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  165.837296\n",
      "1       20002  165.819504\n",
      "2       20003  165.848526\n",
      "3       20004  154.866455\n",
      "4       20005  822.428833\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "Training model with 3 layers, 91 neurons, 19 steps.\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 2s 69ms/step - loss: 908640768.0000 - val_loss: 2802764032.0000\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1253645056.0000 - val_loss: 596466048.0000\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1128641920.0000 - val_loss: 1104327040.0000\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1568970880.0000 - val_loss: 2168701184.0000\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1141237376.0000 - val_loss: 2529803008.0000\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1174767360.0000 - val_loss: 4230677248.0000\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 897252800.0000 - val_loss: 727574848.0000\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 713278272.0000 - val_loss: 1773858944.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1202536064.0000 - val_loss: 3841927168.0000\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 902707456.0000 - val_loss: 171317008.0000\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 548226944.0000 - val_loss: 128245680.0000\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 380323584.0000 - val_loss: 237073216.0000\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 709276416.0000 - val_loss: 296315808.0000\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 884796608.0000 - val_loss: 47547864.0000\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 876535232.0000 - val_loss: 811016448.0000\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 282291968.0000 - val_loss: 200213248.0000\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 559052864.0000 - val_loss: 268335808.0000\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 413782624.0000 - val_loss: 398980800.0000\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 134876752.0000 - val_loss: 49146144.0000\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 156589600.0000 - val_loss: 698585536.0000\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 208731568.0000 - val_loss: 44988876.0000\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 295690560.0000 - val_loss: 29477656.0000\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 290310336.0000 - val_loss: 182459520.0000\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 285291072.0000 - val_loss: 4833779.5000\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 259421632.0000 - val_loss: 201910128.0000\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 356641856.0000 - val_loss: 216210256.0000\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 219545520.0000 - val_loss: 164198480.0000\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 220668848.0000 - val_loss: 178490976.0000\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 243193904.0000 - val_loss: 308011616.0000\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 132754568.0000 - val_loss: 574075968.0000\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 118351248.0000 - val_loss: 163929936.0000\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 129684448.0000 - val_loss: 311848032.0000\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 118694280.0000 - val_loss: 280050944.0000\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 122067992.0000 - val_loss: 1402762880.0000\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01806e+05 2.00010e+04 1.80000e+01 1.90000e+01 2.00000e+01\n",
      "   1.00000e+00 4.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01807e+05 2.00010e+04 1.90000e+01 2.00000e+01 2.10000e+01\n",
      "   1.00000e+00 4.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01808e+05 2.00010e+04 2.00000e+01 2.10000e+01 2.20000e+01\n",
      "   1.00000e+00 4.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00010e+04 2.10000e+01 2.20000e+01 2.30000e+01\n",
      "   1.00000e+00 4.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00010e+04 2.20000e+01 2.30000e+01 2.40000e+01\n",
      "   1.00000e+00 4.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00010e+04 2.30000e+01 2.40000e+01 2.50000e+01\n",
      "   1.00000e+00 5.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00010e+04 2.40000e+01 2.50000e+01 2.60000e+01\n",
      "   1.00000e+00 5.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00010e+04 2.50000e+01 2.60000e+01 2.70000e+01\n",
      "   1.00000e+00 5.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00010e+04 2.60000e+01 2.70000e+01 2.80000e+01\n",
      "   1.00000e+00 5.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00010e+04 2.70000e+01 2.80000e+01 2.90000e+01\n",
      "   1.00000e+00 5.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00010e+04 2.80000e+01 2.90000e+01 3.00000e+01\n",
      "   1.00000e+00 6.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00010e+04 2.90000e+01 3.00000e+01 3.10000e+01\n",
      "   1.00000e+00 6.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00010e+04 3.00000e+01 3.10000e+01 3.20000e+01\n",
      "   1.00000e+00 6.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01806e+05 2.00020e+04 1.95000e+01 2.05000e+01 2.15000e+01\n",
      "   0.00000e+00 6.45000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01807e+05 2.00020e+04 2.05000e+01 2.15000e+01 2.25000e+01\n",
      "   0.00000e+00 6.75000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01808e+05 2.00020e+04 2.15000e+01 2.25000e+01 2.35000e+01\n",
      "   0.00000e+00 7.05000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00020e+04 2.25000e+01 2.35000e+01 2.45000e+01\n",
      "   0.00000e+00 7.35000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00020e+04 2.35000e+01 2.45000e+01 2.55000e+01\n",
      "   0.00000e+00 7.65000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00020e+04 2.45000e+01 2.55000e+01 2.65000e+01\n",
      "   0.00000e+00 7.95000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00020e+04 2.55000e+01 2.65000e+01 2.75000e+01\n",
      "   0.00000e+00 8.25000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00020e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   0.00000e+00 8.55000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00020e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   0.00000e+00 8.85000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00020e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   0.00000e+00 9.15000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00020e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   0.00000e+00 9.45000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00020e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   0.00000e+00 9.75000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00020e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   0.00000e+00 1.00500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:46:59,951] Trial 5 finished with value: 1.065791061966848 and parameters: {'n_layers': 3, 'n_neurons': 91, 'n_steps': 19, 'dropout_threshold': 0.4936380254537177}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01806e+05 2.00030e+04 1.75000e+01 1.85000e+01 1.95000e+01\n",
      "   1.00000e+00 3.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01807e+05 2.00030e+04 1.85000e+01 1.95000e+01 2.05000e+01\n",
      "   1.00000e+00 4.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01808e+05 2.00030e+04 1.95000e+01 2.05000e+01 2.15000e+01\n",
      "   1.00000e+00 4.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00030e+04 2.05000e+01 2.15000e+01 2.25000e+01\n",
      "   1.00000e+00 4.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00030e+04 2.15000e+01 2.25000e+01 2.35000e+01\n",
      "   1.00000e+00 4.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00030e+04 2.25000e+01 2.35000e+01 2.45000e+01\n",
      "   1.00000e+00 4.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00030e+04 2.35000e+01 2.45000e+01 2.55000e+01\n",
      "   1.00000e+00 5.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00030e+04 2.45000e+01 2.55000e+01 2.65000e+01\n",
      "   1.00000e+00 5.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00030e+04 2.55000e+01 2.65000e+01 2.75000e+01\n",
      "   1.00000e+00 5.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00030e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   1.00000e+00 5.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00030e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   1.00000e+00 5.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00030e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   1.00000e+00 6.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00030e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   1.00000e+00 6.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01806e+05 2.00040e+04 1.80000e+02 1.90000e+02 2.00000e+02\n",
      "   0.00000e+00 1.00000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01807e+05 2.00040e+04 1.90000e+02 2.00000e+02 2.10000e+02\n",
      "   0.00000e+00 1.05000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01808e+05 2.00040e+04 2.00000e+02 2.10000e+02 2.20000e+02\n",
      "   0.00000e+00 1.10000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00040e+04 2.10000e+02 2.20000e+02 2.30000e+02\n",
      "   0.00000e+00 1.15000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00040e+04 2.20000e+02 2.30000e+02 2.40000e+02\n",
      "   0.00000e+00 1.20000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00040e+04 2.30000e+02 2.40000e+02 2.50000e+02\n",
      "   0.00000e+00 1.25000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00040e+04 2.40000e+02 2.50000e+02 2.60000e+02\n",
      "   0.00000e+00 1.30000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00040e+04 2.50000e+02 2.60000e+02 2.70000e+02\n",
      "   0.00000e+00 1.35000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00040e+04 2.60000e+02 2.70000e+02 2.80000e+02\n",
      "   0.00000e+00 1.40000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00040e+04 2.70000e+02 2.80000e+02 2.90000e+02\n",
      "   0.00000e+00 1.45000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00040e+04 2.80000e+02 2.90000e+02 3.00000e+02\n",
      "   0.00000e+00 1.50000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00040e+04 2.90000e+02 3.00000e+02 3.10000e+02\n",
      "   0.00000e+00 1.55000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00040e+04 3.00000e+02 3.10000e+02 3.20000e+02\n",
      "   0.00000e+00 1.60000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01806e+05 2.00050e+04 5.60000e+01 6.10000e+01 6.60000e+01\n",
      "   1.00000e+00 6.60000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01807e+05 2.00050e+04 5.90000e+01 6.40000e+01 6.90000e+01\n",
      "   1.00000e+00 6.90000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01808e+05 2.00050e+04 6.20000e+01 6.70000e+01 7.20000e+01\n",
      "   1.00000e+00 7.20000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01809e+05 2.00050e+04 6.50000e+01 7.00000e+01 7.50000e+01\n",
      "   1.00000e+00 7.50000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01810e+05 2.00050e+04 6.80000e+01 7.30000e+01 7.80000e+01\n",
      "   1.00000e+00 7.80000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01811e+05 2.00050e+04 7.10000e+01 7.60000e+01 8.10000e+01\n",
      "   1.00000e+00 8.10000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01812e+05 2.00050e+04 7.40000e+01 7.90000e+01 8.40000e+01\n",
      "   1.00000e+00 8.40000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01901e+05 2.00050e+04 7.70000e+01 8.20000e+01 8.70000e+01\n",
      "   1.00000e+00 8.70000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01902e+05 2.00050e+04 8.00000e+01 8.50000e+01 9.00000e+01\n",
      "   1.00000e+00 9.00000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01903e+05 2.00050e+04 8.30000e+01 8.80000e+01 9.30000e+01\n",
      "   1.00000e+00 9.30000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01904e+05 2.00050e+04 8.60000e+01 9.10000e+01 9.60000e+01\n",
      "   1.00000e+00 9.60000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01905e+05 2.00050e+04 8.90000e+01 9.40000e+01 9.90000e+01\n",
      "   1.00000e+00 9.90000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01906e+05 2.00050e+04 9.20000e+01 9.70000e+01 1.02000e+02\n",
      "   1.00000e+00 1.02000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(162, 19, 13) (162,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001    0.000000\n",
      "1       20002    0.000000\n",
      "2       20003    0.000000\n",
      "3       20004  489.180267\n",
      "4       20005    0.000000\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "Training model with 3 layers, 35 neurons, 14 steps.\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 2s 66ms/step - loss: 616165568.0000 - val_loss: 535994944.0000\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 234137200.0000 - val_loss: 72466272.0000\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 331563712.0000 - val_loss: 78049560.0000\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 224773360.0000 - val_loss: 95155960.0000\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 278592320.0000 - val_loss: 58641296.0000\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 254876640.0000 - val_loss: 13916625.0000\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 185483536.0000 - val_loss: 19678260.0000\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 196334048.0000 - val_loss: 20139602.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 135912272.0000 - val_loss: 90050064.0000\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 135368544.0000 - val_loss: 83124680.0000\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 252567088.0000 - val_loss: 76906320.0000\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 179710848.0000 - val_loss: 4639787.0000\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 169084224.0000 - val_loss: 25205924.0000\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 107176408.0000 - val_loss: 2938721.7500\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 130094864.0000 - val_loss: 3297451.0000\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 109254560.0000 - val_loss: 2688944.0000\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 77094472.0000 - val_loss: 1303431.2500\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 97509120.0000 - val_loss: 531132.5625\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 83312008.0000 - val_loss: 25510952.0000\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 97098088.0000 - val_loss: 20414468.0000\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 68214520.0000 - val_loss: 4485415.0000\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 82176944.0000 - val_loss: 183999.2656\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 108172208.0000 - val_loss: 19318452.0000\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 60037504.0000 - val_loss: 286910.1562\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 62037828.0000 - val_loss: 535301.5000\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 119371616.0000 - val_loss: 1877568.8750\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 78052800.0000 - val_loss: 20863940.0000\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 85368072.0000 - val_loss: 23481850.0000\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 78929904.0000 - val_loss: 16131587.0000\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 59235760.0000 - val_loss: 2011359.0000\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 81038600.0000 - val_loss: 651384.8750\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 72097000.0000 - val_loss: 470819.2500\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01811e+05 2.00010e+04 2.30000e+01 2.40000e+01 2.50000e+01\n",
      "   1.00000e+00 5.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00010e+04 2.40000e+01 2.50000e+01 2.60000e+01\n",
      "   1.00000e+00 5.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00010e+04 2.50000e+01 2.60000e+01 2.70000e+01\n",
      "   1.00000e+00 5.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00010e+04 2.60000e+01 2.70000e+01 2.80000e+01\n",
      "   1.00000e+00 5.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00010e+04 2.70000e+01 2.80000e+01 2.90000e+01\n",
      "   1.00000e+00 5.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00010e+04 2.80000e+01 2.90000e+01 3.00000e+01\n",
      "   1.00000e+00 6.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00010e+04 2.90000e+01 3.00000e+01 3.10000e+01\n",
      "   1.00000e+00 6.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00010e+04 3.00000e+01 3.10000e+01 3.20000e+01\n",
      "   1.00000e+00 6.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:47:04,423] Trial 6 finished with value: 1.0 and parameters: {'n_layers': 3, 'n_neurons': 35, 'n_steps': 14, 'dropout_threshold': 0.3572529365398843}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01811e+05 2.00020e+04 2.45000e+01 2.55000e+01 2.65000e+01\n",
      "   0.00000e+00 7.95000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00020e+04 2.55000e+01 2.65000e+01 2.75000e+01\n",
      "   0.00000e+00 8.25000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00020e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   0.00000e+00 8.55000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00020e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   0.00000e+00 8.85000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00020e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   0.00000e+00 9.15000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00020e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   0.00000e+00 9.45000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00020e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   0.00000e+00 9.75000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00020e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   0.00000e+00 1.00500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01811e+05 2.00030e+04 2.25000e+01 2.35000e+01 2.45000e+01\n",
      "   1.00000e+00 4.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00030e+04 2.35000e+01 2.45000e+01 2.55000e+01\n",
      "   1.00000e+00 5.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00030e+04 2.45000e+01 2.55000e+01 2.65000e+01\n",
      "   1.00000e+00 5.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00030e+04 2.55000e+01 2.65000e+01 2.75000e+01\n",
      "   1.00000e+00 5.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00030e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   1.00000e+00 5.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00030e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   1.00000e+00 5.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00030e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   1.00000e+00 6.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00030e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   1.00000e+00 6.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01811e+05 2.00040e+04 2.30000e+02 2.40000e+02 2.50000e+02\n",
      "   0.00000e+00 1.25000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00040e+04 2.40000e+02 2.50000e+02 2.60000e+02\n",
      "   0.00000e+00 1.30000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00040e+04 2.50000e+02 2.60000e+02 2.70000e+02\n",
      "   0.00000e+00 1.35000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00040e+04 2.60000e+02 2.70000e+02 2.80000e+02\n",
      "   0.00000e+00 1.40000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00040e+04 2.70000e+02 2.80000e+02 2.90000e+02\n",
      "   0.00000e+00 1.45000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00040e+04 2.80000e+02 2.90000e+02 3.00000e+02\n",
      "   0.00000e+00 1.50000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00040e+04 2.90000e+02 3.00000e+02 3.10000e+02\n",
      "   0.00000e+00 1.55000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00040e+04 3.00000e+02 3.10000e+02 3.20000e+02\n",
      "   0.00000e+00 1.60000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01811e+05 2.00050e+04 7.10000e+01 7.60000e+01 8.10000e+01\n",
      "   1.00000e+00 8.10000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01812e+05 2.00050e+04 7.40000e+01 7.90000e+01 8.40000e+01\n",
      "   1.00000e+00 8.40000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01901e+05 2.00050e+04 7.70000e+01 8.20000e+01 8.70000e+01\n",
      "   1.00000e+00 8.70000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01902e+05 2.00050e+04 8.00000e+01 8.50000e+01 9.00000e+01\n",
      "   1.00000e+00 9.00000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01903e+05 2.00050e+04 8.30000e+01 8.80000e+01 9.30000e+01\n",
      "   1.00000e+00 9.30000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01904e+05 2.00050e+04 8.60000e+01 9.10000e+01 9.60000e+01\n",
      "   1.00000e+00 9.60000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01905e+05 2.00050e+04 8.90000e+01 9.40000e+01 9.90000e+01\n",
      "   1.00000e+00 9.90000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01906e+05 2.00050e+04 9.20000e+01 9.70000e+01 1.02000e+02\n",
      "   1.00000e+00 1.02000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(167, 14, 13) (167,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001         0.0\n",
      "1       20002         0.0\n",
      "2       20003         0.0\n",
      "3       20004         0.0\n",
      "4       20005         0.0\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "Training model with 1 layers, 472 neurons, 10 steps.\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 39ms/step - loss: 2201400576.0000 - val_loss: 33438686.0000\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 91441936.0000 - val_loss: 316452640.0000\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 116275768.0000 - val_loss: 19869836.0000\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 24421618.0000 - val_loss: 25975812.0000\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 147620256.0000 - val_loss: 189206384.0000\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 297172768.0000 - val_loss: 472870080.0000\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 428390336.0000 - val_loss: 75150608.0000\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 94072592.0000 - val_loss: 1323449.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 55461100.0000 - val_loss: 45482396.0000\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 65990532.0000 - val_loss: 5568674.0000\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1256868.8750 - val_loss: 298691.4062\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 12594630.0000 - val_loss: 12600169.0000\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 15102287.0000 - val_loss: 8875921.0000\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3653539.2500 - val_loss: 699384.7500\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2067792.2500 - val_loss: 132316.6406\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3201863.5000 - val_loss: 2560066.5000\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3141835.5000 - val_loss: 7420187.0000\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4770480.5000 - val_loss: 235523.4844\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3343396.5000 - val_loss: 983318.1875\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2555436.5000 - val_loss: 1002572.9375\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 681051.5625 - val_loss: 107578.1016\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 613795.1250 - val_loss: 700963.1875\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 854007.1875 - val_loss: 519587.0312\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 267880.6250 - val_loss: 106906.6094\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 195695.6094 - val_loss: 240481.7656\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 147922.9531 - val_loss: 99351.3594\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 106875.1250 - val_loss: 121628.2969\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 97902.8750 - val_loss: 88310.5547\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 92107.7578 - val_loss: 97930.2578\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 87211.0312 - val_loss: 88108.1250\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 85583.6016 - val_loss: 86911.4609\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 81854.0312 - val_loss: 86054.2500\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 82501.8359 - val_loss: 86759.6172\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 82840.0938 - val_loss: 85767.0469\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 82478.8984 - val_loss: 85778.9219\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 82244.2969 - val_loss: 85691.3203\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 82451.3359 - val_loss: 85634.6016\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 81780.2656 - val_loss: 86026.8672\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 83964.8125 - val_loss: 86431.1406\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 81509.0781 - val_loss: 85591.6641\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 82361.6953 - val_loss: 86316.8281\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 84289.9844 - val_loss: 86434.6875\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 82110.8203 - val_loss: 85614.7109\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 82192.5000 - val_loss: 85378.1562\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 81629.1719 - val_loss: 85329.8281\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 82421.0000 - val_loss: 85368.4609\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 81188.4062 - val_loss: 85610.8047\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 83088.2969 - val_loss: 85311.5703\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 82983.3047 - val_loss: 86183.5781\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 81440.0859 - val_loss: 85466.7891\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 84239.4766 - val_loss: 85099.0391\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 85023.3594 - val_loss: 87153.8438\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 85641.1250 - val_loss: 86592.9766\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 81296.3359 - val_loss: 85492.2578\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 82896.9453 - val_loss: 85640.7578\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 81071.2656 - val_loss: 85213.7734\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 81803.3594 - val_loss: 84833.1094\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 81213.5781 - val_loss: 84793.8047\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 81437.4766 - val_loss: 84769.1016\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 81604.1484 - val_loss: 84706.4766\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 81571.9766 - val_loss: 84850.3828\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 80825.8984 - val_loss: 84838.1641\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 82704.2656 - val_loss: 85182.2266\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 80756.3750 - val_loss: 84622.4219\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 82904.8984 - val_loss: 85327.9531\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 81039.3359 - val_loss: 86369.6562\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 83069.7734 - val_loss: 84661.7188\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 80985.6016 - val_loss: 84569.9766\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 81110.7656 - val_loss: 84249.1953\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 80642.1016 - val_loss: 84264.6797\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 81471.6641 - val_loss: 84987.9766\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 81985.5859 - val_loss: 84893.7109\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 80353.8047 - val_loss: 84537.8359\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 80714.6641 - val_loss: 84110.3750\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 82471.1562 - val_loss: 83996.7422\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 81811.5078 - val_loss: 87036.3984\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 81497.3125 - val_loss: 85003.2422\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 83395.1094 - val_loss: 84717.3125\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 80721.3203 - val_loss: 84108.9375\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 79879.8281 - val_loss: 83917.4531\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 82184.3281 - val_loss: 83830.8203\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 80344.1953 - val_loss: 83788.9453\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 82374.4922 - val_loss: 84693.0859\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 80720.4453 - val_loss: 83725.3672\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 80775.7969 - val_loss: 83487.4062\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 79917.6641 - val_loss: 83421.8203\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 80790.4844 - val_loss: 83476.5547\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 80121.7656 - val_loss: 84510.9531\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 80127.9453 - val_loss: 83428.2031\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 79981.6562 - val_loss: 83099.9688\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 80125.4453 - val_loss: 83140.3203\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 80314.3359 - val_loss: 83082.0938\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 82938.3438 - val_loss: 83023.9766\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 85370.3984 - val_loss: 84630.0391\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 85916.3125 - val_loss: 85911.9453\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 78481.9141 - val_loss: 87882.1953\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 81292.5234 - val_loss: 82822.1641\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 82144.7656 - val_loss: 82913.8359\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 81781.7656 - val_loss: 84527.8359\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 80043.7109 - val_loss: 84251.7500\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 81325.2578 - val_loss: 83886.9844\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 79381.3828 - val_loss: 82963.6484\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 79730.6250 - val_loss: 82370.5000\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 79814.4453 - val_loss: 82250.0156\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 80421.9844 - val_loss: 83155.3125\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 80868.1250 - val_loss: 85746.9609\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 81377.4453 - val_loss: 85077.3047\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 79669.6641 - val_loss: 84710.2188\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 80054.3359 - val_loss: 81934.2500\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 79960.6484 - val_loss: 81873.5938\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 81229.1250 - val_loss: 82652.4844\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 77723.9453 - val_loss: 83386.8203\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 78885.5781 - val_loss: 82207.0000\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 81660.7422 - val_loss: 81826.2188\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 77958.8672 - val_loss: 81814.2266\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 78973.0078 - val_loss: 81675.0703\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 80339.3359 - val_loss: 81874.6953\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 77564.4219 - val_loss: 81585.7422\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 79957.1016 - val_loss: 81343.7500\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 79708.4141 - val_loss: 81436.9766\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 78613.5312 - val_loss: 81421.7188\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 77821.2422 - val_loss: 82238.2969\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 77593.5469 - val_loss: 81560.0156\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 79036.4141 - val_loss: 81130.3984\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 77426.6406 - val_loss: 82551.3750\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 78460.4219 - val_loss: 80759.3203\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 77518.6328 - val_loss: 80900.3984\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 77532.6719 - val_loss: 80933.1094\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 78021.7344 - val_loss: 80668.3047\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 79577.4453 - val_loss: 80584.0312\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 77964.5234 - val_loss: 80462.6016\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 76698.0469 - val_loss: 80893.5625\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 77564.9297 - val_loss: 80471.0156\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 77017.5000 - val_loss: 80521.8984\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 76781.8516 - val_loss: 80310.1172\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 77554.5859 - val_loss: 81090.0312\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 78362.8594 - val_loss: 80183.8047\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 79214.3594 - val_loss: 80114.7422\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 78064.7578 - val_loss: 82174.4531\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 76113.1484 - val_loss: 81788.1094\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 78323.3516 - val_loss: 79671.6875\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 76727.8594 - val_loss: 79575.6094\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 76959.0234 - val_loss: 79491.7422\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 76495.3359 - val_loss: 79496.6953\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 77585.9531 - val_loss: 79364.7422\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 76262.3047 - val_loss: 79604.9141\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 76770.9844 - val_loss: 79826.7656\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 78798.3516 - val_loss: 79131.4609\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 75693.3047 - val_loss: 79468.1641\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 75870.5156 - val_loss: 79025.3203\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 76064.4922 - val_loss: 79150.3359\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 77674.3125 - val_loss: 79533.1953\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 74976.5547 - val_loss: 79993.6016\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 76053.1094 - val_loss: 78619.7266\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 75741.6797 - val_loss: 78616.0469\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 75450.4609 - val_loss: 78485.5781\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 76671.3672 - val_loss: 78571.5703\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 77185.7578 - val_loss: 78705.0078\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 75979.3359 - val_loss: 78226.4219\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 77004.8984 - val_loss: 78371.8516\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 80487.2969 - val_loss: 78254.1250\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 86240.8984 - val_loss: 78290.8672\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 80646.2344 - val_loss: 78641.9766\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 84651.8594 - val_loss: 77871.9531\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 79485.1797 - val_loss: 78108.3047\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 74690.2969 - val_loss: 77796.0469\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 75085.1953 - val_loss: 77651.8594\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 78405.7422 - val_loss: 80608.2578\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 80424.3359 - val_loss: 77321.9297\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 77625.0625 - val_loss: 77477.7422\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 76010.4766 - val_loss: 77188.5234\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 76682.8594 - val_loss: 77060.1719\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 76441.2266 - val_loss: 76967.6406\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 78889.8828 - val_loss: 77109.7422\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 74242.6797 - val_loss: 76906.8672\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 74005.5156 - val_loss: 76975.6797\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 75320.2812 - val_loss: 77167.6875\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 75391.9375 - val_loss: 77348.5156\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 74606.5859 - val_loss: 76659.0469\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 76139.3438 - val_loss: 76330.8281\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 76141.8281 - val_loss: 76269.4375\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 82994.6406 - val_loss: 76265.5078\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 85202.8047 - val_loss: 76178.1328\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 80246.7656 - val_loss: 76104.4062\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 82766.8359 - val_loss: 75916.3125\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 80567.8516 - val_loss: 75994.4609\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 79967.4297 - val_loss: 76511.9844\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 84180.6250 - val_loss: 75932.4922\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 74748.4453 - val_loss: 76633.5859\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 72924.2266 - val_loss: 77070.8672\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 74589.4688 - val_loss: 75763.6250\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 74549.5000 - val_loss: 75165.7500\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 73458.4062 - val_loss: 75084.6641\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 75992.5781 - val_loss: 75005.4062\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 76514.3750 - val_loss: 74873.2812\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 87408.5703 - val_loss: 74853.9844\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 88970.2031 - val_loss: 74806.5938\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 80017.9766 - val_loss: 74789.2500\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 84060.9375 - val_loss: 75234.3359\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 75367.2422 - val_loss: 74571.6797\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01903e+05 2.00010e+04 2.70000e+01 2.80000e+01 2.90000e+01\n",
      "   1.00000e+00 5.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00010e+04 2.80000e+01 2.90000e+01 3.00000e+01\n",
      "   1.00000e+00 6.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00010e+04 2.90000e+01 3.00000e+01 3.10000e+01\n",
      "   1.00000e+00 6.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00010e+04 3.00000e+01 3.10000e+01 3.20000e+01\n",
      "   1.00000e+00 6.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01903e+05 2.00020e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   0.00000e+00 9.15000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00020e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   0.00000e+00 9.45000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00020e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   0.00000e+00 9.75000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00020e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   0.00000e+00 1.00500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01903e+05 2.00030e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   1.00000e+00 5.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00030e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   1.00000e+00 5.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00030e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   1.00000e+00 6.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00030e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   1.00000e+00 6.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:47:18,541] Trial 7 finished with value: 0.7668596148742326 and parameters: {'n_layers': 1, 'n_neurons': 472, 'n_steps': 10, 'dropout_threshold': 0.4869581179601722}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01903e+05 2.00040e+04 2.70000e+02 2.80000e+02 2.90000e+02\n",
      "   0.00000e+00 1.45000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00040e+04 2.80000e+02 2.90000e+02 3.00000e+02\n",
      "   0.00000e+00 1.50000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00040e+04 2.90000e+02 3.00000e+02 3.10000e+02\n",
      "   0.00000e+00 1.55000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00040e+04 3.00000e+02 3.10000e+02 3.20000e+02\n",
      "   0.00000e+00 1.60000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01903e+05 2.00050e+04 8.30000e+01 8.80000e+01 9.30000e+01\n",
      "   1.00000e+00 9.30000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01904e+05 2.00050e+04 8.60000e+01 9.10000e+01 9.60000e+01\n",
      "   1.00000e+00 9.60000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01905e+05 2.00050e+04 8.90000e+01 9.40000e+01 9.90000e+01\n",
      "   1.00000e+00 9.90000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01906e+05 2.00050e+04 9.20000e+01 9.70000e+01 1.02000e+02\n",
      "   1.00000e+00 1.02000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(171, 10, 13) (171,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  166.621780\n",
      "1       20002  170.346832\n",
      "2       20003  166.989471\n",
      "3       20004  306.565979\n",
      "4       20005  278.420532\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "Training model with 4 layers, 326 neurons, 19 steps.\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 2s 126ms/step - loss: 102362032.0000 - val_loss: 87669856.0000\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 62ms/step - loss: 159134112.0000 - val_loss: 80359312.0000\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 199245024.0000 - val_loss: 488467552.0000\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 305720864.0000 - val_loss: 259815584.0000\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 423946464.0000 - val_loss: 957421440.0000\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 402801600.0000 - val_loss: 48540644.0000\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 232901184.0000 - val_loss: 4465936384.0000\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 641591616.0000 - val_loss: 1301992448.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 690837568.0000 - val_loss: 1067753024.0000\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 61ms/step - loss: 944625984.0000 - val_loss: 1723551360.0000\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 60ms/step - loss: nan - val_loss: nan\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 61ms/step - loss: nan - val_loss: nan\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 60ms/step - loss: nan - val_loss: nan\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 59ms/step - loss: nan - val_loss: nan\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 60ms/step - loss: nan - val_loss: nan\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 61ms/step - loss: nan - val_loss: nan\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01806e+05 2.00010e+04 1.80000e+01 1.90000e+01 2.00000e+01\n",
      "   1.00000e+00 4.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01807e+05 2.00010e+04 1.90000e+01 2.00000e+01 2.10000e+01\n",
      "   1.00000e+00 4.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01808e+05 2.00010e+04 2.00000e+01 2.10000e+01 2.20000e+01\n",
      "   1.00000e+00 4.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00010e+04 2.10000e+01 2.20000e+01 2.30000e+01\n",
      "   1.00000e+00 4.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00010e+04 2.20000e+01 2.30000e+01 2.40000e+01\n",
      "   1.00000e+00 4.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00010e+04 2.30000e+01 2.40000e+01 2.50000e+01\n",
      "   1.00000e+00 5.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00010e+04 2.40000e+01 2.50000e+01 2.60000e+01\n",
      "   1.00000e+00 5.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00010e+04 2.50000e+01 2.60000e+01 2.70000e+01\n",
      "   1.00000e+00 5.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00010e+04 2.60000e+01 2.70000e+01 2.80000e+01\n",
      "   1.00000e+00 5.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00010e+04 2.70000e+01 2.80000e+01 2.90000e+01\n",
      "   1.00000e+00 5.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00010e+04 2.80000e+01 2.90000e+01 3.00000e+01\n",
      "   1.00000e+00 6.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00010e+04 2.90000e+01 3.00000e+01 3.10000e+01\n",
      "   1.00000e+00 6.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00010e+04 3.00000e+01 3.10000e+01 3.20000e+01\n",
      "   1.00000e+00 6.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:47:25,752] Trial 8 finished with value: 8.750970898496346 and parameters: {'n_layers': 4, 'n_neurons': 326, 'n_steps': 19, 'dropout_threshold': 0.5455693300660107}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01806e+05 2.00020e+04 1.95000e+01 2.05000e+01 2.15000e+01\n",
      "   0.00000e+00 6.45000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01807e+05 2.00020e+04 2.05000e+01 2.15000e+01 2.25000e+01\n",
      "   0.00000e+00 6.75000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01808e+05 2.00020e+04 2.15000e+01 2.25000e+01 2.35000e+01\n",
      "   0.00000e+00 7.05000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00020e+04 2.25000e+01 2.35000e+01 2.45000e+01\n",
      "   0.00000e+00 7.35000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00020e+04 2.35000e+01 2.45000e+01 2.55000e+01\n",
      "   0.00000e+00 7.65000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00020e+04 2.45000e+01 2.55000e+01 2.65000e+01\n",
      "   0.00000e+00 7.95000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00020e+04 2.55000e+01 2.65000e+01 2.75000e+01\n",
      "   0.00000e+00 8.25000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00020e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   0.00000e+00 8.55000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00020e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   0.00000e+00 8.85000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00020e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   0.00000e+00 9.15000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00020e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   0.00000e+00 9.45000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00020e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   0.00000e+00 9.75000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00020e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   0.00000e+00 1.00500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01806e+05 2.00030e+04 1.75000e+01 1.85000e+01 1.95000e+01\n",
      "   1.00000e+00 3.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01807e+05 2.00030e+04 1.85000e+01 1.95000e+01 2.05000e+01\n",
      "   1.00000e+00 4.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01808e+05 2.00030e+04 1.95000e+01 2.05000e+01 2.15000e+01\n",
      "   1.00000e+00 4.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00030e+04 2.05000e+01 2.15000e+01 2.25000e+01\n",
      "   1.00000e+00 4.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00030e+04 2.15000e+01 2.25000e+01 2.35000e+01\n",
      "   1.00000e+00 4.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00030e+04 2.25000e+01 2.35000e+01 2.45000e+01\n",
      "   1.00000e+00 4.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00030e+04 2.35000e+01 2.45000e+01 2.55000e+01\n",
      "   1.00000e+00 5.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00030e+04 2.45000e+01 2.55000e+01 2.65000e+01\n",
      "   1.00000e+00 5.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00030e+04 2.55000e+01 2.65000e+01 2.75000e+01\n",
      "   1.00000e+00 5.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00030e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   1.00000e+00 5.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00030e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   1.00000e+00 5.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00030e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   1.00000e+00 6.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00030e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   1.00000e+00 6.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01806e+05 2.00040e+04 1.80000e+02 1.90000e+02 2.00000e+02\n",
      "   0.00000e+00 1.00000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01807e+05 2.00040e+04 1.90000e+02 2.00000e+02 2.10000e+02\n",
      "   0.00000e+00 1.05000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01808e+05 2.00040e+04 2.00000e+02 2.10000e+02 2.20000e+02\n",
      "   0.00000e+00 1.10000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00040e+04 2.10000e+02 2.20000e+02 2.30000e+02\n",
      "   0.00000e+00 1.15000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00040e+04 2.20000e+02 2.30000e+02 2.40000e+02\n",
      "   0.00000e+00 1.20000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00040e+04 2.30000e+02 2.40000e+02 2.50000e+02\n",
      "   0.00000e+00 1.25000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00040e+04 2.40000e+02 2.50000e+02 2.60000e+02\n",
      "   0.00000e+00 1.30000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00040e+04 2.50000e+02 2.60000e+02 2.70000e+02\n",
      "   0.00000e+00 1.35000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00040e+04 2.60000e+02 2.70000e+02 2.80000e+02\n",
      "   0.00000e+00 1.40000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00040e+04 2.70000e+02 2.80000e+02 2.90000e+02\n",
      "   0.00000e+00 1.45000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00040e+04 2.80000e+02 2.90000e+02 3.00000e+02\n",
      "   0.00000e+00 1.50000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00040e+04 2.90000e+02 3.00000e+02 3.10000e+02\n",
      "   0.00000e+00 1.55000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00040e+04 3.00000e+02 3.10000e+02 3.20000e+02\n",
      "   0.00000e+00 1.60000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01806e+05 2.00050e+04 5.60000e+01 6.10000e+01 6.60000e+01\n",
      "   1.00000e+00 6.60000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01807e+05 2.00050e+04 5.90000e+01 6.40000e+01 6.90000e+01\n",
      "   1.00000e+00 6.90000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01808e+05 2.00050e+04 6.20000e+01 6.70000e+01 7.20000e+01\n",
      "   1.00000e+00 7.20000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01809e+05 2.00050e+04 6.50000e+01 7.00000e+01 7.50000e+01\n",
      "   1.00000e+00 7.50000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01810e+05 2.00050e+04 6.80000e+01 7.30000e+01 7.80000e+01\n",
      "   1.00000e+00 7.80000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01811e+05 2.00050e+04 7.10000e+01 7.60000e+01 8.10000e+01\n",
      "   1.00000e+00 8.10000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01812e+05 2.00050e+04 7.40000e+01 7.90000e+01 8.40000e+01\n",
      "   1.00000e+00 8.40000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01901e+05 2.00050e+04 7.70000e+01 8.20000e+01 8.70000e+01\n",
      "   1.00000e+00 8.70000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01902e+05 2.00050e+04 8.00000e+01 8.50000e+01 9.00000e+01\n",
      "   1.00000e+00 9.00000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01903e+05 2.00050e+04 8.30000e+01 8.80000e+01 9.30000e+01\n",
      "   1.00000e+00 9.30000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01904e+05 2.00050e+04 8.60000e+01 9.10000e+01 9.60000e+01\n",
      "   1.00000e+00 9.60000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01905e+05 2.00050e+04 8.90000e+01 9.40000e+01 9.90000e+01\n",
      "   1.00000e+00 9.90000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01906e+05 2.00050e+04 9.20000e+01 9.70000e+01 1.02000e+02\n",
      "   1.00000e+00 1.02000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(162, 19, 13) (162,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001  2111.711426\n",
      "1       20002  1182.712769\n",
      "2       20003  4209.928223\n",
      "3       20004  2481.437500\n",
      "4       20005  6195.946289\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "Training model with 2 layers, 324 neurons, 16 steps.\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 63ms/step - loss: 831130880.0000 - val_loss: 2026298112.0000\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 499385312.0000 - val_loss: 883725696.0000\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 845852096.0000 - val_loss: 10081586176.0000\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 727771584.0000 - val_loss: 413618720.0000\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 594774848.0000 - val_loss: 866360832.0000\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 491556096.0000 - val_loss: 1840843648.0000\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 430895968.0000 - val_loss: 1508986368.0000\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 399003936.0000 - val_loss: 91298720.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 221220256.0000 - val_loss: 343075616.0000\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 328654432.0000 - val_loss: 649853376.0000\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 477739104.0000 - val_loss: 237782288.0000\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 416173632.0000 - val_loss: 2266814976.0000\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 699094720.0000 - val_loss: 6118402048.0000\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 962843200.0000 - val_loss: 756250112.0000\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 1024736064.0000 - val_loss: 3283612672.0000\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 1483506816.0000 - val_loss: 7501212672.0000\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 912035968.0000 - val_loss: 1878098944.0000\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 723870336.0000 - val_loss: 1407299200.0000\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01809e+05 2.00010e+04 2.10000e+01 2.20000e+01 2.30000e+01\n",
      "   1.00000e+00 4.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00010e+04 2.20000e+01 2.30000e+01 2.40000e+01\n",
      "   1.00000e+00 4.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00010e+04 2.30000e+01 2.40000e+01 2.50000e+01\n",
      "   1.00000e+00 5.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00010e+04 2.40000e+01 2.50000e+01 2.60000e+01\n",
      "   1.00000e+00 5.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00010e+04 2.50000e+01 2.60000e+01 2.70000e+01\n",
      "   1.00000e+00 5.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00010e+04 2.60000e+01 2.70000e+01 2.80000e+01\n",
      "   1.00000e+00 5.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00010e+04 2.70000e+01 2.80000e+01 2.90000e+01\n",
      "   1.00000e+00 5.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00010e+04 2.80000e+01 2.90000e+01 3.00000e+01\n",
      "   1.00000e+00 6.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00010e+04 2.90000e+01 3.00000e+01 3.10000e+01\n",
      "   1.00000e+00 6.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00010e+04 3.00000e+01 3.10000e+01 3.20000e+01\n",
      "   1.00000e+00 6.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01809e+05 2.00020e+04 2.25000e+01 2.35000e+01 2.45000e+01\n",
      "   0.00000e+00 7.35000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00020e+04 2.35000e+01 2.45000e+01 2.55000e+01\n",
      "   0.00000e+00 7.65000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00020e+04 2.45000e+01 2.55000e+01 2.65000e+01\n",
      "   0.00000e+00 7.95000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00020e+04 2.55000e+01 2.65000e+01 2.75000e+01\n",
      "   0.00000e+00 8.25000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00020e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   0.00000e+00 8.55000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00020e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   0.00000e+00 8.85000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00020e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   0.00000e+00 9.15000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00020e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   0.00000e+00 9.45000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00020e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   0.00000e+00 9.75000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00020e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   0.00000e+00 1.00500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01809e+05 2.00030e+04 2.05000e+01 2.15000e+01 2.25000e+01\n",
      "   1.00000e+00 4.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00030e+04 2.15000e+01 2.25000e+01 2.35000e+01\n",
      "   1.00000e+00 4.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00030e+04 2.25000e+01 2.35000e+01 2.45000e+01\n",
      "   1.00000e+00 4.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00030e+04 2.35000e+01 2.45000e+01 2.55000e+01\n",
      "   1.00000e+00 5.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00030e+04 2.45000e+01 2.55000e+01 2.65000e+01\n",
      "   1.00000e+00 5.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00030e+04 2.55000e+01 2.65000e+01 2.75000e+01\n",
      "   1.00000e+00 5.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00030e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   1.00000e+00 5.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00030e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   1.00000e+00 5.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00030e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   1.00000e+00 6.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00030e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   1.00000e+00 6.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01809e+05 2.00040e+04 2.10000e+02 2.20000e+02 2.30000e+02\n",
      "   0.00000e+00 1.15000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00040e+04 2.20000e+02 2.30000e+02 2.40000e+02\n",
      "   0.00000e+00 1.20000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00040e+04 2.30000e+02 2.40000e+02 2.50000e+02\n",
      "   0.00000e+00 1.25000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00040e+04 2.40000e+02 2.50000e+02 2.60000e+02\n",
      "   0.00000e+00 1.30000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00040e+04 2.50000e+02 2.60000e+02 2.70000e+02\n",
      "   0.00000e+00 1.35000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00040e+04 2.60000e+02 2.70000e+02 2.80000e+02\n",
      "   0.00000e+00 1.40000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00040e+04 2.70000e+02 2.80000e+02 2.90000e+02\n",
      "   0.00000e+00 1.45000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00040e+04 2.80000e+02 2.90000e+02 3.00000e+02\n",
      "   0.00000e+00 1.50000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00040e+04 2.90000e+02 3.00000e+02 3.10000e+02\n",
      "   0.00000e+00 1.55000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00040e+04 3.00000e+02 3.10000e+02 3.20000e+02\n",
      "   0.00000e+00 1.60000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:47:29,526] Trial 9 finished with value: 2.7771294005630462 and parameters: {'n_layers': 2, 'n_neurons': 324, 'n_steps': 16, 'dropout_threshold': 0.5605608168474211}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01809e+05 2.00050e+04 6.50000e+01 7.00000e+01 7.50000e+01\n",
      "   1.00000e+00 7.50000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01810e+05 2.00050e+04 6.80000e+01 7.30000e+01 7.80000e+01\n",
      "   1.00000e+00 7.80000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01811e+05 2.00050e+04 7.10000e+01 7.60000e+01 8.10000e+01\n",
      "   1.00000e+00 8.10000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01812e+05 2.00050e+04 7.40000e+01 7.90000e+01 8.40000e+01\n",
      "   1.00000e+00 8.40000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01901e+05 2.00050e+04 7.70000e+01 8.20000e+01 8.70000e+01\n",
      "   1.00000e+00 8.70000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01902e+05 2.00050e+04 8.00000e+01 8.50000e+01 9.00000e+01\n",
      "   1.00000e+00 9.00000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01903e+05 2.00050e+04 8.30000e+01 8.80000e+01 9.30000e+01\n",
      "   1.00000e+00 9.30000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01904e+05 2.00050e+04 8.60000e+01 9.10000e+01 9.60000e+01\n",
      "   1.00000e+00 9.60000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01905e+05 2.00050e+04 8.90000e+01 9.40000e+01 9.90000e+01\n",
      "   1.00000e+00 9.90000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01906e+05 2.00050e+04 9.20000e+01 9.70000e+01 1.02000e+02\n",
      "   1.00000e+00 1.02000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(165, 16, 13) (165,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001     0.00000\n",
      "1       20002     0.00000\n",
      "2       20003     0.00000\n",
      "3       20004  3329.14624\n",
      "4       20005     0.00000\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.3333333333333333, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.36363636363636365, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(160.33333333333334, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(43.72727272727273, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(6.666666666666667, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.8181818181818181, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.18666666666666668, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05090909090909091, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 2 layers, 418 neurons, 1 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 2s 43ms/step - loss: 1330978.0000 - val_loss: 505174.1250\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 363449.5000 - val_loss: 84798.9609\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 205467.7031 - val_loss: 86443.8438\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 171812.7031 - val_loss: 84388.3359\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 126790.8359 - val_loss: 85280.0234\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 104356.4766 - val_loss: 84130.2734\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 114590.1953 - val_loss: 84923.6016\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 119145.1016 - val_loss: 84260.9844\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 97063.0469 - val_loss: 83764.1875\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 103231.0312 - val_loss: 93800.7734\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 112142.0859 - val_loss: 93506.3750\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 96288.7812 - val_loss: 101329.9062\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 103907.4453 - val_loss: 99599.9297\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 99768.8516 - val_loss: 95271.3438\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 96176.8281 - val_loss: 83276.1797\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 91197.9844 - val_loss: 84977.7266\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 92181.7422 - val_loss: 83498.3359\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 96444.7109 - val_loss: 82870.3125\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 87550.5000 - val_loss: 83217.6016\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 87861.1172 - val_loss: 88943.9297\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 99956.9609 - val_loss: 84976.0391\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 88359.3906 - val_loss: 89271.9375\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 88502.4141 - val_loss: 82306.2969\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 94467.9141 - val_loss: 107176.0703\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 108223.9062 - val_loss: 117601.6953\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 105433.8438 - val_loss: 86156.1641\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 100216.5781 - val_loss: 82253.9531\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 87149.3203 - val_loss: 86419.3203\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 86946.7578 - val_loss: 82262.5078\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 88161.6719 - val_loss: 94708.7266\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 83444.9766 - val_loss: 91999.9609\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 90370.1562 - val_loss: 84169.7578\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 97575.6172 - val_loss: 89099.7969\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 89468.2500 - val_loss: 81583.0859\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 92740.2188 - val_loss: 83013.6250\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 88138.6094 - val_loss: 83531.7656\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 88169.3906 - val_loss: 86852.4375\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 83793.6719 - val_loss: 82106.3047\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 82244.0469 - val_loss: 90375.7344\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 87011.5469 - val_loss: 81133.6562\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 87787.0156 - val_loss: 88092.5938\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 81347.9844 - val_loss: 94397.6250\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 102784.6875 - val_loss: 99904.1172\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 96155.1016 - val_loss: 95649.8672\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 87254.6250 - val_loss: 84702.8125\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 91379.1953 - val_loss: 81935.1328\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 104623.5000 - val_loss: 80430.7031\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 102113.3125 - val_loss: 89194.9844\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 94731.5391 - val_loss: 82546.0625\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 85665.1484 - val_loss: 88459.8672\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 86905.2578 - val_loss: 81473.4141\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 85658.3594 - val_loss: 100719.3906\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 104646.1953 - val_loss: 85531.8203\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 81588.2266 - val_loss: 85058.0391\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 85934.3281 - val_loss: 85059.2969\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 88563.4531 - val_loss: 82936.8828\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 81498.9219 - val_loss: 79578.8984\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 79914.3438 - val_loss: 93033.1719\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 84086.5625 - val_loss: 79981.0938\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 82582.9297 - val_loss: 79855.0391\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 86719.5547 - val_loss: 92894.6094\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 84122.0391 - val_loss: 79900.3438\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 77150.0391 - val_loss: 79649.9609\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 85452.1172 - val_loss: 98817.0391\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 97024.2812 - val_loss: 91847.8359\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 95290.1953 - val_loss: 84356.3672\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 104308.4766 - val_loss: 188177.5938\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:47:34,981] Trial 10 finished with value: 0.7482348721082687 and parameters: {'n_layers': 2, 'n_neurons': 418, 'n_steps': 1, 'dropout_threshold': 0.10674969774718134}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(180, 1, 13) (180,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  168.942810\n",
      "1       20002  170.287613\n",
      "2       20003  168.888046\n",
      "3       20004  181.226639\n",
      "4       20005  205.696060\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.36363636363636365, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(43.72727272727273, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(5.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.8181818181818181, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05090909090909091, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 3 layers, 240 neurons, 6 steps.\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 73ms/step - loss: 34560828.0000 - val_loss: 826616.8125\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 25144444.0000 - val_loss: 3946047.5000\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 21878714.0000 - val_loss: 448286.0938\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 25035638.0000 - val_loss: 53782912.0000\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 17841690.0000 - val_loss: 90666.0000\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 16722006.0000 - val_loss: 558329.1875\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 13587632.0000 - val_loss: 9695604.0000\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 16722594.0000 - val_loss: 5688369.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 18431604.0000 - val_loss: 96479.2812\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 14684502.0000 - val_loss: 660959.8750\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14365608.0000 - val_loss: 3030207.5000\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14898454.0000 - val_loss: 34409616.0000\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11166814.0000 - val_loss: 8098432.0000\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11487715.0000 - val_loss: 1445646.1250\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 12008489.0000 - val_loss: 13160591.0000\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:47:38,307] Trial 11 finished with value: 0.775458703381572 and parameters: {'n_layers': 3, 'n_neurons': 240, 'n_steps': 6, 'dropout_threshold': 0.6422903395062898}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(175, 6, 13) (175,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  110.558510\n",
      "1       20002  111.508560\n",
      "2       20003  110.518356\n",
      "3       20004  121.879486\n",
      "4       20005   58.315102\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.3333333333333333, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(40.083333333333336, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(5.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.6666666666666667, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04666666666666667, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 2 layers, 191 neurons, 7 steps.\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 53ms/step - loss: 239378432.0000 - val_loss: 484951.6562\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 113640464.0000 - val_loss: 13321083.0000\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 90287616.0000 - val_loss: 189250448.0000\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 65122156.0000 - val_loss: 35184840.0000\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 58098076.0000 - val_loss: 52151420.0000\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 64094208.0000 - val_loss: 21161202.0000\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 47580580.0000 - val_loss: 9277121.0000\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 32400764.0000 - val_loss: 22522236.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 43029124.0000 - val_loss: 54725956.0000\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 25063662.0000 - val_loss: 12020224.0000\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 28542384.0000 - val_loss: 291511.9375\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 26492082.0000 - val_loss: 2952321.7500\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24079238.0000 - val_loss: 15720603.0000\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20900210.0000 - val_loss: 7470695.0000\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 22004056.0000 - val_loss: 1350716.5000\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 23108096.0000 - val_loss: 9114807.0000\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 22224864.0000 - val_loss: 3180135.5000\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 14205121.0000 - val_loss: 1264338.7500\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 16797154.0000 - val_loss: 1988247.2500\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 15763865.0000 - val_loss: 1062325.8750\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16477743.0000 - val_loss: 261389.0781\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 14873428.0000 - val_loss: 869148.3125\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 15326461.0000 - val_loss: 283176.6250\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 13160114.0000 - val_loss: 144944.5938\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12223037.0000 - val_loss: 103982.1562\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12672412.0000 - val_loss: 266061.9375\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 12193210.0000 - val_loss: 115607.3281\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 11190980.0000 - val_loss: 96867.5234\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 11230049.0000 - val_loss: 424526.1875\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7519852.0000 - val_loss: 204468.5156\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9439264.0000 - val_loss: 280808.7188\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7009284.0000 - val_loss: 4825258.0000\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8810966.0000 - val_loss: 92163.2891\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9222383.0000 - val_loss: 91749.4844\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9702011.0000 - val_loss: 1023792.1250\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7083392.0000 - val_loss: 191899.1719\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8136557.5000 - val_loss: 856888.7500\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 10595424.0000 - val_loss: 18402508.0000\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 11742739.0000 - val_loss: 12318183.0000\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 12040780.0000 - val_loss: 141756.0938\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 16663447.0000 - val_loss: 2381861.5000\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 17353386.0000 - val_loss: 1586346.5000\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17020776.0000 - val_loss: 1863489.5000\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 16357572.0000 - val_loss: 986767.2500\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01906e+05 2.00010e+04 3.00000e+01 3.10000e+01 3.20000e+01\n",
      "   1.00000e+00 6.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01906e+05 2.00020e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   0.00000e+00 1.00500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01906e+05 2.00030e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   1.00000e+00 6.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01906e+05 2.00040e+04 3.00000e+02 3.10000e+02 3.20000e+02\n",
      "   0.00000e+00 1.60000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:47:42,959] Trial 12 finished with value: 0.8879108664572113 and parameters: {'n_layers': 2, 'n_neurons': 191, 'n_steps': 7, 'dropout_threshold': 0.2554267882697203}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01906e+05 2.00050e+04 9.20000e+01 9.70000e+01 1.02000e+02\n",
      "   1.00000e+00 1.02000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(174, 7, 13) (174,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  232.536179\n",
      "1       20002  231.172989\n",
      "2       20003  232.574692\n",
      "3       20004  234.185471\n",
      "4       20005  197.481247\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.3076923076923077, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(37.0, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(5.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.5384615384615385, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04307692307692308, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 4 layers, 258 neurons, 4 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 3s 84ms/step - loss: 473544.5312 - val_loss: 113656.2188\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 455915.5625 - val_loss: 212874.9219\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 504721.8750 - val_loss: 124338.7031\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 504872.7812 - val_loss: 338206.5000\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 490396.8438 - val_loss: 76732.0391\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 408022.8438 - val_loss: 203123.4688\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 433558.8750 - val_loss: 232580.8594\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 395723.9062 - val_loss: 99330.5000\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 393520.5625 - val_loss: 113993.0781\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 414399.0625 - val_loss: 255634.7812\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 387939.1250 - val_loss: 176036.8594\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 337667.3438 - val_loss: 195584.8906\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 440601.4688 - val_loss: 99073.0938\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 347353.5000 - val_loss: 158771.7031\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 374652.0625 - val_loss: 84258.8281\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:47:48,388] Trial 13 finished with value: 1.0524885149453196 and parameters: {'n_layers': 4, 'n_neurons': 258, 'n_steps': 4, 'dropout_threshold': 0.25321644433231627}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(177, 4, 13) (177,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  216.230347\n",
      "1       20002  192.878265\n",
      "2       20003  217.037994\n",
      "3       20004  445.947662\n",
      "4       20005   65.989578\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2857142857142857, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(34.357142857142854, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(5.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.4285714285714286, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 2 layers, 139 neurons, 9 steps.\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 51ms/step - loss: 245741472.0000 - val_loss: 232184.9062\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 96432136.0000 - val_loss: 20266450.0000\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 84513664.0000 - val_loss: 62194916.0000\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 90858376.0000 - val_loss: 4694212.5000\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 65357660.0000 - val_loss: 161273.9375\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 54625552.0000 - val_loss: 853304.5000\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 43620812.0000 - val_loss: 158099.9219\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 32806996.0000 - val_loss: 9150500.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 34875676.0000 - val_loss: 435398.8750\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 29019436.0000 - val_loss: 540024.8750\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 25283454.0000 - val_loss: 32148378.0000\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 28987848.0000 - val_loss: 7421947.5000\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 22363318.0000 - val_loss: 102192.0938\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 23565850.0000 - val_loss: 12437756.0000\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 23091142.0000 - val_loss: 10475857.0000\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 23097982.0000 - val_loss: 1669595.2500\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 21580500.0000 - val_loss: 192980.0312\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 21392060.0000 - val_loss: 4956108.0000\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 19234888.0000 - val_loss: 1247860.0000\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 12917042.0000 - val_loss: 96897.7891\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 16364418.0000 - val_loss: 753674.6250\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 12677479.0000 - val_loss: 1063293.6250\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 12326579.0000 - val_loss: 349577.7188\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 11503249.0000 - val_loss: 159346.5938\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 11422499.0000 - val_loss: 759409.8125\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 10497445.0000 - val_loss: 252132.6250\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 13161428.0000 - val_loss: 279210.3438\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 10246734.0000 - val_loss: 556532.8750\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 14028808.0000 - val_loss: 14837093.0000\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12111439.0000 - val_loss: 35422100.0000\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01904e+05 2.00010e+04 2.80000e+01 2.90000e+01 3.00000e+01\n",
      "   1.00000e+00 6.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00010e+04 2.90000e+01 3.00000e+01 3.10000e+01\n",
      "   1.00000e+00 6.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00010e+04 3.00000e+01 3.10000e+01 3.20000e+01\n",
      "   1.00000e+00 6.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01904e+05 2.00020e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   0.00000e+00 9.45000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00020e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   0.00000e+00 9.75000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00020e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   0.00000e+00 1.00500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01904e+05 2.00030e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   1.00000e+00 5.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00030e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   1.00000e+00 6.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00030e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   1.00000e+00 6.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01904e+05 2.00040e+04 2.80000e+02 2.90000e+02 3.00000e+02\n",
      "   0.00000e+00 1.50000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00040e+04 2.90000e+02 3.00000e+02 3.10000e+02\n",
      "   0.00000e+00 1.55000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00040e+04 3.00000e+02 3.10000e+02 3.20000e+02\n",
      "   0.00000e+00 1.60000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:47:51,893] Trial 14 finished with value: 0.8426409770509283 and parameters: {'n_layers': 2, 'n_neurons': 139, 'n_steps': 9, 'dropout_threshold': 0.43590537189152456}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01904e+05 2.00050e+04 8.60000e+01 9.10000e+01 9.60000e+01\n",
      "   1.00000e+00 9.60000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01905e+05 2.00050e+04 8.90000e+01 9.40000e+01 9.90000e+01\n",
      "   1.00000e+00 9.90000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01906e+05 2.00050e+04 9.20000e+01 9.70000e+01 1.02000e+02\n",
      "   1.00000e+00 1.02000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(172, 9, 13) (172,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  182.951843\n",
      "1       20002  180.367905\n",
      "2       20003  183.033432\n",
      "3       20004  152.959366\n",
      "4       20005  115.531113\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.26666666666666666, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(32.06666666666667, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(5.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.3333333333333333, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.037333333333333336, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 3 layers, 388 neurons, 3 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 2s 56ms/step - loss: 2286197.7500 - val_loss: 87201.8047\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 2620736.5000 - val_loss: 143587.8594\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 3066752.2500 - val_loss: 161049.4219\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1814977.3750 - val_loss: 1471569.0000\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 1906283.2500 - val_loss: 1516053.1250\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 1967645.2500 - val_loss: 1657965.1250\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1679743.2500 - val_loss: 807695.5000\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 1326722.6250 - val_loss: 85973.5156\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1793671.1250 - val_loss: 207451.4062\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1309952.8750 - val_loss: 107674.8906\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1255501.2500 - val_loss: 187140.2188\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1219292.6250 - val_loss: 121684.0703\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 958839.0625 - val_loss: 123417.3359\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1191668.6250 - val_loss: 190348.1875\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1020689.0625 - val_loss: 217932.0781\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1028084.6250 - val_loss: 222499.9219\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 982375.8750 - val_loss: 689394.5000\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1161394.5000 - val_loss: 313401.8438\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:47:55,534] Trial 15 finished with value: 0.7607765577039866 and parameters: {'n_layers': 3, 'n_neurons': 388, 'n_steps': 3, 'dropout_threshold': 0.6586471387446324}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(178, 3, 13) (178,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  157.861145\n",
      "1       20002  157.851532\n",
      "2       20003  157.860825\n",
      "3       20004  171.852173\n",
      "4       20005  159.712631\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.25, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(30.0625, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(5.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.25, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.035, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 291 neurons, 7 steps.\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 33ms/step - loss: 308800992.0000 - val_loss: 69034032.0000\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 60722864.0000 - val_loss: 39570396.0000\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 41372180.0000 - val_loss: 4317780.5000\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 44721416.0000 - val_loss: 68741760.0000\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 50361720.0000 - val_loss: 7621535.0000\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 28862394.0000 - val_loss: 241580.2656\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 47098672.0000 - val_loss: 92992840.0000\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 28592266.0000 - val_loss: 1144188.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 654405.3125 - val_loss: 2686243.0000\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4566970.5000 - val_loss: 3916953.2500\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2537253.5000 - val_loss: 198129216.0000\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 131501104.0000 - val_loss: 25928104.0000\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8539152.0000 - val_loss: 12185652.0000\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 17593148.0000 - val_loss: 16363269.0000\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 10057406.0000 - val_loss: 2118284.2500\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 756531.2500 - val_loss: 903656.6875\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01906e+05 2.00010e+04 3.00000e+01 3.10000e+01 3.20000e+01\n",
      "   1.00000e+00 6.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01906e+05 2.00020e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   0.00000e+00 1.00500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01906e+05 2.00030e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   1.00000e+00 6.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01906e+05 2.00040e+04 3.00000e+02 3.10000e+02 3.20000e+02\n",
      "   0.00000e+00 1.60000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:47:57,109] Trial 16 finished with value: 1.0 and parameters: {'n_layers': 1, 'n_neurons': 291, 'n_steps': 7, 'dropout_threshold': 0.4167172139120297}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01906e+05 2.00050e+04 9.20000e+01 9.70000e+01 1.02000e+02\n",
      "   1.00000e+00 1.02000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(174, 7, 13) (174,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001         0.0\n",
      "1       20002         0.0\n",
      "2       20003         0.0\n",
      "3       20004         0.0\n",
      "4       20005         0.0\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.23529411764705882, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(28.294117647058822, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(5.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.1764705882352942, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03294117647058824, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 4 layers, 446 neurons, 4 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 3s 81ms/step - loss: 219799.4531 - val_loss: 306396.3438\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 269262.8750 - val_loss: 418734.2812\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 346701.8750 - val_loss: 90120.2031\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 336634.0000 - val_loss: 368129.1562\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 376095.7500 - val_loss: 281700.0312\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 332298.9062 - val_loss: 106228.7422\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 420301.0000 - val_loss: 450856.4375\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 492168.0000 - val_loss: 392785.0625\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 686111.0000 - val_loss: 83546.3906\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 527431.6250 - val_loss: 571496.6875\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 655568.7500 - val_loss: 650944.6875\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 584360.0625 - val_loss: 288901.2812\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 761937.9375 - val_loss: 121363.3047\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 692492.1250 - val_loss: 99126.6094\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 486041.2500 - val_loss: 398398.2188\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 559435.8750 - val_loss: 1805969.3750\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 792884.1250 - val_loss: 77676.7188\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 679185.8125 - val_loss: 310458.0625\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 679092.5625 - val_loss: 835900.8125\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 932205.9375 - val_loss: 1907727.7500\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 744475.3125 - val_loss: 479220.7812\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 497734.5312 - val_loss: 87168.8750\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 428560.0625 - val_loss: 141981.7188\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 277072.0625 - val_loss: 85375.4375\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 275941.6250 - val_loss: 88787.0625\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 299160.1250 - val_loss: 188682.9375\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 266200.7500 - val_loss: 99331.4219\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:48:04,619] Trial 17 finished with value: 0.7499509732670797 and parameters: {'n_layers': 4, 'n_neurons': 446, 'n_steps': 4, 'dropout_threshold': 0.3241688610568991}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(177, 4, 13) (177,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  169.480057\n",
      "1       20002  169.585037\n",
      "2       20003  169.478775\n",
      "3       20004  169.535706\n",
      "4       20005  214.964523\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2222222222222222, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(26.72222222222222, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(5.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.1111111111111112, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.031111111111111114, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 2 layers, 199 neurons, 12 steps.\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 60ms/step - loss: 836849536.0000 - val_loss: 298323488.0000\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 383789120.0000 - val_loss: 1143854080.0000\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 525729408.0000 - val_loss: 550891584.0000\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 452081536.0000 - val_loss: 23234382.0000\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 276169664.0000 - val_loss: 61202380.0000\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 300471296.0000 - val_loss: 28175496.0000\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 198881824.0000 - val_loss: 85513480.0000\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 144834336.0000 - val_loss: 18183890.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 172440704.0000 - val_loss: 1761903.2500\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 149269760.0000 - val_loss: 21468458.0000\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 143203760.0000 - val_loss: 348580832.0000\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 375690048.0000 - val_loss: 112254824.0000\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 355556256.0000 - val_loss: 675784896.0000\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 244331296.0000 - val_loss: 28352406.0000\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 75916584.0000 - val_loss: 6262300.5000\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 55222656.0000 - val_loss: 493643.3750\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 31727330.0000 - val_loss: 904672.8125\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 48121292.0000 - val_loss: 1463295.2500\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 39853248.0000 - val_loss: 16993878.0000\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 55530056.0000 - val_loss: 5818410.0000\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 55815396.0000 - val_loss: 68377016.0000\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 44422196.0000 - val_loss: 80059736.0000\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 63396752.0000 - val_loss: 8904550.0000\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 44577344.0000 - val_loss: 24741206.0000\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 95677240.0000 - val_loss: 39425652.0000\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 58144416.0000 - val_loss: 20783902.0000\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01901e+05 2.00010e+04 2.50000e+01 2.60000e+01 2.70000e+01\n",
      "   1.00000e+00 5.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00010e+04 2.60000e+01 2.70000e+01 2.80000e+01\n",
      "   1.00000e+00 5.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00010e+04 2.70000e+01 2.80000e+01 2.90000e+01\n",
      "   1.00000e+00 5.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00010e+04 2.80000e+01 2.90000e+01 3.00000e+01\n",
      "   1.00000e+00 6.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00010e+04 2.90000e+01 3.00000e+01 3.10000e+01\n",
      "   1.00000e+00 6.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00010e+04 3.00000e+01 3.10000e+01 3.20000e+01\n",
      "   1.00000e+00 6.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01901e+05 2.00020e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   0.00000e+00 8.55000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00020e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   0.00000e+00 8.85000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00020e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   0.00000e+00 9.15000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00020e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   0.00000e+00 9.45000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00020e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   0.00000e+00 9.75000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00020e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   0.00000e+00 1.00500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01901e+05 2.00030e+04 2.45000e+01 2.55000e+01 2.65000e+01\n",
      "   1.00000e+00 5.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00030e+04 2.55000e+01 2.65000e+01 2.75000e+01\n",
      "   1.00000e+00 5.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00030e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   1.00000e+00 5.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00030e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   1.00000e+00 5.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00030e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   1.00000e+00 6.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00030e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   1.00000e+00 6.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01901e+05 2.00040e+04 2.50000e+02 2.60000e+02 2.70000e+02\n",
      "   0.00000e+00 1.35000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00040e+04 2.60000e+02 2.70000e+02 2.80000e+02\n",
      "   0.00000e+00 1.40000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00040e+04 2.70000e+02 2.80000e+02 2.90000e+02\n",
      "   0.00000e+00 1.45000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00040e+04 2.80000e+02 2.90000e+02 3.00000e+02\n",
      "   0.00000e+00 1.50000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00040e+04 2.90000e+02 3.00000e+02 3.10000e+02\n",
      "   0.00000e+00 1.55000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00040e+04 3.00000e+02 3.10000e+02 3.20000e+02\n",
      "   0.00000e+00 1.60000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:48:08,998] Trial 18 finished with value: 1.9855250869755574 and parameters: {'n_layers': 2, 'n_neurons': 199, 'n_steps': 12, 'dropout_threshold': 0.1028216298756862}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01901e+05 2.00050e+04 7.70000e+01 8.20000e+01 8.70000e+01\n",
      "   1.00000e+00 8.70000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01902e+05 2.00050e+04 8.00000e+01 8.50000e+01 9.00000e+01\n",
      "   1.00000e+00 9.00000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01903e+05 2.00050e+04 8.30000e+01 8.80000e+01 9.30000e+01\n",
      "   1.00000e+00 9.30000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01904e+05 2.00050e+04 8.60000e+01 9.10000e+01 9.60000e+01\n",
      "   1.00000e+00 9.60000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01905e+05 2.00050e+04 8.90000e+01 9.40000e+01 9.90000e+01\n",
      "   1.00000e+00 9.90000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01906e+05 2.00050e+04 9.20000e+01 9.70000e+01 1.02000e+02\n",
      "   1.00000e+00 1.02000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(169, 12, 13) (169,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  841.637390\n",
      "1       20002  834.457703\n",
      "2       20003  841.565186\n",
      "3       20004  692.601379\n",
      "4       20005  655.782776\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.21052631578947367, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(25.31578947368421, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(5.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0526315789473684, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02947368421052632, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 3 layers, 135 neurons, 8 steps.\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 2s 67ms/step - loss: 24152828.0000 - val_loss: 5224775.0000\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16117898.0000 - val_loss: 15032524.0000\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14363619.0000 - val_loss: 9119037.0000\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14393772.0000 - val_loss: 33723568.0000\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10198304.0000 - val_loss: 1342073.3750\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11438312.0000 - val_loss: 55711796.0000\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9939049.0000 - val_loss: 7300252.5000\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 11524304.0000 - val_loss: 39711472.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10314909.0000 - val_loss: 417042.9375\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6456565.0000 - val_loss: 9972793.0000\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4498913.5000 - val_loss: 568320.1250\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6482779.5000 - val_loss: 726513.2500\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5343912.0000 - val_loss: 114210.9844\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4508363.0000 - val_loss: 3255951.2500\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4095766.7500 - val_loss: 1027363.0625\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4653334.5000 - val_loss: 5297107.0000\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5038769.5000 - val_loss: 5626281.0000\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6612075.5000 - val_loss: 4102039.2500\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5844482.0000 - val_loss: 4560201.5000\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4631061.5000 - val_loss: 1148604.7500\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5208743.0000 - val_loss: 634012.6875\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3490317.2500 - val_loss: 547310.5000\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2930564.5000 - val_loss: 815235.6250\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01905e+05 2.00010e+04 2.90000e+01 3.00000e+01 3.10000e+01\n",
      "   1.00000e+00 6.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00010e+04 3.00000e+01 3.10000e+01 3.20000e+01\n",
      "   1.00000e+00 6.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01905e+05 2.00020e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   0.00000e+00 9.75000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00020e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   0.00000e+00 1.00500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:48:12,843] Trial 19 finished with value: 1.0997415963839823 and parameters: {'n_layers': 3, 'n_neurons': 135, 'n_steps': 8, 'dropout_threshold': 0.2809063661012458}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01905e+05 2.00030e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   1.00000e+00 6.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00030e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   1.00000e+00 6.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01905e+05 2.00040e+04 2.90000e+02 3.00000e+02 3.10000e+02\n",
      "   0.00000e+00 1.55000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00040e+04 3.00000e+02 3.10000e+02 3.20000e+02\n",
      "   0.00000e+00 1.60000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01905e+05 2.00050e+04 8.90000e+01 9.40000e+01 9.90000e+01\n",
      "   1.00000e+00 9.90000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01906e+05 2.00050e+04 9.20000e+01 9.70000e+01 1.02000e+02\n",
      "   1.00000e+00 1.02000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(173, 8, 13) (173,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  343.294098\n",
      "1       20002  341.624756\n",
      "2       20003  343.365692\n",
      "3       20004  325.913635\n",
      "4       20005  269.677002\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(24.05, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(5.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.028000000000000004, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 241 neurons, 5 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 28ms/step - loss: 48550844.0000 - val_loss: 22114710.0000\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14508776.0000 - val_loss: 103733.4688\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2591086.0000 - val_loss: 4955568.0000\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2314941.7500 - val_loss: 652128.3125\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1605557.1250 - val_loss: 786644.5625\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 452197.8125 - val_loss: 749970.8750\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 354131.0938 - val_loss: 266758.0625\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 326387.9375 - val_loss: 105145.4688\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 165434.5156 - val_loss: 319442.1250\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 236899.5469 - val_loss: 118459.6875\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 182283.5156 - val_loss: 103512.3359\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 162258.7188 - val_loss: 194062.5000\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 129745.6797 - val_loss: 181666.0000\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 143744.0312 - val_loss: 100084.1719\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 122158.6328 - val_loss: 92881.5234\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 99201.3047 - val_loss: 113909.2188\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 92791.8281 - val_loss: 98316.3281\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 93750.1484 - val_loss: 99251.5078\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 103803.1641 - val_loss: 91154.5547\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 85870.5234 - val_loss: 89528.7734\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 85886.4297 - val_loss: 91275.9844\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 86771.6250 - val_loss: 87735.4844\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 84339.0234 - val_loss: 88383.4453\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 104788.8438 - val_loss: 86220.5938\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 86909.0156 - val_loss: 85094.3828\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 86301.6484 - val_loss: 84254.3594\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 83759.2656 - val_loss: 87064.3594\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 81865.9609 - val_loss: 83188.5938\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 80377.1797 - val_loss: 86067.9609\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 81634.8047 - val_loss: 82872.3281\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 79182.7812 - val_loss: 85384.5547\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 85435.8516 - val_loss: 79219.9062\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 75818.0703 - val_loss: 79921.4922\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 79310.5234 - val_loss: 78010.2188\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 75504.6016 - val_loss: 81266.2891\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 84429.6641 - val_loss: 75695.2266\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 72818.7031 - val_loss: 87910.1484\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 99150.5859 - val_loss: 76811.0078\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 76347.7344 - val_loss: 78377.0156\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 83227.3203 - val_loss: 77192.3203\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 79403.9375 - val_loss: 71910.3516\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 70048.8750 - val_loss: 71330.2812\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 68154.6250 - val_loss: 89944.8359\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 107635.4297 - val_loss: 90286.7344\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 100932.3906 - val_loss: 104112.1953\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 110113.5781 - val_loss: 112777.0625\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 115749.4922 - val_loss: 89089.4375\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 78155.0391 - val_loss: 87825.2500\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 77787.9922 - val_loss: 68042.7031\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 64594.6328 - val_loss: 68194.4219\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 61875.9688 - val_loss: 68354.5781\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 64498.8242 - val_loss: 67712.6250\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 62228.7695 - val_loss: 61408.2305\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 58553.6875 - val_loss: 63989.2891\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 62173.3594 - val_loss: 60308.4414\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 56007.9688 - val_loss: 64203.0859\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 59545.7656 - val_loss: 57215.3984\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 56984.9062 - val_loss: 88351.7500\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 112136.4453 - val_loss: 123103.0469\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 102885.4766 - val_loss: 111747.8203\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 76982.5938 - val_loss: 110896.9766\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 81037.4688 - val_loss: 58440.1914\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 67935.4844 - val_loss: 51658.9023\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 52344.5234 - val_loss: 52605.3438\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 54775.6094 - val_loss: 100196.7188\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 116362.3828 - val_loss: 122106.9766\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 80228.9062 - val_loss: 209779.3750\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 157735.1406 - val_loss: 178680.8594\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 95651.8281 - val_loss: 86758.7969\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 83738.8906 - val_loss: 48544.5234\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 57702.0820 - val_loss: 52702.1641\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 60746.8008 - val_loss: 51046.7969\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 47714.6523 - val_loss: 47391.6250\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 42120.1484 - val_loss: 42554.3477\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 44865.4883 - val_loss: 57085.9258\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 56251.8867 - val_loss: 42733.0586\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 63075.1172 - val_loss: 73490.5391\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 62209.5391 - val_loss: 50956.6641\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 45923.3789 - val_loss: 41438.6523\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 39584.5391 - val_loss: 39100.2188\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 41427.7969 - val_loss: 81292.6094\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 90166.9844 - val_loss: 116000.9609\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 74624.0312 - val_loss: 60501.4258\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 55973.0547 - val_loss: 35099.7695\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 38165.5391 - val_loss: 41426.2422\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 38660.6289 - val_loss: 34764.0547\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 34399.3672 - val_loss: 33242.0195\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 32282.9160 - val_loss: 33943.5469\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 34748.8125 - val_loss: 31668.2070\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 33209.0078 - val_loss: 40864.2031\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 38438.4180 - val_loss: 61241.0352\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 46075.6445 - val_loss: 48715.2148\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 42308.7500 - val_loss: 30477.0039\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 35664.2344 - val_loss: 34643.7500\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 33232.6133 - val_loss: 33869.2227\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 33859.0391 - val_loss: 32257.4492\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 33231.5469 - val_loss: 34679.9336\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 29215.0098 - val_loss: 27853.5176\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 38367.4453 - val_loss: 30173.9336\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 41197.4414 - val_loss: 64158.4023\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 45090.1758 - val_loss: 29361.4336\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 36290.1055 - val_loss: 28308.0684\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 26057.3574 - val_loss: 33996.6250\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 27658.9375 - val_loss: 23378.6094\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 24494.2695 - val_loss: 22852.3887\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 22622.5957 - val_loss: 22922.0117\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 23476.1934 - val_loss: 22239.0039\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 21181.0625 - val_loss: 22114.4277\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21424.0801 - val_loss: 24832.4258\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 22387.5762 - val_loss: 21486.2871\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21132.2188 - val_loss: 20635.8789\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21132.6875 - val_loss: 21610.6289\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21041.9434 - val_loss: 19492.6934\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18942.5566 - val_loss: 19038.0938\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18557.9570 - val_loss: 24815.3594\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 24379.6309 - val_loss: 19081.6797\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 24758.3711 - val_loss: 24216.9336\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 21205.9688 - val_loss: 17794.2949\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 17797.4590 - val_loss: 17335.9785\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17979.5039 - val_loss: 17937.4570\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 17092.5391 - val_loss: 20524.1094\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 18903.4043 - val_loss: 19005.9922\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 16769.8125 - val_loss: 16342.4551\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17821.0801 - val_loss: 20435.9961\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 16691.6621 - val_loss: 16375.5088\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 16214.7900 - val_loss: 24963.2383\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 25198.7793 - val_loss: 40113.1562\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 26976.2461 - val_loss: 14838.6025\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 22736.8984 - val_loss: 15640.3115\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14951.0361 - val_loss: 13134.9385\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 12582.2109 - val_loss: 33583.3477\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 27675.6953 - val_loss: 44458.4766\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 28944.1309 - val_loss: 17967.9180\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24767.8848 - val_loss: 19352.2012\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16869.6582 - val_loss: 21639.2207\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 19101.8242 - val_loss: 26077.5293\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 19543.9883 - val_loss: 15011.2773\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 17783.5410 - val_loss: 66078.5312\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 43400.3281 - val_loss: 20988.8340\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 25964.0859 - val_loss: 42962.7422\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:48:20,574] Trial 20 finished with value: 0.3174733170259067 and parameters: {'n_layers': 1, 'n_neurons': 241, 'n_steps': 5, 'dropout_threshold': 0.20196617726529006}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(176, 5, 13) (176,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  119.941833\n",
      "1       20002  148.253143\n",
      "2       20003  118.815384\n",
      "3       20004  280.625854\n",
      "4       20005  881.289246\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(24.05, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(4.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.028000000000000004, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 231 neurons, 6 steps.\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 39ms/step - loss: 5427810.0000 - val_loss: 13436815.0000\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 7780009.5000 - val_loss: 1508409.6250\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 687111.1875 - val_loss: 797862.8125\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 390900.9375 - val_loss: 5207390.0000\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2092729.0000 - val_loss: 2859084.0000\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1212596.3750 - val_loss: 1490290.7500\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 846233.1250 - val_loss: 639839.0000\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 576530.5000 - val_loss: 169089.3125\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 281583.1562 - val_loss: 90417.8828\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 160820.0938 - val_loss: 110177.8672\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 121368.7500 - val_loss: 109076.2031\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 98711.2266 - val_loss: 103155.2500\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 88525.2266 - val_loss: 84339.1484\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 75523.5781 - val_loss: 79454.8359\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 73043.2656 - val_loss: 74624.4844\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 83404.1719 - val_loss: 70588.6562\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 72059.6406 - val_loss: 69417.7734\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 74584.3203 - val_loss: 70952.7500\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 69131.4531 - val_loss: 69274.2891\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 65394.1641 - val_loss: 66208.6250\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 64438.9609 - val_loss: 66026.0156\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 64756.1992 - val_loss: 66243.2734\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 64048.6680 - val_loss: 63049.2500\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 60658.9492 - val_loss: 61843.1445\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 59228.0625 - val_loss: 60873.8164\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 60095.4453 - val_loss: 59620.8164\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 60344.1250 - val_loss: 62686.8555\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 64045.4492 - val_loss: 73521.5625\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 61658.0000 - val_loss: 70248.9609\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 61220.5195 - val_loss: 59051.7383\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 55893.6133 - val_loss: 54565.5703\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 53149.5234 - val_loss: 55247.3438\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 51841.2109 - val_loss: 53914.0977\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 50999.7383 - val_loss: 52311.7148\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 50481.1172 - val_loss: 50083.6172\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 47434.0117 - val_loss: 49731.6133\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 47371.3867 - val_loss: 48205.8008\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 46681.4297 - val_loss: 49714.9258\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 46774.5508 - val_loss: 50124.2734\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 51342.8711 - val_loss: 45429.0898\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 49397.6445 - val_loss: 57734.8789\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 53085.8828 - val_loss: 43071.7227\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 41481.5391 - val_loss: 41989.1406\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 41990.4102 - val_loss: 48383.1172\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 44769.1250 - val_loss: 42726.8594\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 41692.2070 - val_loss: 40709.2031\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 44105.5430 - val_loss: 48985.0742\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 48814.5352 - val_loss: 40138.6797\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 48000.1953 - val_loss: 50817.0195\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 38127.3203 - val_loss: 36984.5938\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 35722.6328 - val_loss: 35338.8164\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 33562.4883 - val_loss: 33741.7031\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 32789.9180 - val_loss: 33604.3203\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 31932.5195 - val_loss: 37484.0352\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 33776.9766 - val_loss: 31392.5215\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 34553.0156 - val_loss: 34153.6953\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 32456.7969 - val_loss: 33607.7852\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 31643.2031 - val_loss: 28972.5645\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 31944.9941 - val_loss: 31702.3008\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 27902.4062 - val_loss: 27574.2129\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 29812.1309 - val_loss: 36814.0078\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 31207.7930 - val_loss: 29535.5723\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 29948.7539 - val_loss: 29643.2305\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 28030.2246 - val_loss: 30441.6973\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 30976.9258 - val_loss: 23992.2852\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 28054.9805 - val_loss: 27392.8945\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 26518.5312 - val_loss: 33545.7109\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 32416.7695 - val_loss: 21949.3340\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 29239.5371 - val_loss: 45161.2969\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 28212.2500 - val_loss: 20697.1250\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 27452.6777 - val_loss: 36026.7812\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 27805.9023 - val_loss: 28048.7871\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 37757.2578 - val_loss: 23379.5684\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 27761.0430 - val_loss: 28493.1270\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 24240.0586 - val_loss: 26193.6582\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 25407.5117 - val_loss: 18091.4395\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 25425.4746 - val_loss: 19942.7637\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 26670.4648 - val_loss: 47155.8320\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 31523.6191 - val_loss: 29435.5000\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 27135.1992 - val_loss: 17531.5098\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 20497.1152 - val_loss: 33351.5039\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 22664.8965 - val_loss: 15947.2217\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 16352.7451 - val_loss: 17383.3184\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 13944.3564 - val_loss: 13365.8096\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 13053.7451 - val_loss: 14052.9150\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 15719.7783 - val_loss: 19255.8262\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 14640.6982 - val_loss: 15116.9033\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 14548.3877 - val_loss: 12086.1260\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 12620.5078 - val_loss: 11268.1123\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 11462.1035 - val_loss: 11474.2334\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 10405.2520 - val_loss: 10451.9648\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 10704.5732 - val_loss: 10379.3506\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9999.2217 - val_loss: 9878.9033\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9263.1543 - val_loss: 12512.1836\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 11102.4766 - val_loss: 13927.4336\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 10821.6797 - val_loss: 8993.8320\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 9068.6074 - val_loss: 8493.4531\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 8401.8613 - val_loss: 12005.3457\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10033.1172 - val_loss: 11245.0830\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8670.9434 - val_loss: 7563.0249\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7899.0703 - val_loss: 9622.3545\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9141.1904 - val_loss: 7717.8955\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 7720.9111 - val_loss: 9302.5498\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 7986.8936 - val_loss: 6547.2578\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9200.3701 - val_loss: 7530.3203\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 9849.2021 - val_loss: 8051.7515\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 6661.8267 - val_loss: 5872.6616\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5746.9111 - val_loss: 5700.2563\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 7776.6016 - val_loss: 5433.5312\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 8342.8047 - val_loss: 5360.8711\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9390.1348 - val_loss: 5097.0112\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 10311.2549 - val_loss: 5156.9355\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 7732.0181 - val_loss: 7618.6689\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 7919.0791 - val_loss: 9144.1699\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 6346.1948 - val_loss: 7145.6870\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 6349.8984 - val_loss: 10943.3496\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8245.9893 - val_loss: 10228.1152\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 11117.4229 - val_loss: 12443.0088\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 12221.8223 - val_loss: 8929.3857\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8099.7920 - val_loss: 6142.1851\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4988.6865 - val_loss: 7166.5684\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:48:27,913] Trial 21 finished with value: 0.18504967018580284 and parameters: {'n_layers': 1, 'n_neurons': 231, 'n_steps': 6, 'dropout_threshold': 0.18631008766873972}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(175, 6, 13) (175,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001   116.621376\n",
      "1       20002   152.251495\n",
      "2       20003   116.141373\n",
      "3       20004   247.558838\n",
      "4       20005  1065.983154\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.19047619047619047, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(22.904761904761905, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(4.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.9523809523809523, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02666666666666667, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 318 neurons, 1 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 751035.5625 - val_loss: 202647.9531\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 440111.7500 - val_loss: 93063.7109\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 152611.0781 - val_loss: 159113.7812\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 114565.8672 - val_loss: 139010.8125\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 108245.4062 - val_loss: 102472.3438\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 80762.2578 - val_loss: 88177.7031\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 80651.2969 - val_loss: 76872.9766\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 72702.8906 - val_loss: 72435.0703\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 68718.6719 - val_loss: 71659.9844\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 69191.2188 - val_loss: 70237.1875\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 67094.5547 - val_loss: 68330.7266\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 65000.0547 - val_loss: 66484.1641\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 64977.0078 - val_loss: 65625.4141\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 68554.0156 - val_loss: 64426.2305\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 62371.9141 - val_loss: 63941.9336\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 65207.5195 - val_loss: 68171.4688\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 64335.0312 - val_loss: 61659.0469\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 57984.0000 - val_loss: 58659.1406\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 56919.7930 - val_loss: 59317.9297\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 58384.8008 - val_loss: 56853.3398\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 56245.7266 - val_loss: 54257.2305\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 53381.9922 - val_loss: 52682.8242\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 50607.5820 - val_loss: 53208.7891\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51600.4492 - val_loss: 51120.8828\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 47707.5820 - val_loss: 49422.5469\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 48307.2969 - val_loss: 49160.9375\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 50608.5625 - val_loss: 53302.1484\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 49370.1484 - val_loss: 56748.4766\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 50400.4883 - val_loss: 44596.3867\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 47565.2500 - val_loss: 46978.8945\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 42823.8164 - val_loss: 44774.5234\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 41227.4258 - val_loss: 43689.4258\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40317.6133 - val_loss: 40291.9102\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 44736.0000 - val_loss: 53265.5312\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 45364.1758 - val_loss: 55127.1172\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 44960.4805 - val_loss: 39705.0312\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40326.5547 - val_loss: 43819.5664\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 42381.7852 - val_loss: 37013.9922\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35916.2188 - val_loss: 33635.4922\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32624.2148 - val_loss: 33152.5195\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31813.5879 - val_loss: 39690.2109\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 37443.1016 - val_loss: 39445.6484\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 35173.0820 - val_loss: 30674.6582\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28819.0117 - val_loss: 28963.0117\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28682.8574 - val_loss: 29518.7773\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27312.5488 - val_loss: 28491.3125\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26571.7969 - val_loss: 26841.3262\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26719.3848 - val_loss: 25720.2832\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 26134.5664 - val_loss: 25380.3965\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26051.5234 - val_loss: 30055.3242\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28939.1094 - val_loss: 25555.0234\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25570.6992 - val_loss: 23598.4941\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22123.4727 - val_loss: 39937.2188\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32756.1992 - val_loss: 28221.7773\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25028.9395 - val_loss: 23421.1211\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 23250.9941 - val_loss: 28686.0859\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 24323.4258 - val_loss: 27732.3066\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 27464.2422 - val_loss: 20973.2695\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 22598.3945 - val_loss: 30846.1992\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 27754.1543 - val_loss: 17758.8223\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 17771.8027 - val_loss: 17754.0332\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17428.4336 - val_loss: 23127.3906\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18958.8594 - val_loss: 16605.4297\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16491.7930 - val_loss: 15366.1602\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 14159.5723 - val_loss: 13948.2451\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 13579.4277 - val_loss: 13831.8057\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14029.2695 - val_loss: 17264.6621\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16337.0117 - val_loss: 14739.0967\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12101.6885 - val_loss: 12083.1250\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11571.7930 - val_loss: 18002.9727\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13912.5547 - val_loss: 11685.9834\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10748.2920 - val_loss: 13077.7734\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11940.4814 - val_loss: 11454.3613\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11335.2275 - val_loss: 11307.3525\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9585.5283 - val_loss: 9358.1162\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9444.2949 - val_loss: 12360.5850\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10918.5947 - val_loss: 13501.7969\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11166.3066 - val_loss: 8236.2988\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9720.6357 - val_loss: 12868.9688\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9824.7881 - val_loss: 8506.8613\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8832.8965 - val_loss: 12109.4756\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10155.1240 - val_loss: 7099.0986\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6868.1797 - val_loss: 6540.7354\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6365.5464 - val_loss: 6564.6851\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6164.1528 - val_loss: 6270.0786\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7202.0908 - val_loss: 5623.3662\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5391.2803 - val_loss: 8019.5693\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5879.0005 - val_loss: 5203.9336\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5485.9653 - val_loss: 11901.7695\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9716.2383 - val_loss: 10078.1553\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11810.8691 - val_loss: 4962.0171\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8021.9409 - val_loss: 4953.2603\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7317.5078 - val_loss: 8058.7646\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5717.8906 - val_loss: 6245.1758\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4399.1338 - val_loss: 3620.5933\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3480.0852 - val_loss: 5276.0244\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5141.7964 - val_loss: 3039.6279\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3426.9526 - val_loss: 2975.4365\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2919.9375 - val_loss: 2956.9419\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3012.0806 - val_loss: 2840.7637\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2795.4048 - val_loss: 3831.0278\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2963.7708 - val_loss: 2639.3958\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2863.1169 - val_loss: 3602.7595\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3807.9102 - val_loss: 2744.6177\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2772.3564 - val_loss: 1999.9467\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1935.7620 - val_loss: 3988.5630\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3524.1592 - val_loss: 2519.9521\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2110.6787 - val_loss: 3053.7056\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2475.4585 - val_loss: 3538.4021\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2465.7910 - val_loss: 3667.7944\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3196.2454 - val_loss: 1824.6235\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1519.5361 - val_loss: 1504.7615\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1536.9065 - val_loss: 1370.4584\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1536.8320 - val_loss: 1511.6218\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1521.1613 - val_loss: 1108.1537\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1207.8188 - val_loss: 1053.5408\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1210.5289 - val_loss: 962.4546\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1457.7631 - val_loss: 1015.4298\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1141.3882 - val_loss: 967.1003\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 965.2279 - val_loss: 1082.5985\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 875.6910 - val_loss: 763.5630\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 756.9563 - val_loss: 705.9480\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 729.6194 - val_loss: 678.2751\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 732.3516 - val_loss: 651.5392\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 749.8900 - val_loss: 845.5109\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 820.1480 - val_loss: 1519.0117\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 876.4601 - val_loss: 590.4827\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 664.6770 - val_loss: 775.3815\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 737.2903 - val_loss: 483.1673\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 607.3715 - val_loss: 627.1949\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 633.9748 - val_loss: 620.0007\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 559.6373 - val_loss: 517.5333\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 543.0065 - val_loss: 471.1033\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 458.9320 - val_loss: 366.6711\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 413.4588 - val_loss: 329.0919\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 348.0907 - val_loss: 319.6017\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 348.1595 - val_loss: 294.8624\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 358.3842 - val_loss: 289.9909\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 293.6261 - val_loss: 299.1100\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 314.6630 - val_loss: 452.1273\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 367.3376 - val_loss: 529.8340\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 432.0133 - val_loss: 277.3436\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 354.6663 - val_loss: 218.7999\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 256.3860 - val_loss: 208.9465\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 225.3525 - val_loss: 230.8543\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 216.2846 - val_loss: 190.2610\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 197.0700 - val_loss: 265.9971\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 244.2346 - val_loss: 173.5696\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 209.0481 - val_loss: 182.2014\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 244.0576 - val_loss: 237.8161\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 213.4782 - val_loss: 153.7856\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 163.5153 - val_loss: 393.2938\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 287.4159 - val_loss: 145.0551\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 196.6943 - val_loss: 139.0680\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 168.2965 - val_loss: 143.1763\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 162.4779 - val_loss: 202.1634\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 168.0620 - val_loss: 250.5397\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 213.2759 - val_loss: 157.8939\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 233.3412 - val_loss: 181.0454\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 174.2003 - val_loss: 118.6729\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 120.7290 - val_loss: 131.8563\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 125.6161 - val_loss: 180.9250\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 138.4852 - val_loss: 529.1618\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 335.0343 - val_loss: 211.2028\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 361.2985 - val_loss: 247.7181\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 319.9875 - val_loss: 355.0071\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 249.2192 - val_loss: 111.6208\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 170.7098 - val_loss: 509.7241\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 296.6611 - val_loss: 121.0941\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 192.8734 - val_loss: 256.7783\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 201.1769 - val_loss: 102.6104\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 120.6463 - val_loss: 102.6021\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 104.2585 - val_loss: 124.2110\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 124.7599 - val_loss: 98.2370\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 109.7593 - val_loss: 96.9987\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 102.7353 - val_loss: 96.0559\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 109.4837 - val_loss: 107.4635\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 139.2145 - val_loss: 167.0119\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 173.5385 - val_loss: 148.9355\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 124.1206 - val_loss: 95.6986\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 97.1621 - val_loss: 102.9369\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 101.0819 - val_loss: 185.3958\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 137.7421 - val_loss: 88.6757\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 93.1782 - val_loss: 127.4039\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 99.4776 - val_loss: 87.0412\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 93.4404 - val_loss: 85.2373\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 100.0767 - val_loss: 151.5871\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 130.5512 - val_loss: 147.8711\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 142.4157 - val_loss: 104.1635\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 143.4736 - val_loss: 638.9219\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 368.0737 - val_loss: 86.7983\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 187.9468 - val_loss: 263.9725\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 165.7835 - val_loss: 87.9887\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 124.9100 - val_loss: 98.5312\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 81.8448 - val_loss: 83.1456\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 97.2673 - val_loss: 170.4524\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 166.4274 - val_loss: 184.4073\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 152.0473 - val_loss: 229.5193\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 191.3443 - val_loss: 236.7413\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 292.8580 - val_loss: 289.1251\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:48:34,618] Trial 22 finished with value: 0.11471486817141835 and parameters: {'n_layers': 1, 'n_neurons': 318, 'n_steps': 1, 'dropout_threshold': 0.18640473851338085}. Best is trial 2 with value: 0.09382065982479545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(180, 1, 13) (180,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001   101.286880\n",
      "1       20002   146.360184\n",
      "2       20003   100.016144\n",
      "3       20004   219.337830\n",
      "4       20005  1282.868286\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.18181818181818182, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(21.863636363636363, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(4.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.9090909090909091, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.025454545454545455, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 312 neurons, 1 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 3654199.0000 - val_loss: 1186717.1250\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1829151.7500 - val_loss: 771187.4375\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 285809.0938 - val_loss: 553468.2500\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 618322.0000 - val_loss: 244597.4375\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 147971.1094 - val_loss: 231214.6406\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 224813.5000 - val_loss: 93827.4609\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 84470.5703 - val_loss: 146771.7344\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 122507.5859 - val_loss: 78736.6719\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 83760.9062 - val_loss: 88865.0859\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 77959.2656 - val_loss: 82197.7422\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 82203.2344 - val_loss: 78352.4844\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 74833.5547 - val_loss: 75634.4844\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 71923.2266 - val_loss: 73915.7969\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 69718.7266 - val_loss: 72608.1562\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 70472.6641 - val_loss: 71632.2109\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 68052.3438 - val_loss: 70793.4375\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 70015.9375 - val_loss: 69846.4844\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 72719.0938 - val_loss: 70338.7422\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 68010.4219 - val_loss: 70335.1797\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 68253.6953 - val_loss: 66033.7734\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 74555.2109 - val_loss: 66846.5312\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 66016.9141 - val_loss: 71740.1016\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 63970.4180 - val_loss: 71116.4219\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 65896.4219 - val_loss: 61455.1289\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 58225.8125 - val_loss: 59916.9062\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 57263.7383 - val_loss: 58864.4531\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 56435.3867 - val_loss: 57962.3867\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 55092.4727 - val_loss: 57612.9766\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 58781.8711 - val_loss: 55524.4766\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 57287.7109 - val_loss: 57686.7539\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 52691.0625 - val_loss: 55587.4453\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 53568.5820 - val_loss: 56287.5938\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 50997.0469 - val_loss: 53229.9648\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51891.6133 - val_loss: 50861.0195\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 48347.1680 - val_loss: 49906.0000\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 48986.1992 - val_loss: 49333.8359\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 52230.6484 - val_loss: 47832.4102\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 47844.7891 - val_loss: 46830.7812\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 50003.4531 - val_loss: 45655.6836\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 47659.5742 - val_loss: 47278.6953\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 45550.1055 - val_loss: 44122.3828\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 41695.6016 - val_loss: 45841.7227\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40923.2930 - val_loss: 46233.4883\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 45957.5586 - val_loss: 42925.1133\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 44069.4375 - val_loss: 40469.1602\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40169.3164 - val_loss: 39849.2773\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 39121.7461 - val_loss: 38846.8281\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 37556.3320 - val_loss: 38369.6602\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 38546.3828 - val_loss: 37300.7305\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 36802.6953 - val_loss: 36702.4219\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 35771.3711 - val_loss: 35912.4648\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35502.7031 - val_loss: 35541.5078\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 34092.4805 - val_loss: 36245.8984\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 34767.1133 - val_loss: 33609.6250\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33216.0547 - val_loss: 33007.8633\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 31418.3809 - val_loss: 32180.0566\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30901.8906 - val_loss: 31313.8652\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29785.7422 - val_loss: 30887.3672\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30318.4609 - val_loss: 30034.7500\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28745.7539 - val_loss: 29428.7949\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29079.8613 - val_loss: 29200.8906\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28695.2598 - val_loss: 34172.2070\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 44064.9844 - val_loss: 32142.3848\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33914.9102 - val_loss: 28861.9648\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27481.5938 - val_loss: 27457.7285\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28216.9160 - val_loss: 25805.7949\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 24890.3770 - val_loss: 24305.9004\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24327.8613 - val_loss: 23733.6367\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22366.6738 - val_loss: 23613.7461\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22480.3613 - val_loss: 22654.8125\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22337.0098 - val_loss: 21923.6543\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21437.9902 - val_loss: 23940.2891\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25426.4883 - val_loss: 23819.4551\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23291.7402 - val_loss: 26841.3066\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26759.1211 - val_loss: 22127.1875\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19831.6523 - val_loss: 22059.3828\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19660.0039 - val_loss: 19532.5645\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18111.7129 - val_loss: 17960.1855\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17710.6797 - val_loss: 17475.8945\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17488.8184 - val_loss: 18477.1875\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19158.5938 - val_loss: 20128.7910\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18787.8516 - val_loss: 22429.8594\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18968.1875 - val_loss: 21756.6152\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18629.8984 - val_loss: 23333.2715\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19008.9824 - val_loss: 19408.2090\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20079.3359 - val_loss: 14515.6533\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16408.0723 - val_loss: 14104.7910\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14010.7559 - val_loss: 13955.5283\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14517.1035 - val_loss: 14826.4102\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13404.2881 - val_loss: 13879.6484\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12514.3164 - val_loss: 12364.0166\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11720.4238 - val_loss: 12148.8301\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11687.4277 - val_loss: 11444.8945\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11074.3096 - val_loss: 11071.5137\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10647.7021 - val_loss: 10837.6123\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 10406.2480 - val_loss: 10755.1143\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10228.3506 - val_loss: 10958.0234\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10559.9023 - val_loss: 9909.5898\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10132.3379 - val_loss: 9803.0488\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9439.0010 - val_loss: 10608.3447\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10016.0488 - val_loss: 11304.3672\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10207.8018 - val_loss: 9040.3320\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8669.2588 - val_loss: 9238.0459\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8156.6372 - val_loss: 10636.9980\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9392.4746 - val_loss: 7887.4385\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9119.0713 - val_loss: 10720.6592\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9723.1338 - val_loss: 7834.7207\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8287.6592 - val_loss: 8833.7773\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8728.1846 - val_loss: 10518.4121\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9460.5391 - val_loss: 8328.7393\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7406.2871 - val_loss: 6551.6484\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7753.3516 - val_loss: 6485.7104\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6177.2383 - val_loss: 6022.9507\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5834.0376 - val_loss: 5853.0693\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5881.1597 - val_loss: 6280.6670\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5944.5996 - val_loss: 7002.2456\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6081.4644 - val_loss: 5368.3086\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6147.5181 - val_loss: 5673.4780\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5361.0952 - val_loss: 7457.8252\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6871.4297 - val_loss: 4803.2510\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5436.2051 - val_loss: 5510.2168\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4847.7627 - val_loss: 5455.5493\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5205.1982 - val_loss: 4325.3428\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4520.1006 - val_loss: 5328.9204\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4898.2090 - val_loss: 5305.9302\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4621.7358 - val_loss: 3879.2849\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4080.1318 - val_loss: 3766.0713\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3664.5864 - val_loss: 3616.5110\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3559.6558 - val_loss: 3862.7495\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3692.1636 - val_loss: 4162.9009\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3590.2175 - val_loss: 3260.5769\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3765.6440 - val_loss: 4214.2705\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3930.8667 - val_loss: 3347.8787\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3422.7830 - val_loss: 2941.9287\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2874.9443 - val_loss: 3268.9089\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2955.3308 - val_loss: 2985.5066\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2716.6267 - val_loss: 2868.9414\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2799.5903 - val_loss: 2547.8831\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2455.2036 - val_loss: 2473.4695\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2395.0879 - val_loss: 2335.7937\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2618.9692 - val_loss: 2701.4993\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2291.3340 - val_loss: 2151.5544\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2201.8516 - val_loss: 2282.3801\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2078.7720 - val_loss: 2019.1287\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1905.2013 - val_loss: 1910.1125\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1918.9297 - val_loss: 1906.5154\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1953.3483 - val_loss: 2050.7131\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1773.0667 - val_loss: 1660.4941\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1680.6360 - val_loss: 1598.1680\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1625.9760 - val_loss: 1862.6824\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1720.5188 - val_loss: 1480.3220\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1544.1256 - val_loss: 1526.5287\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1417.1514 - val_loss: 2516.2476\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2039.4647 - val_loss: 1610.4077\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1487.7974 - val_loss: 1239.7208\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1216.1127 - val_loss: 1196.6542\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1168.3407 - val_loss: 1170.3274\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1112.6018 - val_loss: 1098.2014\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1103.8418 - val_loss: 1148.3879\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1119.1685 - val_loss: 1486.2390\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1148.6649 - val_loss: 979.7358\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1185.3105 - val_loss: 987.1342\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1002.8001 - val_loss: 1222.0391\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1092.5869 - val_loss: 1150.2966\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1144.3397 - val_loss: 937.9241\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 952.0414 - val_loss: 877.6642\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 881.0204 - val_loss: 875.3621\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 855.6857 - val_loss: 734.9884\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 726.4977 - val_loss: 884.4324\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 774.1716 - val_loss: 677.6328\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 707.0565 - val_loss: 766.1437\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 705.1910 - val_loss: 788.3063\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 642.0453 - val_loss: 629.3301\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 632.5217 - val_loss: 854.3289\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 885.0709 - val_loss: 600.7091\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 647.7794 - val_loss: 952.6608\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 758.1842 - val_loss: 736.0345\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 591.3302 - val_loss: 496.1704\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 490.6999 - val_loss: 500.7297\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 482.6504 - val_loss: 489.1299\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 497.6234 - val_loss: 808.0732\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 741.2892 - val_loss: 501.2986\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 595.7763 - val_loss: 527.6514\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 492.3749 - val_loss: 630.7306\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 695.0793 - val_loss: 375.8120\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 425.3467 - val_loss: 373.6520\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 407.0424 - val_loss: 513.4893\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 467.0885 - val_loss: 394.3932\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 440.3163 - val_loss: 322.2932\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 366.8831 - val_loss: 450.0833\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 364.5201 - val_loss: 299.2104\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 305.5224 - val_loss: 289.6465\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 294.7046 - val_loss: 320.6004\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 308.3171 - val_loss: 269.6810\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 275.5436 - val_loss: 336.6929\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 293.6628 - val_loss: 252.2801\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 258.8061 - val_loss: 252.9415\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 252.1359 - val_loss: 247.4963\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 271.7690 - val_loss: 290.1704\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 258.2291 - val_loss: 222.6147\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:48:40,894] Trial 23 finished with value: 0.07674217856264359 and parameters: {'n_layers': 1, 'n_neurons': 312, 'n_steps': 1, 'dropout_threshold': 0.16775428543203713}. Best is trial 23 with value: 0.07674217856264359.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(180, 1, 13) (180,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001    89.902893\n",
      "1       20002   133.300232\n",
      "2       20003    88.486320\n",
      "3       20004   223.329483\n",
      "4       20005  1251.834717\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.17391304347826086, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20.91304347826087, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(4.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8695652173913043, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.024347826086956525, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 373 neurons, 1 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 4245185.5000 - val_loss: 1519871.3750\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1912965.2500 - val_loss: 628416.1250\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 295874.5938 - val_loss: 620664.0000\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 607329.6250 - val_loss: 165347.4062\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 146946.4062 - val_loss: 280463.9062\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 213788.1719 - val_loss: 83189.7422\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 100441.5000 - val_loss: 121710.5312\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 95330.6875 - val_loss: 90371.4219\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 92835.2500 - val_loss: 86816.0234\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 78600.8594 - val_loss: 84234.5312\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 81873.1250 - val_loss: 77637.4297\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 74629.6406 - val_loss: 77540.0859\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 74678.0547 - val_loss: 75942.3516\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 71287.2500 - val_loss: 76033.8359\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 73227.9375 - val_loss: 72790.5156\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 68912.4141 - val_loss: 72076.0703\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 72837.6250 - val_loss: 70363.6328\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 66537.7734 - val_loss: 73551.2031\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 68932.5938 - val_loss: 68083.3672\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 65095.1406 - val_loss: 67013.5234\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 63898.1562 - val_loss: 66725.3281\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 63342.2227 - val_loss: 64959.0586\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 61861.2500 - val_loss: 63976.9297\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 65265.4844 - val_loss: 62745.0234\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 63199.7500 - val_loss: 62122.3750\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 64925.3633 - val_loss: 62745.1875\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 58073.5391 - val_loss: 60047.4922\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 58461.8359 - val_loss: 58724.2930\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 54614.0742 - val_loss: 58037.9922\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 55178.6680 - val_loss: 58396.2930\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 54352.3828 - val_loss: 54911.2188\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 52205.4961 - val_loss: 53776.2578\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51104.2188 - val_loss: 52939.7812\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 50649.3828 - val_loss: 51849.1289\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 49909.6836 - val_loss: 50564.9883\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 48400.0938 - val_loss: 49494.7773\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 47509.1289 - val_loss: 49193.2305\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 47723.3438 - val_loss: 47566.9297\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 48302.0625 - val_loss: 46358.9766\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 44962.3164 - val_loss: 45782.6016\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43433.6016 - val_loss: 47199.2227\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43417.7383 - val_loss: 45689.8984\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43898.5625 - val_loss: 42797.6406\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40857.6367 - val_loss: 41252.9141\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 39497.1914 - val_loss: 40730.0586\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 39148.6992 - val_loss: 42398.6367\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 42719.2070 - val_loss: 39730.6484\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40561.7070 - val_loss: 37588.9453\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35761.6680 - val_loss: 36765.9414\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 34954.3047 - val_loss: 36781.1914\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35001.0859 - val_loss: 36550.1406\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35161.5000 - val_loss: 36930.8281\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 39346.7500 - val_loss: 33023.4688\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33095.8203 - val_loss: 32656.8555\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31353.7422 - val_loss: 31014.8906\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 29721.0176 - val_loss: 30319.5762\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29177.4824 - val_loss: 30929.5410\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29238.7812 - val_loss: 29096.8535\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28090.0176 - val_loss: 27785.1035\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26872.6445 - val_loss: 27143.8242\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26027.0430 - val_loss: 26623.0234\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25452.6914 - val_loss: 26004.6172\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24836.0371 - val_loss: 27572.8125\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25735.3184 - val_loss: 26791.0176\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26376.3145 - val_loss: 25134.0098\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24144.8398 - val_loss: 24890.5586\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22402.2520 - val_loss: 23314.9082\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21955.5957 - val_loss: 22792.3457\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21756.4180 - val_loss: 22973.9316\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22949.7871 - val_loss: 24482.9727\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23814.2188 - val_loss: 22508.9766\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20648.7324 - val_loss: 20597.2637\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19388.9355 - val_loss: 20480.4082\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18200.2480 - val_loss: 20109.2285\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18891.5586 - val_loss: 18107.5820\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17116.4219 - val_loss: 17754.8945\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16640.3477 - val_loss: 16988.9316\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16294.6240 - val_loss: 16426.0449\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15774.3457 - val_loss: 16117.5029\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15778.9863 - val_loss: 18364.1680\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16567.6465 - val_loss: 16662.6387\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15301.7686 - val_loss: 15931.1230\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15041.3076 - val_loss: 14233.5938\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14613.4346 - val_loss: 14272.9531\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14210.9727 - val_loss: 13395.4180\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13358.5137 - val_loss: 13766.3867\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14363.3604 - val_loss: 14369.6621\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13983.2227 - val_loss: 12184.2100\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13470.5830 - val_loss: 13764.7822\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12620.2852 - val_loss: 14840.0791\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12597.8047 - val_loss: 13427.7822\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11298.0283 - val_loss: 11001.0547\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10086.3223 - val_loss: 11376.8604\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10217.2988 - val_loss: 13381.2207\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11647.5723 - val_loss: 11244.5879\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10339.5859 - val_loss: 9380.7070\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11123.7354 - val_loss: 12943.4062\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12050.5547 - val_loss: 16550.9473\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11609.0977 - val_loss: 10995.9189\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9933.9375 - val_loss: 8070.2080\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8689.2451 - val_loss: 9934.3496\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9857.1094 - val_loss: 8762.3613\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8619.8281 - val_loss: 7421.6973\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8037.8271 - val_loss: 8460.4219\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7368.3198 - val_loss: 7845.1338\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7578.8887 - val_loss: 6651.0596\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6490.6343 - val_loss: 6878.2544\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6470.3828 - val_loss: 7653.8340\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6860.4819 - val_loss: 9016.5820\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7084.8711 - val_loss: 5751.3647\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7418.4756 - val_loss: 7725.5352\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7028.0493 - val_loss: 5256.6523\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6319.9644 - val_loss: 6493.3662\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6037.2163 - val_loss: 6185.5410\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6072.7593 - val_loss: 4646.7300\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4623.8657 - val_loss: 4483.5771\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4418.3335 - val_loss: 4997.5874\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4738.7017 - val_loss: 4696.4888\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4805.7222 - val_loss: 4030.0771\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4566.0649 - val_loss: 5104.5840\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4132.8657 - val_loss: 3758.5830\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4062.8958 - val_loss: 5265.8208\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5118.2061 - val_loss: 6528.0220\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5311.3564 - val_loss: 3332.4692\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4618.7397 - val_loss: 4779.6348\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4028.6594 - val_loss: 3439.3022\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3396.3467 - val_loss: 3467.4985\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3532.7808 - val_loss: 4777.0737\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4155.4521 - val_loss: 2684.4937\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3272.1360 - val_loss: 3087.3416\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2837.7471 - val_loss: 2463.5640\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2439.7771 - val_loss: 2883.8286\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2696.3501 - val_loss: 3142.7783\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3108.4075 - val_loss: 2407.7561\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2462.7139 - val_loss: 2965.0532\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2403.3721 - val_loss: 1996.8402\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2028.9500 - val_loss: 2400.2434\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2042.0568 - val_loss: 1918.7114\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1757.0240 - val_loss: 1824.1403\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1846.6714 - val_loss: 2549.3225\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2374.4219 - val_loss: 2413.3442\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2153.0127 - val_loss: 1598.8198\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1626.7228 - val_loss: 1480.1232\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1538.2700 - val_loss: 1442.9292\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1389.0281 - val_loss: 1354.4338\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1384.0718 - val_loss: 1312.5465\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1376.3103 - val_loss: 1289.8759\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1361.7755 - val_loss: 1273.3567\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1260.3802 - val_loss: 1369.5664\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1256.7747 - val_loss: 1281.6201\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1193.5195 - val_loss: 1123.6860\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1161.1838 - val_loss: 1068.8019\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1235.2484 - val_loss: 1001.7662\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1075.6294 - val_loss: 941.5636\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 889.1257 - val_loss: 877.3397\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 870.6092 - val_loss: 802.7423\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 788.1685 - val_loss: 790.8681\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 751.0297 - val_loss: 906.7772\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 818.6700 - val_loss: 782.1180\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 712.0148 - val_loss: 784.1760\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 761.8563 - val_loss: 973.2941\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 801.0577 - val_loss: 874.4940\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 829.1094 - val_loss: 1050.2241\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 896.2871 - val_loss: 1064.6609\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 805.8336 - val_loss: 622.3113\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 765.8638 - val_loss: 634.0086\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 688.1130 - val_loss: 815.5380\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 731.4237 - val_loss: 527.1939\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 507.4570 - val_loss: 480.1763\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 501.0422 - val_loss: 484.5273\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 494.1347 - val_loss: 433.1333\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 503.7839 - val_loss: 450.6113\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 452.1085 - val_loss: 382.8033\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 375.2635 - val_loss: 385.0211\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 402.9672 - val_loss: 359.2643\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 427.5743 - val_loss: 478.5420\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 491.4954 - val_loss: 470.9217\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 430.7368 - val_loss: 371.3359\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 383.8566 - val_loss: 441.8851\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 393.8141 - val_loss: 293.1057\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 412.8755 - val_loss: 360.5617\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 375.6713 - val_loss: 374.5193\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 329.2120 - val_loss: 275.8877\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 281.9692 - val_loss: 277.9091\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 350.7872 - val_loss: 244.0434\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 268.4986 - val_loss: 261.1083\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 271.0826 - val_loss: 292.0197\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 251.3807 - val_loss: 218.6139\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 226.2052 - val_loss: 239.1145\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 248.8844 - val_loss: 203.8053\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 230.5027 - val_loss: 200.7575\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 212.1739 - val_loss: 191.1194\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 223.6172 - val_loss: 249.2713\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 199.7960 - val_loss: 186.6914\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 191.6140 - val_loss: 174.4778\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 180.9161 - val_loss: 174.5945\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 174.8904 - val_loss: 175.7399\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 167.1481 - val_loss: 183.6878\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 163.3268 - val_loss: 253.1235\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 223.0025 - val_loss: 201.9098\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:48:47,360] Trial 24 finished with value: 0.09700499991761449 and parameters: {'n_layers': 1, 'n_neurons': 373, 'n_steps': 1, 'dropout_threshold': 0.14771756819392948}. Best is trial 23 with value: 0.07674217856264359.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(180, 1, 13) (180,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001    94.791832\n",
      "1       20002   138.690948\n",
      "2       20003    93.407921\n",
      "3       20004   223.121201\n",
      "4       20005  1270.467896\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.16666666666666666, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20.041666666666668, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(4.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8333333333333334, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.023333333333333334, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 381 neurons, 2 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 7738765.5000 - val_loss: 6353036.0000\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2396062.0000 - val_loss: 2122397.5000\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2111987.7500 - val_loss: 104572.0000\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 558503.4375 - val_loss: 833517.6250\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 364669.2812 - val_loss: 413952.2812\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 347997.7812 - val_loss: 82318.6953\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 169944.4688 - val_loss: 142911.9375\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 97466.6641 - val_loss: 129997.2812\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 94122.2500 - val_loss: 105525.1719\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 104557.9375 - val_loss: 76414.1094\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 76831.1250 - val_loss: 79778.7578\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 75493.1562 - val_loss: 84832.8438\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 77263.3438 - val_loss: 76051.9844\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 74232.1250 - val_loss: 72000.9609\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 68203.3438 - val_loss: 70405.7500\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 67348.9688 - val_loss: 70117.6094\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 68096.5078 - val_loss: 68270.7422\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 65025.2812 - val_loss: 66979.1250\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 63919.0234 - val_loss: 65856.6641\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 65614.1562 - val_loss: 64630.0469\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 62998.2930 - val_loss: 63704.4844\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 60532.6953 - val_loss: 62460.6758\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 60788.3945 - val_loss: 62009.1133\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 63462.7305 - val_loss: 65063.9922\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 64342.6094 - val_loss: 64601.0195\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 76567.5703 - val_loss: 59012.4023\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 56975.7070 - val_loss: 55585.5508\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 53832.3672 - val_loss: 55357.2773\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 52054.3164 - val_loss: 52580.3359\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 50665.1328 - val_loss: 51464.4961\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 49630.7695 - val_loss: 50028.6758\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 48525.9922 - val_loss: 48823.8711\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 46452.4648 - val_loss: 49331.5625\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 46671.7422 - val_loss: 46493.5273\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 44335.4648 - val_loss: 45449.5977\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 43986.8789 - val_loss: 44477.7266\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 42845.2305 - val_loss: 45836.6680\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 47361.9727 - val_loss: 48428.8945\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 45608.7617 - val_loss: 48469.4062\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 43276.0898 - val_loss: 44690.8945\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 43253.7852 - val_loss: 39305.9180\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 38341.8633 - val_loss: 38686.6094\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 36763.5938 - val_loss: 37544.8750\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 35747.3594 - val_loss: 36632.7852\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 35541.6289 - val_loss: 35674.7266\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 34208.2031 - val_loss: 34950.8477\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 33901.1445 - val_loss: 35744.0898\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 34474.5586 - val_loss: 33462.9961\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30822.4277 - val_loss: 37217.9648\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 35005.6211 - val_loss: 37149.1406\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 32855.6719 - val_loss: 29988.6914\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 31239.0176 - val_loss: 28824.0039\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 27761.0234 - val_loss: 29233.6797\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28435.2344 - val_loss: 27280.3926\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 26804.8105 - val_loss: 26597.4492\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 25241.5137 - val_loss: 25771.3398\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 24732.1641 - val_loss: 25663.2871\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 24024.5176 - val_loss: 24833.8906\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 23633.5234 - val_loss: 24719.0684\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 23419.1484 - val_loss: 22672.3867\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 21862.2461 - val_loss: 22305.9023\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 21831.4062 - val_loss: 22160.7363\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 21062.3359 - val_loss: 27467.8203\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 23176.5137 - val_loss: 20710.2207\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 20940.9375 - val_loss: 21530.0508\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 20021.6797 - val_loss: 21597.9473\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 20008.9141 - val_loss: 18941.6953\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 16855.4609 - val_loss: 20747.4062\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 17630.7168 - val_loss: 21146.6367\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 18701.6934 - val_loss: 16384.5684\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 22722.8965 - val_loss: 22823.9590\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 17812.0586 - val_loss: 17056.8770\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 16187.9238 - val_loss: 14691.6113\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 13849.4404 - val_loss: 14214.2637\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 13312.5840 - val_loss: 15903.8789\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 14173.1494 - val_loss: 13106.5977\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 13168.0117 - val_loss: 15176.0811\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 14239.0547 - val_loss: 12935.2393\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 12470.0508 - val_loss: 13916.9590\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 12497.6709 - val_loss: 11556.2988\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 11588.5625 - val_loss: 13294.9590\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 11317.3594 - val_loss: 26573.8984\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 18011.3828 - val_loss: 12309.1719\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16358.3555 - val_loss: 25161.9961\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 16958.8418 - val_loss: 10527.8037\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 12595.6543 - val_loss: 14241.8828\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 12203.2383 - val_loss: 9198.1670\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9359.3389 - val_loss: 8076.1196\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7922.3325 - val_loss: 8090.4141\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7547.9697 - val_loss: 7538.5771\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7836.7715 - val_loss: 7242.4512\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7198.9512 - val_loss: 9121.1445\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9498.3730 - val_loss: 6635.8950\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8102.5327 - val_loss: 14093.1270\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9679.6064 - val_loss: 9192.0371\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7543.8643 - val_loss: 5726.7310\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5678.3892 - val_loss: 5903.2153\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5711.2856 - val_loss: 6812.7300\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6862.7568 - val_loss: 5252.7202\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6038.3618 - val_loss: 8370.8350\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6557.6084 - val_loss: 4492.5161\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4485.5674 - val_loss: 4288.6470\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5011.4434 - val_loss: 4809.3965\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4363.4038 - val_loss: 4372.1821\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4340.1001 - val_loss: 5446.5181\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4034.5178 - val_loss: 3941.7832\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4414.5308 - val_loss: 3471.8389\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4367.5137 - val_loss: 5215.6602\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4392.5010 - val_loss: 4896.4141\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4382.9819 - val_loss: 6478.4229\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8213.1904 - val_loss: 3848.6826\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4750.4746 - val_loss: 7785.3652\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5633.7124 - val_loss: 4278.1631\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4184.3804 - val_loss: 2488.4221\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3672.7446 - val_loss: 2425.5938\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2938.9771 - val_loss: 2409.6262\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2678.6924 - val_loss: 2747.9165\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2259.9126 - val_loss: 2503.0120\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2479.8140 - val_loss: 1982.4446\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1936.9813 - val_loss: 1992.5958\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2114.3850 - val_loss: 1796.7269\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2188.0303 - val_loss: 1742.8373\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1756.9617 - val_loss: 1890.2313\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1766.7498 - val_loss: 1606.2550\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1875.9724 - val_loss: 1610.1788\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1953.6362 - val_loss: 1429.7479\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1865.7732 - val_loss: 1436.8922\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1356.4307 - val_loss: 1367.5359\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1336.6010 - val_loss: 1211.8696\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1190.3135 - val_loss: 1170.2306\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1133.0536 - val_loss: 1676.8132\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1337.8431 - val_loss: 1091.2246\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1226.4420 - val_loss: 1139.6278\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1269.1858 - val_loss: 1585.6627\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1062.0391 - val_loss: 998.1255\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1311.8596 - val_loss: 937.8096\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1233.0826 - val_loss: 1187.5109\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1372.1326 - val_loss: 820.8366\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1434.3524 - val_loss: 1065.1527\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1191.0402 - val_loss: 1156.4872\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1110.0421 - val_loss: 2105.4150\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1236.5157 - val_loss: 859.9285\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 809.8525 - val_loss: 638.4485\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 630.6819 - val_loss: 650.8740\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 618.5021 - val_loss: 699.2048\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 726.0708 - val_loss: 880.0853\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 742.8250 - val_loss: 531.6536\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 512.1782 - val_loss: 498.2644\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 495.3902 - val_loss: 533.3074\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 640.6713 - val_loss: 579.6436\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 535.8547 - val_loss: 549.8488\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 702.0948 - val_loss: 464.5297\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 429.4105 - val_loss: 509.7729\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 472.2572 - val_loss: 437.8440\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 467.3559 - val_loss: 454.3631\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 561.5803 - val_loss: 408.0981\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 504.7805 - val_loss: 350.5074\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 371.9493 - val_loss: 346.9800\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 346.3468 - val_loss: 414.6696\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 440.4917 - val_loss: 332.8259\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 320.6490 - val_loss: 940.2778\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 629.1591 - val_loss: 832.4169\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 861.7517 - val_loss: 514.6059\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 419.0399 - val_loss: 282.0196\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 285.4357 - val_loss: 280.1549\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 284.6495 - val_loss: 276.5076\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 346.3281 - val_loss: 485.4072\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 509.7154 - val_loss: 258.8446\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 301.3753 - val_loss: 251.0209\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 258.6357 - val_loss: 246.2576\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 275.9935 - val_loss: 316.6451\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 285.2048 - val_loss: 332.7907\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 299.1702 - val_loss: 261.4003\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 317.1086 - val_loss: 486.7882\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 363.5976 - val_loss: 246.3532\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 246.7230 - val_loss: 331.5002\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 294.6399 - val_loss: 395.0469\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 352.1655 - val_loss: 404.8799\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 510.3391 - val_loss: 454.1776\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 412.4406 - val_loss: 691.1494\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:48:54,006] Trial 25 finished with value: 0.08028520593875932 and parameters: {'n_layers': 1, 'n_neurons': 381, 'n_steps': 2, 'dropout_threshold': 0.13841569797445297}. Best is trial 23 with value: 0.07674217856264359.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(179, 2, 13) (179,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001    87.355614\n",
      "1       20002   131.431000\n",
      "2       20003    86.056580\n",
      "3       20004   221.756561\n",
      "4       20005  1266.133545\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.16, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(19.24, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(4.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.022400000000000003, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 2 layers, 425 neurons, 3 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 40ms/step - loss: 4224904.0000 - val_loss: 5594375.0000\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4297199.5000 - val_loss: 111783.1172\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2604263.7500 - val_loss: 2352072.5000\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1810355.6250 - val_loss: 114366.6172\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1426751.1250 - val_loss: 1075107.8750\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 841196.0625 - val_loss: 253104.9062\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 564661.3750 - val_loss: 443400.8125\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 514117.7500 - val_loss: 109481.1328\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 650679.6250 - val_loss: 292288.6250\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 559132.0000 - val_loss: 248503.6875\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 371090.4375 - val_loss: 117170.8594\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 285509.3125 - val_loss: 116060.0469\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 213605.8906 - val_loss: 240862.0312\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 212493.3750 - val_loss: 139718.2500\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 219390.2500 - val_loss: 88724.0859\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 261630.1875 - val_loss: 467166.0938\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 264615.2812 - val_loss: 86211.7500\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 315123.9375 - val_loss: 120882.5234\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 364486.2500 - val_loss: 854464.6875\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 307464.2188 - val_loss: 578554.5000\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 297190.2188 - val_loss: 422036.2500\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 271532.2812 - val_loss: 88780.0234\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 222724.9062 - val_loss: 122033.9766\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 202128.7656 - val_loss: 256388.9531\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 214260.2188 - val_loss: 114838.8125\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 196268.7188 - val_loss: 136630.0312\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 143320.5938 - val_loss: 100648.5703\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:48:57,270] Trial 26 finished with value: 0.8683911743347958 and parameters: {'n_layers': 2, 'n_neurons': 425, 'n_steps': 3, 'dropout_threshold': 0.13823091189125825}. Best is trial 23 with value: 0.07674217856264359.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(178, 3, 13) (178,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  233.781921\n",
      "1       20002  233.777771\n",
      "2       20003  233.787781\n",
      "3       20004  232.813629\n",
      "4       20005  233.565948\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15384615384615385, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(18.5, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(4.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7692307692307693, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02153846153846154, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 405 neurons, 3 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 32ms/step - loss: 35525424.0000 - val_loss: 29398982.0000\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 12291590.0000 - val_loss: 2327501.2500\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 5484174.5000 - val_loss: 1449125.3750\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 812601.8125 - val_loss: 2144064.5000\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1331967.7500 - val_loss: 136999.6250\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 604929.0625 - val_loss: 522871.8125\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 228805.0781 - val_loss: 446740.3750\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 401115.2812 - val_loss: 82162.3438\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 181094.1250 - val_loss: 192398.3125\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 119606.3906 - val_loss: 149590.0781\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 135949.1094 - val_loss: 79897.7188\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 82804.3594 - val_loss: 82054.5000\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 85174.6562 - val_loss: 86494.7031\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 75696.6719 - val_loss: 87205.2891\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 81381.4766 - val_loss: 76677.8203\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 87222.2734 - val_loss: 75776.4375\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 76943.5312 - val_loss: 82471.7266\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 73302.8984 - val_loss: 85564.5000\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 80309.7812 - val_loss: 72623.6797\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 69466.1719 - val_loss: 71896.7812\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 68361.8125 - val_loss: 72538.8984\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 68802.7422 - val_loss: 69385.7109\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 66810.7500 - val_loss: 69751.6875\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 71316.6250 - val_loss: 68906.7891\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 73207.4297 - val_loss: 66808.5938\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 64034.4297 - val_loss: 71453.4141\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 69897.6016 - val_loss: 70566.4297\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 74637.5234 - val_loss: 64005.5938\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 62726.2500 - val_loss: 62972.2188\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 60329.4961 - val_loss: 62035.9062\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 59521.0859 - val_loss: 61081.3945\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 59139.5938 - val_loss: 62672.5820\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 59860.1289 - val_loss: 67522.9531\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 78355.0312 - val_loss: 67440.9844\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 77225.2109 - val_loss: 64474.3164\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 70454.9141 - val_loss: 57179.7852\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 56582.3672 - val_loss: 55084.7969\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 53210.4414 - val_loss: 56294.0234\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 56355.9141 - val_loss: 56012.8320\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 54267.0625 - val_loss: 52417.0586\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 49792.0703 - val_loss: 51559.4336\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 49447.3867 - val_loss: 51543.7383\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 48569.9570 - val_loss: 49749.3320\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 49595.7812 - val_loss: 60392.5352\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 61325.2148 - val_loss: 65753.1875\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 64031.6133 - val_loss: 113785.2344\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 121643.7266 - val_loss: 122300.9531\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 98960.8594 - val_loss: 91319.9453\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 63974.5039 - val_loss: 74190.3125\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 52529.2227 - val_loss: 52569.0430\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 45898.1250 - val_loss: 48673.2500\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 44221.1094 - val_loss: 40620.0938\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 41469.8008 - val_loss: 55342.4062\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 70672.9844 - val_loss: 70840.7109\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 62634.8164 - val_loss: 51325.1914\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 42536.3320 - val_loss: 43562.9570\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 39116.9375 - val_loss: 36301.5703\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 39071.5938 - val_loss: 36820.4805\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 41433.8203 - val_loss: 39336.3242\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 34616.8516 - val_loss: 46635.2188\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 43235.9375 - val_loss: 54289.5938\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 45373.3438 - val_loss: 59404.9531\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 45745.9961 - val_loss: 39789.5234\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 33536.7344 - val_loss: 30802.6602\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 34401.5781 - val_loss: 30021.4941\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 29209.9512 - val_loss: 29810.5176\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 28664.5176 - val_loss: 31818.6426\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 28748.6250 - val_loss: 28334.9902\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 26604.9629 - val_loss: 31722.2559\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 27914.7305 - val_loss: 32799.1680\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 27088.5703 - val_loss: 35341.0469\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 27657.5547 - val_loss: 27270.9258\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 29827.1387 - val_loss: 36075.7383\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 41041.8242 - val_loss: 46775.8398\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 34437.6016 - val_loss: 32542.7832\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 27696.2363 - val_loss: 24860.6191\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 25979.3535 - val_loss: 26515.3164\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 27358.9961 - val_loss: 25999.2832\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 22668.0762 - val_loss: 21905.5977\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 20759.5371 - val_loss: 21189.6992\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 21189.6504 - val_loss: 31390.6914\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 30604.1680 - val_loss: 25334.5664\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 23925.1406 - val_loss: 21209.2324\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 25461.5039 - val_loss: 28234.0469\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 22145.7109 - val_loss: 29181.3223\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 23988.1621 - val_loss: 17945.8203\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 22284.4688 - val_loss: 28813.9551\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 23460.0098 - val_loss: 19565.0586\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 18602.8535 - val_loss: 18495.3711\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 22906.5156 - val_loss: 30812.9551\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 22397.8770 - val_loss: 20363.5059\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 17194.6973 - val_loss: 15904.4111\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 18437.6777 - val_loss: 19703.8125\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 16798.4941 - val_loss: 15186.4199\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14458.8867 - val_loss: 14273.0176\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 14478.1426 - val_loss: 17594.9414\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 16375.5322 - val_loss: 17442.9023\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 15607.6230 - val_loss: 12726.8672\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 14100.7129 - val_loss: 14014.7129\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 12264.0664 - val_loss: 13259.5361\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 12699.1123 - val_loss: 11695.9170\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 11832.9424 - val_loss: 12084.2891\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 11295.0596 - val_loss: 11251.9297\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 11084.6553 - val_loss: 10627.0059\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 10295.3545 - val_loss: 10381.1816\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9981.0186 - val_loss: 10120.9795\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9755.9863 - val_loss: 10464.0332\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10665.6045 - val_loss: 9288.4043\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9581.0664 - val_loss: 8937.3594\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 9746.5928 - val_loss: 14268.9775\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 11503.1650 - val_loss: 8753.6611\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 9129.4414 - val_loss: 8019.4854\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 357887.5312 - val_loss: 797662.2500\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 433498.1562 - val_loss: 136580.7656\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 214334.4844 - val_loss: 239830.8125\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 149513.6719 - val_loss: 52335.8633\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 81240.1016 - val_loss: 106470.2344\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 60348.2500 - val_loss: 14112.4639\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 32375.5391 - val_loss: 35571.2656\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 20307.4375 - val_loss: 7354.2441\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 12723.8047 - val_loss: 12224.9141\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 9507.5059 - val_loss: 7805.7383\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 9813.1953 - val_loss: 7512.3750\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 7621.5078 - val_loss: 7042.2979\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6485.2202 - val_loss: 6235.7476\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 6639.2539 - val_loss: 6495.9502\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6150.1104 - val_loss: 6590.7681\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6159.0630 - val_loss: 12060.2637\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8320.2109 - val_loss: 5543.0557\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6305.6191 - val_loss: 10735.6738\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 10370.8945 - val_loss: 15577.4639\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9692.2285 - val_loss: 4891.6978\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 7041.8237 - val_loss: 5046.9263\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5589.1455 - val_loss: 5338.4785\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 4880.4028 - val_loss: 4312.4219\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 4520.1182 - val_loss: 4169.0278\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 4598.1597 - val_loss: 5905.8491\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 4730.4639 - val_loss: 4051.2131\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 5061.0752 - val_loss: 8744.6846\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5662.7759 - val_loss: 3906.5583\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5063.1494 - val_loss: 3346.0403\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3480.7400 - val_loss: 5382.0444\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 4357.7993 - val_loss: 3117.9731\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4244.4365 - val_loss: 3139.3933\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3030.7786 - val_loss: 3318.5298\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3039.5452 - val_loss: 3095.3879\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2768.8914 - val_loss: 2846.0083\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2788.9497 - val_loss: 3120.7471\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2961.0771 - val_loss: 2529.4185\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4235.9531 - val_loss: 5640.3203\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4342.0635 - val_loss: 3090.6792\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2736.3711 - val_loss: 2402.0022\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3099.9260 - val_loss: 2162.7834\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2614.3435 - val_loss: 3137.6121\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2411.0610 - val_loss: 2469.7207\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2526.3896 - val_loss: 4294.1133\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2994.7327 - val_loss: 2058.6113\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2281.2593 - val_loss: 2212.3347\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2647.3591 - val_loss: 1763.6392\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1936.1559 - val_loss: 2833.9854\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2342.7175 - val_loss: 1826.2490\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1661.1459 - val_loss: 2292.8621\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2153.3840 - val_loss: 2748.2737\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2134.6274 - val_loss: 2431.9963\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2417.4944 - val_loss: 3980.1316\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2669.6592 - val_loss: 1892.5327\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2368.6655 - val_loss: 1549.1436\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1492.2539 - val_loss: 2295.0564\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1554.5709 - val_loss: 1223.4456\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1531.1426 - val_loss: 1217.3295\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1336.1970 - val_loss: 2784.5591\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2300.2715 - val_loss: 2416.7559\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2573.8884 - val_loss: 1184.6006\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2236.1992 - val_loss: 2564.2773\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1890.0031 - val_loss: 1128.6591\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1003.2608 - val_loss: 1320.9701\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1393.5099 - val_loss: 1089.9624\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1239.9207 - val_loss: 1069.7723\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1665.0154 - val_loss: 807.0142\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1043.2166 - val_loss: 835.8667\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 832.5958 - val_loss: 1226.8893\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1092.2623 - val_loss: 1056.5988\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1094.6479 - val_loss: 729.1243\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 754.9059 - val_loss: 1323.9104\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 990.2424 - val_loss: 1014.2476\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 872.2903 - val_loss: 777.0046\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 835.3979 - val_loss: 677.3384\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 674.4548 - val_loss: 671.1252\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 743.2247 - val_loss: 570.0601\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 631.2959 - val_loss: 563.2262\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 573.5853 - val_loss: 594.4165\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 568.7101 - val_loss: 566.3539\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 555.2880 - val_loss: 503.1554\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 514.8731 - val_loss: 632.3075\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 591.9377 - val_loss: 481.9093\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 508.6862 - val_loss: 576.1385\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 509.3702 - val_loss: 487.0085\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 526.0877 - val_loss: 662.1923\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 614.7842 - val_loss: 633.5479\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 532.0331 - val_loss: 778.0208\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:49:08,267] Trial 27 finished with value: 0.14114151851496592 and parameters: {'n_layers': 1, 'n_neurons': 405, 'n_steps': 3, 'dropout_threshold': 0.22618750390239822}. Best is trial 23 with value: 0.07674217856264359.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(178, 3, 13) (178,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001   108.221931\n",
      "1       20002   151.754181\n",
      "2       20003   106.843025\n",
      "3       20004   253.700500\n",
      "4       20005  1273.204712\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14814814814814814, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(17.814814814814813, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(4.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7407407407407407, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.020740740740740744, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 349 neurons, 2 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 7523499.0000 - val_loss: 2496324.5000\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1373529.1250 - val_loss: 173198.9531\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 581965.2500 - val_loss: 293716.4062\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 160462.9375 - val_loss: 297611.5312\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 237516.5312 - val_loss: 85319.4922\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 153299.5625 - val_loss: 141103.0156\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 93652.0312 - val_loss: 149955.2656\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 124026.3906 - val_loss: 83243.0156\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 94091.2578 - val_loss: 86759.2891\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 77111.2031 - val_loss: 89488.7031\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 84067.6172 - val_loss: 77956.3750\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 76347.7656 - val_loss: 76717.2578\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 77108.0938 - val_loss: 79272.0000\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 74744.6562 - val_loss: 74518.4375\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 69631.0312 - val_loss: 77976.6250\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 74587.8672 - val_loss: 72596.9766\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 71682.3438 - val_loss: 71041.7500\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 69687.8203 - val_loss: 69488.4844\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 73331.8750 - val_loss: 68131.5625\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 65381.7188 - val_loss: 71314.9922\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 67138.3516 - val_loss: 67789.0859\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 64012.4648 - val_loss: 64343.7031\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 62440.7812 - val_loss: 63291.1602\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 60098.4023 - val_loss: 61858.4961\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 59544.8125 - val_loss: 61890.8867\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 61178.8281 - val_loss: 60343.0312\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 60550.0977 - val_loss: 57005.6328\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 56860.7734 - val_loss: 55886.9336\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 52625.4805 - val_loss: 54532.2773\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 52074.6172 - val_loss: 53245.7852\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 51217.5977 - val_loss: 51834.1289\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 49515.7695 - val_loss: 50229.9531\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 48362.0586 - val_loss: 50378.6758\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 50219.9531 - val_loss: 56547.5508\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 60229.0234 - val_loss: 63529.7148\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 61002.7695 - val_loss: 68545.6797\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 60309.2383 - val_loss: 59101.4102\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 51038.8359 - val_loss: 43518.1406\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 47314.4023 - val_loss: 41449.5156\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 40353.6875 - val_loss: 40382.9531\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 38945.5391 - val_loss: 40182.2656\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 38292.3945 - val_loss: 39221.4453\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 37187.3164 - val_loss: 37313.2695\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 35701.2500 - val_loss: 36651.7812\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 34894.2422 - val_loss: 35434.6523\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 34698.6406 - val_loss: 35365.2812\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 34170.6836 - val_loss: 35084.5391\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 33357.8672 - val_loss: 32710.5977\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 33541.5742 - val_loss: 33774.7109\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 32291.2129 - val_loss: 33519.3438\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 30484.5879 - val_loss: 30014.5215\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 28961.6680 - val_loss: 31357.6445\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 33241.9102 - val_loss: 39270.2617\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 34599.5859 - val_loss: 33655.2070\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 33273.4219 - val_loss: 26690.0449\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 30638.0938 - val_loss: 33790.0820\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 31951.4180 - val_loss: 31519.5703\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 28697.2070 - val_loss: 27013.1328\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 27316.2676 - val_loss: 25690.3145\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27502.6309 - val_loss: 29246.7832\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 24189.3809 - val_loss: 36191.5664\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 27399.2812 - val_loss: 23049.3340\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 23802.7617 - val_loss: 22963.9062\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 23263.7461 - val_loss: 29842.4961\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 24536.4473 - val_loss: 44473.6758\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 32520.4824 - val_loss: 41842.0977\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 27943.4121 - val_loss: 19857.6094\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 23875.5742 - val_loss: 20702.7168\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 22210.7871 - val_loss: 20192.8145\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 18850.1074 - val_loss: 16721.5742\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 17496.1777 - val_loss: 16748.6895\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 16297.6719 - val_loss: 16027.1055\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 15166.9355 - val_loss: 17039.0391\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 16035.8154 - val_loss: 14830.9775\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 14703.1094 - val_loss: 14450.4463\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 14001.0020 - val_loss: 13897.3076\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 13670.6953 - val_loss: 14761.5293\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 14020.6299 - val_loss: 19005.4688\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 14557.6357 - val_loss: 14341.8018\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 12694.9326 - val_loss: 11713.4678\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 11893.5361 - val_loss: 12680.7158\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 11560.3135 - val_loss: 10792.5654\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 10708.0801 - val_loss: 10518.9639\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11362.8672 - val_loss: 14447.5146\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 12114.1758 - val_loss: 9421.3398\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 10134.7871 - val_loss: 9449.4629\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9029.9219 - val_loss: 8983.8213\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8548.2578 - val_loss: 8369.5635\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8470.9922 - val_loss: 10168.7324\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9073.0645 - val_loss: 7642.7871\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8008.8491 - val_loss: 8966.6426\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7710.5913 - val_loss: 10612.4092\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9045.1504 - val_loss: 7006.0664\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7253.5269 - val_loss: 6941.3491\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6662.5947 - val_loss: 6964.0376\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6494.4077 - val_loss: 6036.9702\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5921.5742 - val_loss: 6114.4180\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5815.2378 - val_loss: 5959.1074\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6394.2944 - val_loss: 5362.1567\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5421.3887 - val_loss: 6977.9683\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6048.1465 - val_loss: 5569.8477\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6405.2632 - val_loss: 9201.3105\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6194.6714 - val_loss: 4466.4062\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4675.7891 - val_loss: 5674.1865\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4531.9092 - val_loss: 4075.3860\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5092.3389 - val_loss: 5553.5176\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4413.3721 - val_loss: 3770.8735\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3969.9177 - val_loss: 4264.8740\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3576.0271 - val_loss: 4363.4214\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3543.2192 - val_loss: 3527.1091\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4014.6201 - val_loss: 3922.1409\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3359.9858 - val_loss: 2977.6157\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2944.0146 - val_loss: 3195.4985\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2961.3962 - val_loss: 2765.9714\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2581.5310 - val_loss: 2973.1018\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2913.5623 - val_loss: 2463.7070\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3241.9866 - val_loss: 2661.7419\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2344.5520 - val_loss: 2617.2988\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2649.9006 - val_loss: 2433.5342\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2356.1873 - val_loss: 3597.8350\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3080.3784 - val_loss: 1921.5988\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2029.6595 - val_loss: 1972.6160\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2095.4163 - val_loss: 2544.6899\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2227.5166 - val_loss: 1737.5446\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1933.2018 - val_loss: 1588.0118\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1758.6366 - val_loss: 2484.3799\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2048.9197 - val_loss: 1583.4303\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1578.9369 - val_loss: 1698.6008\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1414.7196 - val_loss: 1693.9911\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1681.0869 - val_loss: 1439.7219\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1375.7646 - val_loss: 1351.4231\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1262.8335 - val_loss: 1115.6046\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1088.4282 - val_loss: 1078.1715\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1099.8756 - val_loss: 1059.4778\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1035.3973 - val_loss: 974.9324\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 946.9572 - val_loss: 1099.9728\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 985.2927 - val_loss: 1286.8748\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1165.0247 - val_loss: 923.9119\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 875.3468 - val_loss: 1267.2588\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1358.0298 - val_loss: 771.6729\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 791.6509 - val_loss: 747.0241\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 805.2541 - val_loss: 769.1956\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 689.1201 - val_loss: 694.4191\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 698.3860 - val_loss: 959.7556\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 805.8823 - val_loss: 846.8503\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 915.0061 - val_loss: 623.8804\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 951.8215 - val_loss: 745.5255\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 677.7759 - val_loss: 594.1309\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 570.4805 - val_loss: 567.0485\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 610.6137 - val_loss: 570.1111\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 539.6439 - val_loss: 540.4493\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 528.8168 - val_loss: 479.3120\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 476.1215 - val_loss: 551.3818\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 565.3326 - val_loss: 554.5825\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 550.1946 - val_loss: 660.6023\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 710.9844 - val_loss: 577.0307\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 520.9622 - val_loss: 599.7287\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 474.7997 - val_loss: 472.9368\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 424.5267 - val_loss: 378.1769\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 371.2009 - val_loss: 348.0449\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 351.6035 - val_loss: 380.9540\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 383.8607 - val_loss: 358.9847\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 449.9676 - val_loss: 318.8638\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 323.0370 - val_loss: 304.5230\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 322.3837 - val_loss: 315.4093\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 332.1875 - val_loss: 396.3044\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 381.9297 - val_loss: 327.6018\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 306.5375 - val_loss: 409.0229\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 326.7811 - val_loss: 333.9341\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 410.3905 - val_loss: 261.8296\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 415.5401 - val_loss: 322.6323\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 281.1858 - val_loss: 266.9236\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 262.1879 - val_loss: 252.4231\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 252.2926 - val_loss: 265.0933\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 267.7167 - val_loss: 231.6306\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 268.2205 - val_loss: 538.3585\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 358.4957 - val_loss: 450.6444\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 348.5047 - val_loss: 273.1760\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 241.4945 - val_loss: 214.4435\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 222.2338 - val_loss: 350.5876\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 303.9198 - val_loss: 555.4864\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 436.2201 - val_loss: 408.3647\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 373.9028 - val_loss: 305.2063\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 476.4472 - val_loss: 200.5893\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 453.4954 - val_loss: 213.7477\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 396.3335 - val_loss: 194.7186\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 423.0447 - val_loss: 216.1219\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 314.9790 - val_loss: 280.0829\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 323.6079 - val_loss: 203.8369\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 298.8267 - val_loss: 200.7505\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 276.8557 - val_loss: 251.7317\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 264.8799 - val_loss: 182.8944\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 195.7827 - val_loss: 256.5167\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 194.0153 - val_loss: 302.0853\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 242.1199 - val_loss: 183.2800\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 196.3963 - val_loss: 322.6855\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 246.9500 - val_loss: 217.4118\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 195.0069 - val_loss: 338.3817\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 337.4711 - val_loss: 471.8528\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 353.9657 - val_loss: 430.3523\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:49:15,703] Trial 28 finished with value: 0.052441262810946444 and parameters: {'n_layers': 1, 'n_neurons': 349, 'n_steps': 2, 'dropout_threshold': 0.1383753113327677}. Best is trial 28 with value: 0.052441262810946444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(179, 2, 13) (179,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001    70.313591\n",
      "1       20002   115.346329\n",
      "2       20003    69.253532\n",
      "3       20004   196.248810\n",
      "4       20005  1266.190918\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14285714285714285, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(17.178571428571427, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(4.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7142857142857143, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 287 neurons, 2 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 3420785.2500 - val_loss: 3165634.5000\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1597999.8750 - val_loss: 343143.2188\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 768244.4375 - val_loss: 347493.5938\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 169216.6250 - val_loss: 444200.1562\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 292633.3125 - val_loss: 97217.0156\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 215223.1406 - val_loss: 150439.6094\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 100409.4609 - val_loss: 165667.8438\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 130094.1328 - val_loss: 80571.0781\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 92398.8359 - val_loss: 76069.6172\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 83102.8594 - val_loss: 83867.6953\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 80831.8125 - val_loss: 80559.9844\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 77485.8906 - val_loss: 74964.4531\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 71623.0859 - val_loss: 72854.1641\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 69298.4609 - val_loss: 73430.4844\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 69138.0859 - val_loss: 70534.6953\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 68771.0547 - val_loss: 71236.9688\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 66954.9688 - val_loss: 67576.7500\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 64408.4102 - val_loss: 68766.6172\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 66807.7891 - val_loss: 65774.7891\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 64866.9219 - val_loss: 64483.0664\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 60814.5859 - val_loss: 66202.7812\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 63837.6406 - val_loss: 62162.4961\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 60094.4375 - val_loss: 61894.9805\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 60078.0625 - val_loss: 60933.1172\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 61125.5430 - val_loss: 59521.0312\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 58482.8281 - val_loss: 59288.3906\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 59499.3477 - val_loss: 61060.3594\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 59823.6055 - val_loss: 58115.2148\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 58778.8281 - val_loss: 55024.0117\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 52811.5469 - val_loss: 54151.6562\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51664.9883 - val_loss: 53275.3828\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51831.0195 - val_loss: 53874.5781\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 52409.9219 - val_loss: 50774.3789\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 48983.9453 - val_loss: 50040.7578\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 48710.9883 - val_loss: 49727.7461\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 47250.4336 - val_loss: 48771.7734\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 46398.1406 - val_loss: 47996.9219\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 49442.3945 - val_loss: 48939.2656\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 46623.2422 - val_loss: 55713.4219\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51608.6445 - val_loss: 57747.0781\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51224.8125 - val_loss: 49892.2617\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43090.8867 - val_loss: 47335.3242\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43423.9219 - val_loss: 42762.7969\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40433.5312 - val_loss: 43462.3125\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 42948.0469 - val_loss: 42232.8242\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43185.6406 - val_loss: 45415.4023\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 41722.5078 - val_loss: 55289.7930\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 44120.3086 - val_loss: 53323.0078\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40798.7891 - val_loss: 53879.5156\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 43964.7812 - val_loss: 37588.9766\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 40319.5195 - val_loss: 35156.7852\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 34210.0859 - val_loss: 34784.7930\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 33091.1172 - val_loss: 35318.9883\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 34640.9531 - val_loss: 34293.0078\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31764.7715 - val_loss: 33649.8359\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32819.1172 - val_loss: 31617.3379\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30176.1641 - val_loss: 30587.8496\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 31147.9512 - val_loss: 31323.5918\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28522.0117 - val_loss: 34143.5430\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 30030.4551 - val_loss: 34271.0234\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30952.2676 - val_loss: 30764.5332\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 29176.6582 - val_loss: 28082.1250\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29984.6914 - val_loss: 35473.5508\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30112.4902 - val_loss: 41031.0508\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31296.8594 - val_loss: 33508.5703\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27153.4609 - val_loss: 28387.5879\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26897.4180 - val_loss: 24248.0625\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24265.5723 - val_loss: 24401.9883\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23517.4277 - val_loss: 22966.4375\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21998.5645 - val_loss: 23347.1250\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21989.2891 - val_loss: 23210.1973\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22012.0664 - val_loss: 20623.1367\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19649.7852 - val_loss: 19868.9980\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18908.6035 - val_loss: 19899.6250\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18977.5566 - val_loss: 20830.6348\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18976.7148 - val_loss: 18398.5742\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19063.5410 - val_loss: 23337.8828\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19020.4922 - val_loss: 19929.9590\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16472.0000 - val_loss: 20144.5684\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18002.4043 - val_loss: 17824.0879\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 16110.0625 - val_loss: 16310.2705\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15957.0547 - val_loss: 18882.7988\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 15825.5547 - val_loss: 19333.5352\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 15641.2207 - val_loss: 15990.1816\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16832.4609 - val_loss: 17402.7285\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 15085.6416 - val_loss: 23383.9199\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18620.6660 - val_loss: 13470.6514\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18358.2578 - val_loss: 18980.4238\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17383.0820 - val_loss: 12298.8760\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11819.5889 - val_loss: 11877.1104\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11264.0742 - val_loss: 11441.9404\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13359.4482 - val_loss: 14385.8555\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 13177.2305 - val_loss: 10660.8594\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10840.7588 - val_loss: 12393.3701\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11046.0059 - val_loss: 17156.1543\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11672.3574 - val_loss: 15652.2100\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12373.9043 - val_loss: 9690.5371\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9898.6768 - val_loss: 14462.9316\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11200.2715 - val_loss: 10233.5791\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9208.9619 - val_loss: 8978.1641\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8190.4648 - val_loss: 11373.4775\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9715.8232 - val_loss: 7890.9990\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9071.1738 - val_loss: 15471.4805\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 10552.2295 - val_loss: 7304.5815\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9291.2998 - val_loss: 7674.5376\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7027.5654 - val_loss: 6828.0576\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6576.3467 - val_loss: 6606.1724\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7282.6714 - val_loss: 10577.7119\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8160.6118 - val_loss: 7141.4443\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7664.0474 - val_loss: 7168.1196\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7477.8491 - val_loss: 12007.8662\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7980.2349 - val_loss: 6932.7642\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7769.0669 - val_loss: 5469.2686\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5278.7539 - val_loss: 6921.7202\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6550.1138 - val_loss: 6953.8804\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4850.5249 - val_loss: 4825.0146\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5282.8550 - val_loss: 4301.6943\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4236.7471 - val_loss: 4310.1782\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4432.3813 - val_loss: 5805.1641\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4795.8071 - val_loss: 6297.9019\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4869.0522 - val_loss: 3870.6787\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4087.2627 - val_loss: 6007.4380\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4404.8506 - val_loss: 3314.8254\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3731.8411 - val_loss: 10422.2725\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7561.6675 - val_loss: 4625.1548\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6818.2896 - val_loss: 3189.2725\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5311.2598 - val_loss: 4069.6213\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4205.9346 - val_loss: 8149.2832\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10698.3018 - val_loss: 9649.3350\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6405.8794 - val_loss: 7894.2568\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6730.7368 - val_loss: 2899.3806\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5665.0698 - val_loss: 6090.1172\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5159.8247 - val_loss: 12328.0381\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8602.2305 - val_loss: 3202.8425\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5041.9512 - val_loss: 2558.0896\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2495.3398 - val_loss: 2560.3726\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2149.0798 - val_loss: 3090.9595\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2578.9333 - val_loss: 2065.6555\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2640.6580 - val_loss: 2487.5781\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1892.5554 - val_loss: 1701.7062\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1543.5909 - val_loss: 1488.2611\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1394.1672 - val_loss: 1525.2665\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1548.4675 - val_loss: 1319.5732\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1422.9617 - val_loss: 1311.9181\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1449.4102 - val_loss: 1845.3772\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1497.6715 - val_loss: 1621.0769\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1659.9066 - val_loss: 1459.3011\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1838.4840 - val_loss: 1166.2762\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1447.6379 - val_loss: 1376.3290\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1375.4347 - val_loss: 1284.7535\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 996.7768 - val_loss: 909.8894\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 893.0732 - val_loss: 946.1920\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 889.5760 - val_loss: 806.5218\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 835.4210 - val_loss: 990.4498\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 845.0925 - val_loss: 722.6289\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 829.1025 - val_loss: 1473.2231\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1039.8392 - val_loss: 2788.0613\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1976.6298 - val_loss: 684.4570\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 709.9344 - val_loss: 880.0022\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 857.8142 - val_loss: 1016.3427\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 673.9499 - val_loss: 568.5723\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 536.5797 - val_loss: 736.2312\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 617.2325 - val_loss: 510.7772\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 513.1677 - val_loss: 506.1722\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 495.3646 - val_loss: 457.9708\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 477.3827 - val_loss: 588.5965\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 528.5884 - val_loss: 433.8554\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 414.7216 - val_loss: 456.0114\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 470.9418 - val_loss: 569.7764\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 482.6397 - val_loss: 791.1321\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 627.5738 - val_loss: 856.4247\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 626.2180 - val_loss: 345.8229\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 365.6838 - val_loss: 375.3697\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 460.1328 - val_loss: 454.7509\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 465.8880 - val_loss: 305.8690\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 306.2912 - val_loss: 335.3505\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 318.2875 - val_loss: 282.7082\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 305.4559 - val_loss: 275.3613\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 276.9118 - val_loss: 374.1518\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 304.5256 - val_loss: 290.9556\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 272.5989 - val_loss: 247.0141\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 259.2086 - val_loss: 239.0640\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 260.6419 - val_loss: 233.3079\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 237.2740 - val_loss: 262.3194\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 267.5929 - val_loss: 300.8820\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 240.1096 - val_loss: 560.0772\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 533.6312 - val_loss: 410.6121\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 316.8076 - val_loss: 329.6825\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 266.8384 - val_loss: 267.1502\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 246.6642 - val_loss: 464.7646\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 387.3206 - val_loss: 601.7401\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 382.2325 - val_loss: 288.0162\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 284.9428 - val_loss: 256.5245\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(179, 2, 13) (179,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001    88.079178\n",
      "1       20002   131.896484\n",
      "2       20003    86.457542\n",
      "3       20004   219.408752\n",
      "4       20005  1260.428833\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:49:22,370] Trial 29 finished with value: 0.07639095516727788 and parameters: {'n_layers': 1, 'n_neurons': 287, 'n_steps': 2, 'dropout_threshold': 0.1550245965809626}. Best is trial 28 with value: 0.052441262810946444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13793103448275862, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(16.586206896551722, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(4.0, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6896551724137931, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.019310344827586208, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 2 layers, 350 neurons, 2 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 38ms/step - loss: 1257821.3750 - val_loss: 103478.0625\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 820930.3125 - val_loss: 1094908.0000\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 899620.1875 - val_loss: 395598.6250\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 517651.9062 - val_loss: 1127553.1250\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 522724.3750 - val_loss: 349658.8125\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 466674.8438 - val_loss: 184098.6094\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 344822.3750 - val_loss: 128507.8828\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 321092.6250 - val_loss: 110420.1328\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 220810.2500 - val_loss: 105745.1328\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 274837.6250 - val_loss: 151140.8594\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 234255.3438 - val_loss: 84532.8984\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 232828.2656 - val_loss: 101836.5938\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 185597.0312 - val_loss: 89520.7344\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 165963.2344 - val_loss: 115123.5469\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 161148.2188 - val_loss: 131868.5625\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 126960.4531 - val_loss: 87222.6562\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 132300.3594 - val_loss: 95131.7734\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207781.2188 - val_loss: 96978.4297\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 163856.5312 - val_loss: 88600.7188\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 141984.6406 - val_loss: 93776.6484\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 127077.2344 - val_loss: 85077.2188\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:49:24,874] Trial 30 finished with value: 0.786630555210648 and parameters: {'n_layers': 2, 'n_neurons': 350, 'n_steps': 2, 'dropout_threshold': 0.16413636521371072}. Best is trial 28 with value: 0.052441262810946444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(179, 2, 13) (179,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  187.344757\n",
      "1       20002  187.332443\n",
      "2       20003  187.343307\n",
      "3       20004  189.873581\n",
      "4       20005  187.233521\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13793103448275862, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(16.586206896551722, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(3.3333333333333335, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6896551724137931, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.019310344827586208, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 289 neurons, 2 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 7402306.5000 - val_loss: 5014249.5000\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 3812903.7500 - val_loss: 128383.1797\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 706146.7500 - val_loss: 1645059.6250\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 865137.5625 - val_loss: 159505.6406\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 472427.4062 - val_loss: 416536.7500\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 185968.6406 - val_loss: 270128.2500\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 277969.5000 - val_loss: 100357.1953\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 112485.0625 - val_loss: 175891.7656\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 127637.6562 - val_loss: 107868.1875\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 110410.9062 - val_loss: 85836.5703\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 78467.6094 - val_loss: 91027.1328\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 82829.5312 - val_loss: 84063.3281\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 80329.6562 - val_loss: 83612.7812\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 78871.0703 - val_loss: 81636.9844\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 79442.8516 - val_loss: 80755.1797\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 79586.2578 - val_loss: 82340.0703\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 80263.1797 - val_loss: 79552.7891\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 77834.0625 - val_loss: 80910.9453\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 77645.7578 - val_loss: 79732.5000\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 76833.1953 - val_loss: 77792.9219\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 74398.1250 - val_loss: 77443.0625\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 76901.9062 - val_loss: 76763.3594\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 80351.3359 - val_loss: 75505.9141\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 92811.4766 - val_loss: 75154.0000\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 132624.3281 - val_loss: 75014.1328\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 95805.8359 - val_loss: 78079.7812\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 94134.1094 - val_loss: 92161.0547\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 79718.1797 - val_loss: 88196.1328\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 78141.6797 - val_loss: 76853.2188\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 74304.4531 - val_loss: 74665.9297\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 70100.2812 - val_loss: 74162.9375\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 67841.4531 - val_loss: 75981.5469\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 68747.8281 - val_loss: 71198.0469\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 67154.1562 - val_loss: 68650.1641\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 65284.4570 - val_loss: 66206.8672\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 62970.7070 - val_loss: 65531.9922\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 64469.8398 - val_loss: 64886.3203\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 65554.0703 - val_loss: 66001.8203\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 70854.2500 - val_loss: 64868.1719\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 69692.5703 - val_loss: 63368.2891\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 61629.3555 - val_loss: 62033.9062\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 67715.4844 - val_loss: 61424.7227\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 62150.0977 - val_loss: 61135.6445\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 58962.0898 - val_loss: 59931.7031\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 58336.4219 - val_loss: 59227.1602\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 56342.8789 - val_loss: 59989.5391\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 60451.8672 - val_loss: 60958.0430\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 58337.6836 - val_loss: 58216.1133\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 56505.3398 - val_loss: 64639.4453\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 67778.5078 - val_loss: 55483.8477\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 54110.8672 - val_loss: 55335.2891\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 58451.0781 - val_loss: 54249.2891\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 52378.9336 - val_loss: 54972.8633\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 59801.8555 - val_loss: 56511.8125\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 59032.3711 - val_loss: 54303.3828\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51372.1836 - val_loss: 51843.9570\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 49102.1094 - val_loss: 56553.6055\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 60175.5625 - val_loss: 57571.7227\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51745.9648 - val_loss: 50186.8164\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 48500.6133 - val_loss: 53291.7852\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 49585.5352 - val_loss: 51858.0938\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 48993.7422 - val_loss: 53312.1758\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 47447.6719 - val_loss: 46426.7109\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 44855.5547 - val_loss: 46558.5742\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 45052.9570 - val_loss: 44606.4570\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 42605.9648 - val_loss: 44478.2148\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 42839.3555 - val_loss: 48785.9258\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 49831.3359 - val_loss: 50821.5742\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43138.5508 - val_loss: 52576.2500\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 44704.9648 - val_loss: 61792.3203\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51103.7930 - val_loss: 53008.0898\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 46172.4648 - val_loss: 40241.8633\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 46776.0469 - val_loss: 43592.0117\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43459.0781 - val_loss: 61200.8477\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 55210.5352 - val_loss: 42996.5039\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 39840.9375 - val_loss: 37445.7734\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 41180.5430 - val_loss: 36800.6523\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35909.3984 - val_loss: 36549.0898\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35830.6133 - val_loss: 35652.1367\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 34501.9961 - val_loss: 46650.9961\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 50527.8398 - val_loss: 58456.4727\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 46345.3125 - val_loss: 68785.6875\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43508.7148 - val_loss: 57382.5625\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 41467.7227 - val_loss: 39348.7617\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43787.6367 - val_loss: 40302.8125\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 49182.4180 - val_loss: 46956.9297\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 36218.1094 - val_loss: 34481.8594\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32707.0977 - val_loss: 31123.2637\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33531.7695 - val_loss: 33609.5312\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30704.7871 - val_loss: 29646.1621\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29758.0156 - val_loss: 28797.4902\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27725.1152 - val_loss: 28464.9473\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27159.2129 - val_loss: 28129.3203\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29027.0977 - val_loss: 28050.7012\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 28353.4395 - val_loss: 26916.0508\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 26320.6191 - val_loss: 26861.2480\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25602.8867 - val_loss: 26149.7695\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25102.2637 - val_loss: 28623.8516\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27248.5820 - val_loss: 28112.6211\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 27146.2539 - val_loss: 24770.8379\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 23590.2227 - val_loss: 24101.4062\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23306.0566 - val_loss: 23627.4844\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 22697.9805 - val_loss: 26285.4492\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26617.8965 - val_loss: 22636.8008\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31698.7012 - val_loss: 29239.7461\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22507.8516 - val_loss: 22557.4727\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21697.3145 - val_loss: 22016.8047\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21286.9590 - val_loss: 21002.1738\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20491.4883 - val_loss: 20275.8926\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19328.8770 - val_loss: 19931.4922\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20505.1699 - val_loss: 25363.8691\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21788.7109 - val_loss: 19747.5332\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20285.9277 - val_loss: 18912.1719\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17800.7559 - val_loss: 18539.4141\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17880.4902 - val_loss: 20482.3730\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22453.9727 - val_loss: 19770.7109\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16799.2480 - val_loss: 18404.7461\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16679.2676 - val_loss: 17083.0566\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17741.9004 - val_loss: 20545.1914\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17586.2734 - val_loss: 20346.3145\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19975.5625 - val_loss: 18193.7871\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23654.3594 - val_loss: 36455.6641\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24617.6133 - val_loss: 15499.0771\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18211.7246 - val_loss: 29881.0469\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28435.7598 - val_loss: 22658.2285\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 18747.7402 - val_loss: 17083.0059\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18071.5059 - val_loss: 17569.0176\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15849.1543 - val_loss: 21756.3027\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31803.5312 - val_loss: 18332.0625\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15710.1064 - val_loss: 14855.0439\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16610.8203 - val_loss: 33562.2695\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22709.8867 - val_loss: 14048.8975\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14463.5645 - val_loss: 11836.9570\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10700.7090 - val_loss: 12957.1719\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13229.3066 - val_loss: 10645.9922\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10532.2178 - val_loss: 14459.9053\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 14594.1299 - val_loss: 12037.1445\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12408.3320 - val_loss: 14758.5166\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12532.3926 - val_loss: 16943.0215\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13252.6475 - val_loss: 9228.5674\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9523.1885 - val_loss: 13689.5059\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11581.3008 - val_loss: 10340.3418\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9709.0283 - val_loss: 8581.9600\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8233.9756 - val_loss: 8836.6953\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9356.0723 - val_loss: 9607.5293\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10193.7451 - val_loss: 8546.9805\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7940.3994 - val_loss: 8035.3550\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7700.8027 - val_loss: 9077.9453\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9444.6104 - val_loss: 22296.9922\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 24007.6562 - val_loss: 18457.1484\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16317.3809 - val_loss: 18283.4902\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14454.9971 - val_loss: 7544.5400\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10428.1270 - val_loss: 16948.6816\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13327.9082 - val_loss: 6786.4482\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11021.9609 - val_loss: 6690.5103\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9240.5420 - val_loss: 9058.1123\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7417.1904 - val_loss: 6048.6792\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5811.4688 - val_loss: 5730.3955\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5542.7666 - val_loss: 9665.4873\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7917.4946 - val_loss: 6563.6333\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5218.4312 - val_loss: 5452.5610\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5168.3926 - val_loss: 5053.0698\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5264.4702 - val_loss: 4858.6318\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5006.6396 - val_loss: 5304.6377\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4831.7510 - val_loss: 4644.9487\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4975.0283 - val_loss: 4451.1304\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4365.1455 - val_loss: 4951.5068\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4603.2959 - val_loss: 4321.8574\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4793.5259 - val_loss: 6052.2944\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5188.9102 - val_loss: 6344.4580\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6405.0015 - val_loss: 3963.3989\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5463.1797 - val_loss: 4811.8774\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5442.0977 - val_loss: 6117.0903\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4453.2256 - val_loss: 3558.0100\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4350.0786 - val_loss: 3521.5552\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3502.9307 - val_loss: 4563.5068\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4006.9211 - val_loss: 5124.6899\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4650.3999 - val_loss: 3300.0881\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3918.9683 - val_loss: 5983.8208\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5031.6147 - val_loss: 4180.0044\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4177.6152 - val_loss: 3893.9951\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3365.3696 - val_loss: 5466.5835\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4495.8774 - val_loss: 2936.5698\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3117.7729 - val_loss: 2987.6279\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2893.6331 - val_loss: 3144.7766\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2622.8057 - val_loss: 2520.9050\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2947.6306 - val_loss: 2829.0317\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2560.8765 - val_loss: 2304.5806\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2239.2239 - val_loss: 4484.9404\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3945.3293 - val_loss: 2157.7058\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2587.3562 - val_loss: 2667.8633\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2231.6255 - val_loss: 2020.1838\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1971.8733 - val_loss: 2389.4956\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2164.7002 - val_loss: 1931.9493\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2214.6511 - val_loss: 2000.9556\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2160.9561 - val_loss: 1806.1139\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2352.8623 - val_loss: 2596.9143\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2366.2156 - val_loss: 2051.9609\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1882.7816 - val_loss: 1649.0414\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1757.6108 - val_loss: 1661.2792\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(179, 2, 13) (179,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001    87.736076\n",
      "1       20002   127.303825\n",
      "2       20003    86.785721\n",
      "3       20004   245.355713\n",
      "4       20005  1147.603271\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:49:31,784] Trial 31 finished with value: 0.08440980052114719 and parameters: {'n_layers': 1, 'n_neurons': 289, 'n_steps': 2, 'dropout_threshold': 0.1405315788982752}. Best is trial 28 with value: 0.052441262810946444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13333333333333333, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(16.033333333333335, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(3.3333333333333335, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.018666666666666668, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 289 neurons, 4 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 9013017.0000 - val_loss: 2312933.2500\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 688032.5000 - val_loss: 150550.9844\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 404714.9062 - val_loss: 423390.1875\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 233658.1406 - val_loss: 82652.1328\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 122989.2969 - val_loss: 106534.6094\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 83208.5938 - val_loss: 84736.9531\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 65353.4883 - val_loss: 63077.1250\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 61903.9922 - val_loss: 60594.2891\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 58729.7969 - val_loss: 57531.2461\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 58055.2422 - val_loss: 92821.2031\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 64696.6992 - val_loss: 52441.5078\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 54832.1172 - val_loss: 73348.4297\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 74533.7812 - val_loss: 47500.1992\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 45178.4258 - val_loss: 69985.2891\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 55543.3594 - val_loss: 71143.3984\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 67210.6484 - val_loss: 56545.7539\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 48927.4102 - val_loss: 197143.2031\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 199255.2969 - val_loss: 300425.6875\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 205396.4219 - val_loss: 556912.1875\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 364292.2812 - val_loss: 215986.4375\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 177458.3750 - val_loss: 110997.9609\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 83935.8438 - val_loss: 36646.5977\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 50720.5469 - val_loss: 41877.0039\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 39839.3242 - val_loss: 30677.2852\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 30276.2188 - val_loss: 22200.8750\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 21084.4297 - val_loss: 26546.9512\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 20973.2129 - val_loss: 31716.7695\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 27606.1816 - val_loss: 48547.7734\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 26044.2871 - val_loss: 38130.2773\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 29836.2441 - val_loss: 61022.9805\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 35861.1562 - val_loss: 59416.1719\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 181370.5469 - val_loss: 155888.5156\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 128766.6797 - val_loss: 124144.9141\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 60113.7461 - val_loss: 32285.7129\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 28075.7715 - val_loss: 14390.0723\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 29478.1602 - val_loss: 56031.0078\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 35447.8438 - val_loss: 31489.5547\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 18570.0762 - val_loss: 14138.0361\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 19735.0898 - val_loss: 10312.1719\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 14621.9492 - val_loss: 9197.1445\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8772.2705 - val_loss: 13916.9824\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 9275.0898 - val_loss: 9296.1875\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9053.9756 - val_loss: 7700.9678\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8067.0356 - val_loss: 15239.5537\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 11976.9512 - val_loss: 7173.5044\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7638.4067 - val_loss: 6356.6519\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6042.5356 - val_loss: 6292.3779\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6092.5410 - val_loss: 8494.5801\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6946.6650 - val_loss: 7262.6963\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5840.3979 - val_loss: 5140.1821\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5155.4810 - val_loss: 5817.1997\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6657.0137 - val_loss: 8608.6572\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8420.7832 - val_loss: 4247.6499\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5521.4648 - val_loss: 3519.5505\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3865.0005 - val_loss: 5837.4116\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3827.9753 - val_loss: 4542.9575\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3725.9966 - val_loss: 6189.1924\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6274.3169 - val_loss: 2898.0449\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3342.8638 - val_loss: 5611.5015\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4426.7778 - val_loss: 2346.4644\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3129.1357 - val_loss: 11517.9150\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6972.7417 - val_loss: 7498.2529\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4592.4683 - val_loss: 2029.0819\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2821.1484 - val_loss: 2688.2375\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3175.2263 - val_loss: 2507.8479\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3305.4390 - val_loss: 2871.1831\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2151.7251 - val_loss: 2929.6934\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2728.6777 - val_loss: 1292.5728\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2111.1597 - val_loss: 1382.9207\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1358.7151 - val_loss: 4051.0681\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2564.9641 - val_loss: 8322.4990\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4370.0444 - val_loss: 2913.0083\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2071.5188 - val_loss: 1023.3070\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 946.4330 - val_loss: 2315.8264\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1673.2920 - val_loss: 4182.4614\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2263.8088 - val_loss: 2290.0093\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2070.6318 - val_loss: 785.7086\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1434.4548 - val_loss: 2922.1111\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1942.5013 - val_loss: 9531.5127\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4253.7485 - val_loss: 11028.3672\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5838.8579 - val_loss: 4436.8999\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3470.8040 - val_loss: 747.9495\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1578.1761 - val_loss: 663.7153\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 978.1581 - val_loss: 843.7960\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1268.0325 - val_loss: 1400.4099\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1516.7208 - val_loss: 4446.8779\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1936.1011 - val_loss: 1438.2545\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1004.0883 - val_loss: 1739.7257\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1245.7695 - val_loss: 3053.9983\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2107.0647 - val_loss: 2307.0701\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1965.7444 - val_loss: 526.1448\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 925.4194 - val_loss: 1151.9441\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 817.9305 - val_loss: 1021.3299\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 902.2919 - val_loss: 661.8605\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 715.8243 - val_loss: 865.7517\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 802.5569 - val_loss: 1509.0281\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1422.9869 - val_loss: 465.8782\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1487.0815 - val_loss: 425.5294\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 481.2136 - val_loss: 638.8510\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 931.5225 - val_loss: 506.4762\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 571.2680 - val_loss: 504.1892\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 975.6345 - val_loss: 439.3001\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1297.4124 - val_loss: 658.9194\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 734.4749 - val_loss: 562.6104\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 656.4960 - val_loss: 2468.8645\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1333.4313 - val_loss: 389.8831\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1683.1547 - val_loss: 3813.2874\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2020.3348 - val_loss: 2822.8650\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2578.0259 - val_loss: 708.6790\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1345.1846 - val_loss: 1173.6396\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1582.8037 - val_loss: 2325.4507\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1883.0957 - val_loss: 14970.0049\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 7677.2324 - val_loss: 380.2309\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2463.4307 - val_loss: 2268.6042\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1178.9337 - val_loss: 470.6633\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 735.5416 - val_loss: 1254.6663\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1668.3997 - val_loss: 400.9309\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 670.6041 - val_loss: 391.7740\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 514.4633 - val_loss: 373.9294\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 486.7962 - val_loss: 797.7592\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 653.3607 - val_loss: 8114.7896\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3801.7654 - val_loss: 1942.0642\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1461.7668 - val_loss: 546.5316\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 491.8015 - val_loss: 369.4753\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 518.8830 - val_loss: 1087.8844\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 478.4397 - val_loss: 371.3759\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 507.8558 - val_loss: 379.6425\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 574.5059 - val_loss: 569.4986\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 503.4413 - val_loss: 1183.5375\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1015.6978 - val_loss: 1827.5907\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1167.3601 - val_loss: 724.2352\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1684.8524 - val_loss: 1330.5785\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 769.4610 - val_loss: 614.0482\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 698.0229 - val_loss: 448.9626\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:49:37,446] Trial 32 finished with value: 0.09344123364500556 and parameters: {'n_layers': 1, 'n_neurons': 289, 'n_steps': 4, 'dropout_threshold': 0.20745753286194285}. Best is trial 28 with value: 0.052441262810946444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(177, 4, 13) (177,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001    87.990463\n",
      "1       20002   133.439682\n",
      "2       20003    87.005745\n",
      "3       20004   209.313309\n",
      "4       20005  1296.816528\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12903225806451613, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(15.516129032258064, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(3.3333333333333335, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6451612903225806, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01806451612903226, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 338 neurons, 2 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 42295968.0000 - val_loss: 2893317.7500\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 11235740.0000 - val_loss: 12694486.0000\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6505009.5000 - val_loss: 98007.3672\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1520658.2500 - val_loss: 3657509.0000\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2667761.7500 - val_loss: 379236.0625\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 259708.7812 - val_loss: 833847.5625\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 831444.8750 - val_loss: 299646.1250\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 144089.2500 - val_loss: 257963.8281\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 298404.0625 - val_loss: 178547.0156\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 109897.3281 - val_loss: 119388.2578\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 137171.1719 - val_loss: 105231.7500\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 87858.9297 - val_loss: 99984.7891\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 90015.7109 - val_loss: 79804.7266\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 76705.7891 - val_loss: 82709.5469\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 78862.6016 - val_loss: 77883.3359\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 75697.4609 - val_loss: 77488.8438\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 73367.8750 - val_loss: 76624.9141\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 73636.9609 - val_loss: 75805.2188\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 74885.1953 - val_loss: 79793.5391\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 73778.2969 - val_loss: 75039.4219\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 74048.5938 - val_loss: 75377.3359\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 71596.4609 - val_loss: 75120.0078\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 70429.3125 - val_loss: 73966.3516\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 72026.0938 - val_loss: 71779.6719\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 67795.0469 - val_loss: 71742.3750\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 68570.0938 - val_loss: 69893.1797\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 66572.8984 - val_loss: 69146.6016\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 66536.9844 - val_loss: 68449.7188\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 65352.5352 - val_loss: 67477.6484\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 65134.1445 - val_loss: 66972.5234\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 64042.0312 - val_loss: 66163.8984\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 66877.8203 - val_loss: 66817.5391\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 65400.7148 - val_loss: 68955.5469\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 64824.0898 - val_loss: 65310.7461\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 61811.2969 - val_loss: 65051.4492\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 62008.8281 - val_loss: 61881.2773\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 62753.2070 - val_loss: 63666.4258\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 60359.6836 - val_loss: 60376.4102\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 57333.4258 - val_loss: 60968.6641\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 57853.0898 - val_loss: 58405.5625\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 57027.6445 - val_loss: 60130.7734\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 55432.5781 - val_loss: 58622.7344\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 56275.0312 - val_loss: 56288.7227\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 53005.0430 - val_loss: 55740.2383\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 53951.8906 - val_loss: 54515.1641\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 54075.4258 - val_loss: 56225.9766\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 51553.0625 - val_loss: 56985.4805\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 59753.2305 - val_loss: 56968.4219\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 51857.0742 - val_loss: 52983.3242\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 51040.5938 - val_loss: 50392.1875\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 48401.5898 - val_loss: 50818.9805\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 52489.8789 - val_loss: 48857.4414\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 52059.0000 - val_loss: 49990.6406\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 50061.7305 - val_loss: 51806.5898\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 47278.9102 - val_loss: 50558.7617\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 46346.1953 - val_loss: 48201.3359\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 48254.7461 - val_loss: 45385.8281\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 43743.6914 - val_loss: 45061.6875\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 42823.7773 - val_loss: 43465.0586\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 41890.5781 - val_loss: 43516.3789\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 47868.2734 - val_loss: 42053.7539\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 43025.4219 - val_loss: 44270.3438\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 41289.0820 - val_loss: 42437.4453\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 37999.2969 - val_loss: 44384.2617\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 40853.2656 - val_loss: 40010.9180\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 42629.8594 - val_loss: 38643.1172\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 37600.0391 - val_loss: 37949.1484\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 37089.9102 - val_loss: 39137.8398\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 35805.4102 - val_loss: 37571.5547\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 35863.1797 - val_loss: 36041.5078\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 33807.6406 - val_loss: 37074.6289\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 35057.6758 - val_loss: 34713.3438\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 33841.1875 - val_loss: 34691.7852\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 32585.3105 - val_loss: 34082.4453\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 33836.1562 - val_loss: 32988.8086\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 32630.4395 - val_loss: 32340.3438\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 31372.3691 - val_loss: 31669.3223\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 31011.0000 - val_loss: 31865.5703\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 32764.4199 - val_loss: 31062.7812\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 33763.4180 - val_loss: 30001.2070\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 35318.5312 - val_loss: 30009.7988\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 30043.2500 - val_loss: 29648.1953\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 27677.1367 - val_loss: 28480.2715\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 28222.7930 - val_loss: 27777.2891\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 26967.3027 - val_loss: 29341.1660\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 27065.0859 - val_loss: 27509.1250\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 26247.1699 - val_loss: 26275.9941\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 27222.7383 - val_loss: 25713.2773\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 25523.1777 - val_loss: 25283.7227\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 25681.6465 - val_loss: 24651.1133\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 25628.2500 - val_loss: 25198.2051\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 27183.1016 - val_loss: 23972.9375\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 27930.4062 - val_loss: 23143.3086\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 24184.3652 - val_loss: 22733.7188\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 24283.5938 - val_loss: 22415.8926\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 21122.2930 - val_loss: 21877.7969\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 22561.4141 - val_loss: 21620.8262\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 25305.7715 - val_loss: 21694.3965\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 26591.0117 - val_loss: 20658.3164\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 23134.5957 - val_loss: 20901.0293\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 19473.0723 - val_loss: 20099.3047\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 18993.7871 - val_loss: 19137.5117\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 18965.6211 - val_loss: 19418.7305\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 20497.4941 - val_loss: 19189.4160\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 20566.2363 - val_loss: 17862.9414\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 17729.2656 - val_loss: 17465.3457\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 16696.2109 - val_loss: 17438.4023\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 16925.3887 - val_loss: 16704.8828\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 16099.8867 - val_loss: 16407.4961\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 15812.7393 - val_loss: 16105.2549\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 15691.8750 - val_loss: 15663.6865\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 15072.6328 - val_loss: 15510.2188\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 14846.0869 - val_loss: 15217.0225\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 14465.0273 - val_loss: 14693.6289\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14252.5020 - val_loss: 14383.9795\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 13820.4805 - val_loss: 14098.1631\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13767.8945 - val_loss: 14995.5918\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 14232.4150 - val_loss: 14162.1729\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 14205.6875 - val_loss: 14335.6377\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 13191.6270 - val_loss: 13548.6201\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 13320.0547 - val_loss: 13790.4512\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 12926.2822 - val_loss: 12249.4404\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 12104.0400 - val_loss: 11964.4463\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 11459.5928 - val_loss: 11639.8447\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 11719.3711 - val_loss: 11295.3633\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 10890.8086 - val_loss: 11037.4443\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 10574.1162 - val_loss: 11124.9258\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 10850.8486 - val_loss: 11221.7236\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 11284.7715 - val_loss: 12738.1699\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12526.0332 - val_loss: 11703.3213\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 9940.6299 - val_loss: 12455.9160\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 11416.0781 - val_loss: 11265.6924\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 9382.4658 - val_loss: 11432.2178\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 10342.0156 - val_loss: 9319.8535\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 8575.3311 - val_loss: 9338.8740\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 8748.6738 - val_loss: 8548.9238\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 8774.3115 - val_loss: 8348.1123\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 8107.5229 - val_loss: 8442.0752\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 8521.5488 - val_loss: 8851.1543\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8755.9199 - val_loss: 8744.9375\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 8139.9170 - val_loss: 7638.2676\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7256.1724 - val_loss: 7277.6509\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 7543.0356 - val_loss: 7118.9990\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 6809.3071 - val_loss: 6858.1294\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6601.3618 - val_loss: 6759.4814\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 6611.1929 - val_loss: 6673.0288\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6269.2446 - val_loss: 6265.3237\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6205.8428 - val_loss: 6096.6567\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 5863.9194 - val_loss: 5932.0723\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 5695.9897 - val_loss: 6023.0083\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5693.7100 - val_loss: 5895.0078\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 5698.5923 - val_loss: 5894.1562\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5483.7979 - val_loss: 5862.4551\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5538.7837 - val_loss: 5393.9277\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5180.2266 - val_loss: 4993.8735\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4860.4102 - val_loss: 4848.2832\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4696.8672 - val_loss: 4678.9595\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4487.0513 - val_loss: 4519.4448\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4388.0054 - val_loss: 4603.6475\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4568.6763 - val_loss: 4253.3394\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4633.8066 - val_loss: 5052.6260\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5114.9834 - val_loss: 5077.3496\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4108.3877 - val_loss: 4705.7690\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4425.9180 - val_loss: 3937.0767\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3794.2976 - val_loss: 3710.5896\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3680.2002 - val_loss: 3832.6868\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 3663.5701 - val_loss: 3534.4297\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3308.9702 - val_loss: 3395.6582\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3231.6895 - val_loss: 3480.1848\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 3344.8711 - val_loss: 4408.8281\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4242.9849 - val_loss: 3474.3987\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 3434.8098 - val_loss: 2927.0149\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2890.6484 - val_loss: 2837.9075\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2742.7009 - val_loss: 2760.3364\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2767.5403 - val_loss: 2810.9158\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2635.2126 - val_loss: 2854.2830\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 2757.9509 - val_loss: 2548.2134\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2530.3147 - val_loss: 2421.6934\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2379.3152 - val_loss: 2522.0918\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2463.0994 - val_loss: 2308.1958\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2279.5681 - val_loss: 2230.5115\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2230.4263 - val_loss: 2420.8433\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2404.8069 - val_loss: 2695.5786\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2354.6558 - val_loss: 2176.0396\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1987.2163 - val_loss: 2064.1162\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2017.2571 - val_loss: 1885.8151\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1912.8790 - val_loss: 1865.7285\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1988.9417 - val_loss: 2523.9346\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2340.6558 - val_loss: 3153.7954\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2473.3276 - val_loss: 1854.5969\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2205.3398 - val_loss: 1807.9490\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1799.8597 - val_loss: 1588.7618\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1545.4846 - val_loss: 1490.9141\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1447.4530 - val_loss: 1480.3553\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1490.3831 - val_loss: 1460.7188\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1622.6938 - val_loss: 1363.8872\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1378.9668 - val_loss: 1308.9470\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1327.3668 - val_loss: 2013.1786\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1675.4360 - val_loss: 2321.3684\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1567.0784 - val_loss: 1925.5934\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:49:49,684] Trial 33 finished with value: 0.1428560703481023 and parameters: {'n_layers': 1, 'n_neurons': 338, 'n_steps': 2, 'dropout_threshold': 0.10518537884898078}. Best is trial 28 with value: 0.052441262810946444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(179, 2, 13) (179,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001   121.321648\n",
      "1       20002   162.012405\n",
      "2       20003   120.246002\n",
      "3       20004   281.797455\n",
      "4       20005  1211.192139\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.125, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(15.03125, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(3.3333333333333335, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.625, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0175, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 381 neurons, 1 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 27ms/step - loss: 33325662.0000 - val_loss: 4466493.0000\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1478604.1250 - val_loss: 3043905.0000\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4812941.0000 - val_loss: 4848980.0000\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3107489.7500 - val_loss: 438783.0625\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 220003.3906 - val_loss: 690273.1875\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 906949.0625 - val_loss: 837813.6250\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 516459.7812 - val_loss: 96786.7969\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 98697.6875 - val_loss: 220618.9688\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222597.3594 - val_loss: 145518.2969\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 93028.0469 - val_loss: 86192.5859\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 106706.8516 - val_loss: 123851.5078\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 99506.5234 - val_loss: 77275.6719\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 76592.2266 - val_loss: 86199.4844\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 85486.4922 - val_loss: 79890.0469\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 71852.0938 - val_loss: 78564.9766\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 78746.1016 - val_loss: 80156.3047\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 72710.0859 - val_loss: 73897.2109\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 72085.6875 - val_loss: 74523.0703\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 70446.9609 - val_loss: 73346.2500\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 71324.2109 - val_loss: 73408.7266\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 68209.3438 - val_loss: 70926.9766\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 67915.5000 - val_loss: 70287.7500\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 66975.9141 - val_loss: 70200.9844\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 66906.7969 - val_loss: 69503.8828\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 66379.9062 - val_loss: 68378.2578\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 65000.4414 - val_loss: 68040.3438\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 64778.6367 - val_loss: 67146.6094\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 64195.4727 - val_loss: 66793.6016\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 63304.0859 - val_loss: 65797.1094\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 62986.7344 - val_loss: 65081.3750\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 62375.8359 - val_loss: 64375.5781\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 61406.6406 - val_loss: 63795.3516\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 61138.2500 - val_loss: 63088.2930\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 60030.4805 - val_loss: 62243.7578\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 59782.7109 - val_loss: 62229.7344\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 58964.0078 - val_loss: 60766.5352\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 58191.5508 - val_loss: 60233.5898\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 58269.7812 - val_loss: 59796.6484\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 57412.3281 - val_loss: 58252.3125\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 56755.5195 - val_loss: 59094.5781\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 55770.0469 - val_loss: 56898.6016\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 54307.8555 - val_loss: 56102.9688\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 53320.9648 - val_loss: 56035.4297\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 53435.5742 - val_loss: 54729.3594\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 52243.0430 - val_loss: 54068.8477\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 53885.8125 - val_loss: 54101.5664\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 51344.7617 - val_loss: 55027.5195\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 51640.8477 - val_loss: 52224.0781\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 51215.7500 - val_loss: 51635.8828\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 52074.1641 - val_loss: 52975.5781\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51332.4492 - val_loss: 52299.4297\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 48614.0078 - val_loss: 49321.1484\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 47637.2539 - val_loss: 48188.2773\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 45692.1992 - val_loss: 49219.8281\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 46292.6914 - val_loss: 46729.5781\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 44885.4141 - val_loss: 46221.0977\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 44515.4883 - val_loss: 45427.8398\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 44752.7734 - val_loss: 45456.8164\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 42405.1484 - val_loss: 45217.8047\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 43617.0859 - val_loss: 43383.8516\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 41892.3164 - val_loss: 44497.3633\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 43200.5586 - val_loss: 42169.4414\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 40122.2344 - val_loss: 41630.1484\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 39890.7930 - val_loss: 41172.9570\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 38797.3320 - val_loss: 40416.4570\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 38683.4922 - val_loss: 39311.9336\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 37862.4648 - val_loss: 38501.7891\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 36751.2539 - val_loss: 38111.7812\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 36327.4570 - val_loss: 37220.0703\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 35764.0469 - val_loss: 36611.4883\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 34872.8438 - val_loss: 36951.2070\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 35148.0156 - val_loss: 35448.2109\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 33762.3867 - val_loss: 34969.2812\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 33741.0156 - val_loss: 34613.1562\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 32930.0312 - val_loss: 33829.6172\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 32274.1660 - val_loss: 33311.1641\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 32008.4434 - val_loss: 32667.1465\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 31326.5781 - val_loss: 32114.8477\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 30998.8203 - val_loss: 31507.8594\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 30083.3809 - val_loss: 31759.8555\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 29854.5547 - val_loss: 30463.7852\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 29174.6602 - val_loss: 30011.7598\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 28710.3008 - val_loss: 29638.0938\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 28378.1309 - val_loss: 28935.9180\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 28157.3184 - val_loss: 28278.1641\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 27980.0547 - val_loss: 29387.1816\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 27446.1191 - val_loss: 27713.8887\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 28080.7754 - val_loss: 26795.6797\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 25168.4609 - val_loss: 26657.1055\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 24906.2637 - val_loss: 26071.9180\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 24666.2148 - val_loss: 25113.4590\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24299.3887 - val_loss: 24774.6680\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 23747.7070 - val_loss: 24200.4473\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 23065.7500 - val_loss: 23781.4668\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 22584.5977 - val_loss: 23426.5801\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 22432.4727 - val_loss: 22925.6660\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 23435.7324 - val_loss: 22407.7715\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 23149.3164 - val_loss: 23006.6465\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 22148.3848 - val_loss: 23773.6328\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21159.7070 - val_loss: 21608.7402\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 21221.7695 - val_loss: 20599.6758\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20093.8066 - val_loss: 20396.9590\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 19691.7578 - val_loss: 20269.4082\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 19060.4199 - val_loss: 19515.8184\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18792.8125 - val_loss: 18910.8203\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 18990.9902 - val_loss: 18514.6758\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18578.9082 - val_loss: 18632.4785\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 17348.8926 - val_loss: 18663.6914\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 17915.1309 - val_loss: 17471.7891\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 16616.5176 - val_loss: 17013.6777\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16203.5723 - val_loss: 16646.4258\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 16625.9355 - val_loss: 16222.2090\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 15282.7754 - val_loss: 16270.4678\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15155.2363 - val_loss: 15566.6895\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 14791.8057 - val_loss: 15734.6709\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 15377.4678 - val_loss: 14848.7266\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 14383.2715 - val_loss: 14516.9912\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 14127.7178 - val_loss: 14327.9326\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 14079.8662 - val_loss: 14196.8496\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13705.6973 - val_loss: 13761.0469\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 13013.8701 - val_loss: 13297.3770\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12876.7881 - val_loss: 13504.1914\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 13024.9834 - val_loss: 12841.7695\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 12611.0693 - val_loss: 12476.0029\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 12618.2832 - val_loss: 12148.5508\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12126.8359 - val_loss: 12033.6396\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11591.2686 - val_loss: 11806.9209\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11376.7080 - val_loss: 11632.6348\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10960.1973 - val_loss: 11302.4092\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 10810.4746 - val_loss: 10776.4473\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10388.8213 - val_loss: 10540.7207\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 10539.8389 - val_loss: 10304.5605\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10121.4883 - val_loss: 10297.1807\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 11249.4541 - val_loss: 9807.0049\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 10752.9111 - val_loss: 10337.6172\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9247.3037 - val_loss: 11295.6533\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 10512.3828 - val_loss: 9898.1680\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 11391.4795 - val_loss: 8937.3525\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9754.8730 - val_loss: 8720.8613\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 8207.6113 - val_loss: 8924.2119\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 8313.6504 - val_loss: 8454.3145\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8331.1641 - val_loss: 8102.3184\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7956.0151 - val_loss: 7775.3833\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7743.3184 - val_loss: 7723.2427\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7244.6460 - val_loss: 7732.3384\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 7323.1787 - val_loss: 7171.4897\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 6963.3682 - val_loss: 6972.9233\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7076.8521 - val_loss: 6771.9087\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6877.9907 - val_loss: 6599.0654\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6352.1699 - val_loss: 6385.1191\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6107.7886 - val_loss: 6383.1587\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 6545.1719 - val_loss: 6016.5796\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 6140.0879 - val_loss: 5823.0483\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 5606.9722 - val_loss: 5632.9863\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 5405.0796 - val_loss: 5536.9663\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 5292.2778 - val_loss: 5374.0107\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 5091.0503 - val_loss: 5246.5303\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5070.9731 - val_loss: 5539.5029\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5982.6704 - val_loss: 5020.1226\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4882.6357 - val_loss: 4739.9561\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4674.7373 - val_loss: 4593.4976\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4545.7744 - val_loss: 4461.2280\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4352.0308 - val_loss: 4370.6338\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4349.2314 - val_loss: 4204.3115\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4327.7275 - val_loss: 4301.4692\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4516.1895 - val_loss: 4037.3054\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4430.1235 - val_loss: 3852.3140\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3718.7900 - val_loss: 3816.6284\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3662.7314 - val_loss: 3670.5552\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3521.7402 - val_loss: 3551.7878\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3445.7061 - val_loss: 3422.0088\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3473.2344 - val_loss: 3354.3457\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3305.3970 - val_loss: 3232.5217\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3108.7295 - val_loss: 3132.6309\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3231.2910 - val_loss: 3130.3745\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3053.8574 - val_loss: 2962.1655\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2880.7019 - val_loss: 2873.5703\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2767.0955 - val_loss: 2794.0310\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2705.2366 - val_loss: 2722.6494\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2621.4429 - val_loss: 2849.2290\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3181.1326 - val_loss: 2650.8184\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2646.0183 - val_loss: 2465.8430\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2436.3799 - val_loss: 2464.9290\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2446.6487 - val_loss: 2278.7798\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2210.9709 - val_loss: 2203.8867\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2167.2336 - val_loss: 2132.8489\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2138.0527 - val_loss: 2065.1157\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2103.8096 - val_loss: 2003.9008\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1989.4968 - val_loss: 1954.9150\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1890.4208 - val_loss: 1903.4073\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1939.1862 - val_loss: 1841.3801\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1818.8492 - val_loss: 1776.5767\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1737.4318 - val_loss: 1740.6133\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1693.2920 - val_loss: 1714.3264\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1687.1024 - val_loss: 1686.2493\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1620.5474 - val_loss: 1596.4120\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1530.3010 - val_loss: 1555.8566\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1491.9801 - val_loss: 1650.3026\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1563.7468 - val_loss: 1522.8998\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1557.3003 - val_loss: 1600.2103\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:49:59,781] Trial 34 finished with value: 0.06879342525398276 and parameters: {'n_layers': 1, 'n_neurons': 381, 'n_steps': 1, 'dropout_threshold': 0.1624135247535411}. Best is trial 28 with value: 0.052441262810946444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(180, 1, 13) (180,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001    79.955093\n",
      "1       20002   119.759621\n",
      "2       20003    79.001976\n",
      "3       20004   242.140579\n",
      "4       20005  1147.194580\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12121212121212122, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(14.575757575757576, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(3.3333333333333335, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6060606060606061, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01696969696969697, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 284 neurons, 1 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 548294.1875 - val_loss: 158008.8750\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 192284.7188 - val_loss: 188401.9219\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 111893.2969 - val_loss: 180084.7031\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 114332.9219 - val_loss: 148099.0781\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 118115.9531 - val_loss: 103477.5312\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 113839.9062 - val_loss: 77824.2578\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 79123.4688 - val_loss: 74782.7656\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 75746.6406 - val_loss: 73126.7656\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 76128.5781 - val_loss: 71970.2500\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 68542.0391 - val_loss: 71981.2500\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 70279.0000 - val_loss: 68928.4688\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 65170.1641 - val_loss: 67440.5625\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 68718.3438 - val_loss: 66050.4375\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 65296.8477 - val_loss: 65290.8594\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 61888.7266 - val_loss: 67929.9297\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 67018.4453 - val_loss: 66265.9922\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 61260.8555 - val_loss: 62409.2656\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 62111.1836 - val_loss: 59072.8008\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 57349.2109 - val_loss: 57657.1719\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 55152.1758 - val_loss: 56109.6406\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 54515.8984 - val_loss: 59019.8359\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 56832.6016 - val_loss: 59486.8477\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51334.4961 - val_loss: 85294.4141\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 84952.5000 - val_loss: 97377.0391\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 71069.5547 - val_loss: 77496.4609\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 56876.0352 - val_loss: 51504.2812\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 53526.4414 - val_loss: 46944.7461\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 45156.3750 - val_loss: 45696.8008\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 44137.2695 - val_loss: 46220.8828\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 46332.2227 - val_loss: 44220.1719\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 42423.6562 - val_loss: 42322.9883\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 41453.4844 - val_loss: 41742.2109\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40221.9961 - val_loss: 40291.0898\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 39743.9453 - val_loss: 39695.7422\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 37853.2695 - val_loss: 38675.8828\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 38350.5234 - val_loss: 37835.4414\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 36060.8867 - val_loss: 36428.6758\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 38549.6953 - val_loss: 57440.1758\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 44608.7930 - val_loss: 37945.0938\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36868.7930 - val_loss: 36476.7656\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35830.9883 - val_loss: 46798.2812\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 37948.5703 - val_loss: 32228.7461\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 39104.5781 - val_loss: 44406.1367\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33575.1562 - val_loss: 44749.8633\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43703.2617 - val_loss: 39898.1055\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 34800.6836 - val_loss: 53180.5898\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 38565.7031 - val_loss: 31277.0645\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27409.9082 - val_loss: 27429.5176\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26935.0332 - val_loss: 28787.0098\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26229.8750 - val_loss: 26483.6523\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28075.2676 - val_loss: 35410.5820\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27347.4004 - val_loss: 24580.6387\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24946.9434 - val_loss: 25123.7402\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23082.7559 - val_loss: 23122.7910\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23787.4512 - val_loss: 29778.6387\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27860.4336 - val_loss: 23291.2188\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21505.3418 - val_loss: 25260.3418\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24598.7715 - val_loss: 23995.9727\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22213.2676 - val_loss: 25543.9531\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24664.9199 - val_loss: 18910.4785\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22081.4570 - val_loss: 18206.7539\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19017.1523 - val_loss: 19732.3418\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19243.6797 - val_loss: 17155.9844\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16426.1973 - val_loss: 16370.5488\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 15822.0742 - val_loss: 20515.1816\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 16744.8047 - val_loss: 15827.0762\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 17308.5918 - val_loss: 17638.0410\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14807.1367 - val_loss: 15615.9238\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13637.1758 - val_loss: 13914.6504\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13157.1182 - val_loss: 13508.4531\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13365.0322 - val_loss: 17481.3789\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 11893.8486 - val_loss: 15871.1338\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15560.3271 - val_loss: 14510.6914\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 11394.9277 - val_loss: 14806.6768\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 12522.7598 - val_loss: 12464.8350\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12250.3564 - val_loss: 10684.8906\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11775.1338 - val_loss: 11523.8574\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10327.0107 - val_loss: 11780.5674\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 9980.3359 - val_loss: 13504.0615\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 11846.8154 - val_loss: 12526.3496\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 12601.3066 - val_loss: 12604.9033\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 10972.5625 - val_loss: 9191.4775\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 9919.1143 - val_loss: 10260.1016\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9559.5283 - val_loss: 8533.5381\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8023.2661 - val_loss: 7306.1958\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6983.9131 - val_loss: 9053.4951\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8508.7891 - val_loss: 9131.4268\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13409.9775 - val_loss: 7731.0601\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12616.2607 - val_loss: 6175.3325\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12328.0527 - val_loss: 8967.5869\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7566.7471 - val_loss: 6273.7354\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5617.9062 - val_loss: 5569.3848\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5312.1831 - val_loss: 5482.5913\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5337.9370 - val_loss: 6631.6777\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5864.7695 - val_loss: 5260.4902\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5427.7222 - val_loss: 4636.1289\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4830.9731 - val_loss: 4127.8770\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4091.1401 - val_loss: 6858.9214\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5465.3926 - val_loss: 3677.2529\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3866.7329 - val_loss: 3479.5933\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3598.2053 - val_loss: 3862.7979\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3108.5779 - val_loss: 3189.4370\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2959.8250 - val_loss: 3311.0464\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3528.8853 - val_loss: 2797.7302\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2876.7754 - val_loss: 2580.1953\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2720.1829 - val_loss: 2648.1704\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2754.5027 - val_loss: 2942.7534\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3543.0522 - val_loss: 2174.6956\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2199.9226 - val_loss: 2339.6692\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2071.0391 - val_loss: 2288.7703\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2057.4871 - val_loss: 2965.1001\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2288.2148 - val_loss: 3253.3367\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2571.3745 - val_loss: 2165.5449\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1681.6404 - val_loss: 1490.2582\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1704.8950 - val_loss: 1713.0868\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1726.8413 - val_loss: 1383.9180\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1821.9711 - val_loss: 1541.1611\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1954.9005 - val_loss: 1546.3428\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1381.2025 - val_loss: 2111.6594\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1380.5856 - val_loss: 1086.5248\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1273.4316 - val_loss: 993.8665\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1003.1800 - val_loss: 4088.1816\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2191.1084 - val_loss: 3985.7188\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2798.5198 - val_loss: 7242.0640\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3505.8455 - val_loss: 3022.8047\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1900.8794 - val_loss: 1007.3514\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1161.5842 - val_loss: 1406.8318\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 988.6840 - val_loss: 710.7769\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 635.5710 - val_loss: 2078.3015\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1089.0408 - val_loss: 1315.2076\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 944.0230 - val_loss: 2361.7139\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1051.6522 - val_loss: 798.2628\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 516.9938 - val_loss: 423.5134\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 418.3437 - val_loss: 564.3055\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 476.4543 - val_loss: 496.1053\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 396.4742 - val_loss: 354.0007\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 362.2422 - val_loss: 340.0252\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 342.5951 - val_loss: 358.1046\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 353.4529 - val_loss: 498.4290\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 408.9354 - val_loss: 313.7547\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 324.2435 - val_loss: 265.9113\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 301.3796 - val_loss: 333.4726\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 313.2646 - val_loss: 244.0805\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 327.0916 - val_loss: 239.1069\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 241.7626 - val_loss: 248.4430\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 253.4019 - val_loss: 224.8236\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 288.1028 - val_loss: 236.9541\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 254.3612 - val_loss: 185.8187\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 236.7746 - val_loss: 300.2159\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 327.4444 - val_loss: 349.5103\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 325.5598 - val_loss: 182.6570\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 309.9122 - val_loss: 163.8719\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 167.9145 - val_loss: 155.4678\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 180.2837 - val_loss: 181.0658\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 187.5119 - val_loss: 141.6364\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 159.4503 - val_loss: 138.8536\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 144.3784 - val_loss: 412.4842\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 260.6235 - val_loss: 229.0360\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 205.8248 - val_loss: 128.6335\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 149.4500 - val_loss: 168.2876\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 178.9856 - val_loss: 120.6915\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 126.2252 - val_loss: 203.3096\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 184.6916 - val_loss: 126.6768\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 135.6012 - val_loss: 209.5861\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 180.8373 - val_loss: 222.5826\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 189.7599 - val_loss: 236.2524\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 262.5076 - val_loss: 519.2695\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 263.2039 - val_loss: 109.5590\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 144.5170 - val_loss: 641.8978\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 298.6745 - val_loss: 116.7982\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 149.2447 - val_loss: 319.7989\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 186.3031 - val_loss: 126.8254\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 112.6621 - val_loss: 110.6561\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 140.0275 - val_loss: 106.8079\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 144.6035 - val_loss: 247.8788\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 318.9823 - val_loss: 781.7647\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 418.2930 - val_loss: 265.7003\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 256.1013 - val_loss: 323.6923\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 198.9683 - val_loss: 106.6309\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 127.3594 - val_loss: 103.5879\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 118.2063 - val_loss: 191.6056\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 137.5013 - val_loss: 131.8911\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 122.7670 - val_loss: 164.2210\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 131.5523 - val_loss: 100.7623\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 120.6392 - val_loss: 143.5462\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 164.5173 - val_loss: 234.9203\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 155.5083 - val_loss: 176.1998\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 140.1963 - val_loss: 130.4810\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 143.4447 - val_loss: 117.2788\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 104.2946 - val_loss: 108.6499\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 106.2538 - val_loss: 147.5475\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 140.0464 - val_loss: 225.9113\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 141.3225 - val_loss: 242.3276\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 269.0960 - val_loss: 414.3701\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(180, 1, 13) (180,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001    84.671562\n",
      "1       20002   129.277176\n",
      "2       20003    83.414230\n",
      "3       20004   202.360168\n",
      "4       20005  1282.256592\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:50:06,290] Trial 35 finished with value: 0.0738051995774212 and parameters: {'n_layers': 1, 'n_neurons': 284, 'n_steps': 1, 'dropout_threshold': 0.17276666143513308}. Best is trial 28 with value: 0.052441262810946444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11764705882352941, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(14.147058823529411, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(3.3333333333333335, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5882352941176471, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01647058823529412, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 271 neurons, 5 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 27ms/step - loss: 29192046.0000 - val_loss: 13437235.0000\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4378220.5000 - val_loss: 1345321.1250\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1194892.5000 - val_loss: 983294.7500\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 490903.3125 - val_loss: 299968.5000\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 205059.2500 - val_loss: 132470.1719\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 94078.6172 - val_loss: 86992.3984\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 75597.7891 - val_loss: 85474.0000\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 86431.0156 - val_loss: 70067.4766\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 79168.8047 - val_loss: 162178.0469\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 12200683.0000 - val_loss: 6018863.0000\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 9778532.0000 - val_loss: 115814.4453\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3929907.0000 - val_loss: 1286246.0000\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1201273.1250 - val_loss: 2126407.0000\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 793051.5000 - val_loss: 1452243.6250\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 802141.9375 - val_loss: 459887.8125\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 453530.9375 - val_loss: 108826.1484\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 280516.7500 - val_loss: 69800.7734\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 223043.5781 - val_loss: 178704.1406\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 111592.6562 - val_loss: 160037.6875\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 100630.5781 - val_loss: 99026.3828\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 77423.7422 - val_loss: 84702.7656\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 96466.3359 - val_loss: 66997.0234\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 67701.4453 - val_loss: 65505.9766\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 70638.2656 - val_loss: 66518.9531\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 65333.0859 - val_loss: 79428.5625\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 84896.2656 - val_loss: 70139.4844\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 84178.4531 - val_loss: 69293.1797\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 85628.7578 - val_loss: 81037.4453\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 129051.6406 - val_loss: 62429.7461\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 62473.0625 - val_loss: 65381.0859\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 61267.0977 - val_loss: 60245.9141\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 57738.9141 - val_loss: 65800.8750\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 81338.2031 - val_loss: 62953.8242\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 80936.6328 - val_loss: 58002.4883\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 56567.7695 - val_loss: 58009.0742\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 62486.9922 - val_loss: 103700.5625\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 161959.6406 - val_loss: 86570.8516\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 110230.1875 - val_loss: 62771.8672\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 69640.3516 - val_loss: 67722.3125\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 66711.2656 - val_loss: 56122.2773\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 52910.1562 - val_loss: 54122.6523\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 65233.9570 - val_loss: 60908.2188\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 74213.7812 - val_loss: 60722.8789\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 62015.8828 - val_loss: 57275.5664\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 50761.7578 - val_loss: 119870.6641\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 131736.0781 - val_loss: 122093.4922\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 103407.4922 - val_loss: 87081.7734\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 69262.3750 - val_loss: 77674.8828\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 60968.3867 - val_loss: 68835.8750\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 58386.3906 - val_loss: 49516.7812\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 53128.7656 - val_loss: 49668.6133\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 52796.3594 - val_loss: 46647.4492\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 45358.3711 - val_loss: 62359.3633\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 70757.8750 - val_loss: 71969.6875\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 65099.5469 - val_loss: 56964.2344\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 44188.9414 - val_loss: 56379.2969\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 47922.0664 - val_loss: 47802.8984\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 44963.8242 - val_loss: 43234.5898\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 51091.9492 - val_loss: 55694.0469\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 59390.9336 - val_loss: 50308.3047\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 44038.8477 - val_loss: 59378.5000\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 51107.9688 - val_loss: 40904.3164\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 49395.4336 - val_loss: 41412.6602\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 40639.7500 - val_loss: 47472.7969\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 43254.8320 - val_loss: 39157.8359\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 37662.5430 - val_loss: 39748.5742\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 37863.3867 - val_loss: 39397.2500\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 40522.0664 - val_loss: 37912.1992\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 37227.0469 - val_loss: 37511.7734\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 36774.0977 - val_loss: 37987.2227\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 37463.0820 - val_loss: 47898.4570\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 49355.7344 - val_loss: 56122.7305\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 41105.8711 - val_loss: 44829.9375\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 43962.2891 - val_loss: 35314.2539\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 35164.1953 - val_loss: 38683.1484\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 37422.6680 - val_loss: 139624.8750\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 127635.5391 - val_loss: 119422.9375\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 60467.5039 - val_loss: 110937.0859\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 77355.4922 - val_loss: 59677.6523\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 45849.2969 - val_loss: 31567.9219\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 41632.9961 - val_loss: 37445.9023\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 34485.6680 - val_loss: 40308.5430\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 32733.8633 - val_loss: 31506.7461\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 32264.7832 - val_loss: 29695.5547\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 28879.9102 - val_loss: 55859.8789\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 58098.9883 - val_loss: 95199.0391\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 52363.1758 - val_loss: 37217.2695\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 42627.7383 - val_loss: 95305.0859\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 109244.0469 - val_loss: 156933.5156\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 86966.4219 - val_loss: 40385.0859\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 62543.3594 - val_loss: 67583.5625\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 69647.8203 - val_loss: 105379.9062\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 62303.3789 - val_loss: 26765.5723\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 49124.1758 - val_loss: 44421.2188\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 33135.9023 - val_loss: 41469.2578\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 31748.6016 - val_loss: 23746.4844\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 27090.8477 - val_loss: 113389.1094\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 106583.4766 - val_loss: 150729.8438\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 84985.2266 - val_loss: 22328.0938\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 52872.5391 - val_loss: 36026.9922\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 27255.8848 - val_loss: 27263.8066\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 25602.8477 - val_loss: 38915.7422\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 39952.1758 - val_loss: 45192.2266\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 35028.1016 - val_loss: 24794.5801\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 24416.4980 - val_loss: 57028.6328\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 63816.5586 - val_loss: 19613.6172\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 50811.3789 - val_loss: 73031.3516\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 52685.5703 - val_loss: 20782.0312\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 30040.5527 - val_loss: 25922.5156\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 21660.3066 - val_loss: 42262.2461\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 49791.5078 - val_loss: 28195.9336\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 34742.6680 - val_loss: 39896.7812\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 29967.9043 - val_loss: 74836.2031\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 40275.2500 - val_loss: 26482.5547\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 22606.1777 - val_loss: 20629.6055\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 22594.2227 - val_loss: 23596.0547\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:50:11,731] Trial 36 finished with value: 0.3315900002089061 and parameters: {'n_layers': 1, 'n_neurons': 271, 'n_steps': 5, 'dropout_threshold': 0.22750823704476408}. Best is trial 28 with value: 0.052441262810946444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(176, 5, 13) (176,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  105.508919\n",
      "1       20002  130.130066\n",
      "2       20003  104.792000\n",
      "3       20004  239.322174\n",
      "4       20005  769.979553\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11428571428571428, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(13.742857142857142, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(3.3333333333333335, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.016, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 2 layers, 487 neurons, 3 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 44ms/step - loss: 5236950.0000 - val_loss: 243793.7344\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1748211.3750 - val_loss: 183668.8438\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 969966.4375 - val_loss: 1826682.2500\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 904284.0625 - val_loss: 364415.8438\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 943484.4375 - val_loss: 382276.4375\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 724209.6875 - val_loss: 633415.1875\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 729539.8125 - val_loss: 1422564.6250\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 719134.0625 - val_loss: 106972.0469\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 596954.4375 - val_loss: 86524.1797\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 456265.0938 - val_loss: 107850.1094\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 414629.1562 - val_loss: 120264.3828\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 388002.2188 - val_loss: 182727.0000\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 268021.4688 - val_loss: 107542.0234\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 324657.7812 - val_loss: 85876.0078\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 294707.3750 - val_loss: 300533.9375\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 265719.2812 - val_loss: 199857.9062\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 260611.2344 - val_loss: 92317.7500\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 276283.8750 - val_loss: 256034.9219\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 274468.0625 - val_loss: 110720.2969\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 262713.5625 - val_loss: 220641.8281\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 301999.3750 - val_loss: 91639.0078\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 238427.9531 - val_loss: 355787.6562\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 198741.0625 - val_loss: 132782.2969\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 255873.0312 - val_loss: 217715.0938\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:50:15,118] Trial 37 finished with value: 0.7657736433166522 and parameters: {'n_layers': 2, 'n_neurons': 487, 'n_steps': 3, 'dropout_threshold': 0.1677059494795091}. Best is trial 28 with value: 0.052441262810946444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(178, 3, 13) (178,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  154.941025\n",
      "1       20002  155.073227\n",
      "2       20003  154.957687\n",
      "3       20004  156.133133\n",
      "4       20005  158.537445\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.1111111111111111, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(13.36111111111111, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(3.3333333333333335, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5555555555555556, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015555555555555557, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 352 neurons, 5 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 28ms/step - loss: 3510177.5000 - val_loss: 471944.4375\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 950174.6875 - val_loss: 1354046.7500\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 499775.4062 - val_loss: 224883.0156\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 512044.2812 - val_loss: 586442.6250\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2524131.0000 - val_loss: 4206150.5000\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3081817.7500 - val_loss: 3220513.2500\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2573281.5000 - val_loss: 156177.5000\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1327708.8750 - val_loss: 1399944.1250\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 683509.3125 - val_loss: 405098.9062\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 652518.8750 - val_loss: 83273.2812\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 303531.2188 - val_loss: 182901.5000\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 102547.6562 - val_loss: 166701.2344\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 92797.4922 - val_loss: 109820.8438\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 90214.7109 - val_loss: 74944.6172\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 72930.4062 - val_loss: 63426.3750\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 64602.3594 - val_loss: 62272.0000\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 60312.5586 - val_loss: 62564.5312\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 66728.4922 - val_loss: 64397.9922\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 60217.2617 - val_loss: 64862.2305\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 58426.9336 - val_loss: 59811.3008\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 64942.4727 - val_loss: 58860.6016\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 56330.5156 - val_loss: 57915.5547\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 56269.8398 - val_loss: 56551.0000\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 60100.2930 - val_loss: 60806.8203\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 54636.5586 - val_loss: 71530.0859\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 63765.5156 - val_loss: 56149.1758\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 53811.5469 - val_loss: 163100.8906\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 187047.7344 - val_loss: 238298.0469\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 152560.8906 - val_loss: 55205.1523\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 75549.7109 - val_loss: 56241.0000\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 59112.3125 - val_loss: 49216.3281\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 60232.5156 - val_loss: 61002.3984\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 58838.1055 - val_loss: 46569.6836\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 56362.4102 - val_loss: 49138.1016\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 44235.8867 - val_loss: 49595.3633\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 43310.8438 - val_loss: 176931.0156\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 150778.8125 - val_loss: 158652.7031\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 105552.9922 - val_loss: 53639.8438\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 73440.4375 - val_loss: 240634.1719\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 155514.8281 - val_loss: 138687.0312\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 100307.8906 - val_loss: 72192.4062\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 81243.7344 - val_loss: 89476.8047\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 65960.6797 - val_loss: 179837.4219\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:50:17,818] Trial 38 finished with value: 0.5951005246632787 and parameters: {'n_layers': 1, 'n_neurons': 352, 'n_steps': 5, 'dropout_threshold': 0.12646910046849166}. Best is trial 28 with value: 0.052441262810946444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(176, 5, 13) (176,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  163.675034\n",
      "1       20002  175.860855\n",
      "2       20003  163.526764\n",
      "3       20004  233.148483\n",
      "4       20005  489.141815\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10810810810810811, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(13.0, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(3.3333333333333335, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5405405405405406, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015135135135135137, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 210 neurons, 1 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 22ms/step - loss: 28378816.0000 - val_loss: 14594187.0000\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9852117.0000 - val_loss: 3582130.0000\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1971906.7500 - val_loss: 229832.2344\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 152856.5625 - val_loss: 411590.7188\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 634106.6875 - val_loss: 894431.6250\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 831567.5000 - val_loss: 619345.6250\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 454223.6562 - val_loss: 212407.2500\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 136179.2031 - val_loss: 85192.3281\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 88306.9062 - val_loss: 126136.6797\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 133058.2344 - val_loss: 134346.5469\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 120674.6641 - val_loss: 100020.3750\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 88439.6094 - val_loss: 84560.9141\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 81405.8203 - val_loss: 88036.9453\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 83911.9375 - val_loss: 87930.6562\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 82739.5156 - val_loss: 85229.2188\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 80387.3203 - val_loss: 83574.6641\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 80672.8281 - val_loss: 83552.6016\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 79990.5625 - val_loss: 83146.9766\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 79199.6641 - val_loss: 83172.7891\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 79319.6094 - val_loss: 82821.3984\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 78910.2812 - val_loss: 82510.2266\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 78811.3359 - val_loss: 82279.9453\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 78759.9375 - val_loss: 81997.2891\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 78244.6328 - val_loss: 81603.8672\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 77909.2656 - val_loss: 81871.5391\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 77924.8750 - val_loss: 81190.2578\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 77781.0391 - val_loss: 80828.8203\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 77104.0625 - val_loss: 80504.4844\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 76737.4531 - val_loss: 80417.2578\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 76610.1484 - val_loss: 80412.3359\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 76753.7656 - val_loss: 80610.1172\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 76855.3984 - val_loss: 80411.6797\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 76282.7188 - val_loss: 79355.7422\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 76121.9062 - val_loss: 79008.7578\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 75556.9297 - val_loss: 78603.3203\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 76017.6172 - val_loss: 78485.6328\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 75009.2031 - val_loss: 78015.5859\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 74494.1250 - val_loss: 77842.2500\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 74661.5781 - val_loss: 77279.5547\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 74215.6172 - val_loss: 78141.2109\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 74837.4453 - val_loss: 77250.6562\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 73572.7188 - val_loss: 76189.8438\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 72558.8750 - val_loss: 75840.7500\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 72366.7656 - val_loss: 75435.4922\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 71522.0078 - val_loss: 75575.3203\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 72389.7188 - val_loss: 74927.3516\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 72267.7031 - val_loss: 74605.3281\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 70976.6094 - val_loss: 74026.0078\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 70730.3125 - val_loss: 73717.0156\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 70971.0078 - val_loss: 73570.1094\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 70082.8828 - val_loss: 73004.1172\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 69793.1250 - val_loss: 72640.9531\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 69593.9453 - val_loss: 72535.8828\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 68976.9297 - val_loss: 71999.3203\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 68740.6797 - val_loss: 71654.1875\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 68318.1875 - val_loss: 71481.6172\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 68128.8125 - val_loss: 71559.2578\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 68231.8906 - val_loss: 71449.7812\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 68077.6953 - val_loss: 70809.2031\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 67388.2422 - val_loss: 70094.9062\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 66903.9062 - val_loss: 69789.9297\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 66710.2031 - val_loss: 69425.8906\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 66264.9297 - val_loss: 69106.2344\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 65960.8672 - val_loss: 69183.8750\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 65808.5625 - val_loss: 68463.2969\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 65181.0312 - val_loss: 68479.2188\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 68155.8906 - val_loss: 71363.3672\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 66734.2891 - val_loss: 67225.5312\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 63947.8711 - val_loss: 67618.9609\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 64554.3203 - val_loss: 67051.2734\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 63595.2109 - val_loss: 66204.1094\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 64755.7617 - val_loss: 66254.3672\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 64095.4609 - val_loss: 65857.2109\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 62667.7461 - val_loss: 65262.6133\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 62245.7930 - val_loss: 64940.6719\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 61953.3320 - val_loss: 64754.5117\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 61656.3828 - val_loss: 64150.9297\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 61341.7461 - val_loss: 64094.6133\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 60927.4922 - val_loss: 63587.2344\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 60892.0859 - val_loss: 63864.4922\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 60875.5742 - val_loss: 62723.7344\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 59943.5938 - val_loss: 62410.6055\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 59737.1133 - val_loss: 62015.1875\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 60222.6250 - val_loss: 62546.7344\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 59451.4727 - val_loss: 61403.5781\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 58397.8438 - val_loss: 60941.3398\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 58116.2969 - val_loss: 60638.9297\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 58102.8555 - val_loss: 60241.7539\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 57253.2773 - val_loss: 60616.9297\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 58694.2734 - val_loss: 60103.6016\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 56570.3203 - val_loss: 59870.4648\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 58020.5078 - val_loss: 60836.2812\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 57129.7891 - val_loss: 58351.4102\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 57655.0664 - val_loss: 61004.9648\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 57969.9453 - val_loss: 57625.7656\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 55294.7773 - val_loss: 59154.1250\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 56061.4609 - val_loss: 57047.1289\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 54128.9688 - val_loss: 57539.3516\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 58879.4805 - val_loss: 60241.5234\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 54603.7695 - val_loss: 56334.6836\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 53691.7617 - val_loss: 56012.4805\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 53251.8477 - val_loss: 55038.5898\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 52775.3633 - val_loss: 54956.3867\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 53301.4609 - val_loss: 54889.4062\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 52138.9453 - val_loss: 53975.9570\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 51127.4180 - val_loss: 54459.5898\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 52422.9336 - val_loss: 53517.8281\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 51221.3398 - val_loss: 53309.1133\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 51856.3281 - val_loss: 54208.3711\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 51864.5820 - val_loss: 52026.2773\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 51613.5469 - val_loss: 53617.2891\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 50220.7031 - val_loss: 51258.2773\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 48720.2188 - val_loss: 51478.2227\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 49891.8047 - val_loss: 51411.0234\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 49244.0430 - val_loss: 50271.0938\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 48000.2070 - val_loss: 49796.2305\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 47563.8398 - val_loss: 49492.4805\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 47279.2227 - val_loss: 49163.8359\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 47136.1719 - val_loss: 49132.7578\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 46862.5742 - val_loss: 48510.5430\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 47206.0859 - val_loss: 48368.0781\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 46257.2383 - val_loss: 48336.6250\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 46187.6211 - val_loss: 47550.1641\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 46532.2500 - val_loss: 48500.6133\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 46129.0195 - val_loss: 46692.5391\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 45526.8242 - val_loss: 47277.3438\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 43962.5312 - val_loss: 46533.3984\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 44523.3086 - val_loss: 45675.3398\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 43702.9570 - val_loss: 45386.9609\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 43571.8320 - val_loss: 45196.0938\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 43227.4492 - val_loss: 45085.8281\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 42881.6836 - val_loss: 44565.1719\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 42767.4609 - val_loss: 44309.9062\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 42233.2891 - val_loss: 43865.4453\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 41948.9023 - val_loss: 43862.0625\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 42276.3672 - val_loss: 43620.4961\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 41764.7109 - val_loss: 42937.7930\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 41262.6289 - val_loss: 42638.7930\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 40790.5703 - val_loss: 42352.2070\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 40340.0469 - val_loss: 42042.7969\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 40169.5586 - val_loss: 41624.3711\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 39886.2500 - val_loss: 41306.6719\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 39745.6406 - val_loss: 41394.5078\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 39532.0859 - val_loss: 40698.1172\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 39528.0586 - val_loss: 40348.8516\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 38876.9258 - val_loss: 41009.8359\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 38532.2812 - val_loss: 39884.6719\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 41204.6172 - val_loss: 41054.9805\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 38226.5664 - val_loss: 39350.6758\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 37677.9883 - val_loss: 39150.2578\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 37158.3438 - val_loss: 38331.8477\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 36806.3594 - val_loss: 39297.7422\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 37637.2266 - val_loss: 37907.8516\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 35791.4414 - val_loss: 38120.3008\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 36912.8984 - val_loss: 37692.2461\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 36093.0391 - val_loss: 36983.2617\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 35021.1914 - val_loss: 36755.7930\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 35422.8203 - val_loss: 36375.9570\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 34847.1406 - val_loss: 36845.7930\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 34770.3984 - val_loss: 35815.5469\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 36654.6328 - val_loss: 37086.3711\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 34567.9922 - val_loss: 35111.2695\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 33398.2070 - val_loss: 34752.5352\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 33540.1914 - val_loss: 34889.2656\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 33278.6953 - val_loss: 34125.8633\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 32627.0098 - val_loss: 34035.6719\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 32496.4453 - val_loss: 33521.7188\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 32323.2969 - val_loss: 33233.7930\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 32059.9453 - val_loss: 34543.6602\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 32626.4941 - val_loss: 32657.6504\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 31361.5762 - val_loss: 32290.1055\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 30867.5332 - val_loss: 32087.0586\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 31092.6426 - val_loss: 31671.6699\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 30769.3340 - val_loss: 31700.2832\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 29851.4902 - val_loss: 31956.4297\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 30778.0312 - val_loss: 31207.5645\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29672.1309 - val_loss: 30498.9238\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29230.5273 - val_loss: 30347.6270\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 28946.2637 - val_loss: 30130.6914\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 28586.5059 - val_loss: 29692.6914\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 28529.6094 - val_loss: 29416.5059\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 28250.8066 - val_loss: 29444.9766\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 28340.9219 - val_loss: 28924.7227\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 27543.5664 - val_loss: 28806.0098\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27770.0820 - val_loss: 28256.8535\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 26996.5312 - val_loss: 28326.6289\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 27014.6719 - val_loss: 27796.4258\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 26540.1152 - val_loss: 27422.0352\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 26348.2520 - val_loss: 27146.4648\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 26986.0938 - val_loss: 27094.0098\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 27007.8242 - val_loss: 27363.5820\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 26285.4570 - val_loss: 27681.6113\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 25840.0547 - val_loss: 25914.1406\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 24801.7773 - val_loss: 25783.9121\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 24554.9668 - val_loss: 25334.1387\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 25437.4082 - val_loss: 25386.4785\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 23721.5840 - val_loss: 25531.4473\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24182.4707 - val_loss: 24523.5039\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 23648.4219 - val_loss: 24476.0625\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 23453.7227 - val_loss: 24012.4258\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:50:26,793] Trial 39 finished with value: 0.43307690047183356 and parameters: {'n_layers': 1, 'n_neurons': 210, 'n_steps': 1, 'dropout_threshold': 0.30075067092787455}. Best is trial 28 with value: 0.052441262810946444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(180, 1, 13) (180,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  131.485062\n",
      "1       20002  152.509659\n",
      "2       20003  130.889267\n",
      "3       20004  264.876343\n",
      "4       20005  701.569214\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10526315789473684, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.657894736842104, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(3.3333333333333335, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5263157894736842, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01473684210526316, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 162 neurons, 15 steps.\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 41ms/step - loss: 384956960.0000 - val_loss: 1659322752.0000\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 624275776.0000 - val_loss: 74999384.0000\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 40263304.0000 - val_loss: 550593216.0000\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 196833920.0000 - val_loss: 17782104.0000\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8399825.0000 - val_loss: 35709760.0000\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 16697430.0000 - val_loss: 44075676.0000\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 19713892.0000 - val_loss: 26087690.0000\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 29012664.0000 - val_loss: 13399723.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7152725.5000 - val_loss: 22803698.0000\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 27241672.0000 - val_loss: 5995049.0000\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 23792148.0000 - val_loss: 127001272.0000\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 70885296.0000 - val_loss: 120928832.0000\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 347130560.0000 - val_loss: 1585830656.0000\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 877820864.0000 - val_loss: 483166208.0000\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 130230568.0000 - val_loss: 50217416.0000\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 132744632.0000 - val_loss: 348520512.0000\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 256543712.0000 - val_loss: 454582688.0000\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 316128256.0000 - val_loss: 214353536.0000\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 212774336.0000 - val_loss: 186094912.0000\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 199873744.0000 - val_loss: 202022288.0000\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01810e+05 2.00010e+04 2.20000e+01 2.30000e+01 2.40000e+01\n",
      "   1.00000e+00 4.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00010e+04 2.30000e+01 2.40000e+01 2.50000e+01\n",
      "   1.00000e+00 5.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00010e+04 2.40000e+01 2.50000e+01 2.60000e+01\n",
      "   1.00000e+00 5.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00010e+04 2.50000e+01 2.60000e+01 2.70000e+01\n",
      "   1.00000e+00 5.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00010e+04 2.60000e+01 2.70000e+01 2.80000e+01\n",
      "   1.00000e+00 5.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00010e+04 2.70000e+01 2.80000e+01 2.90000e+01\n",
      "   1.00000e+00 5.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00010e+04 2.80000e+01 2.90000e+01 3.00000e+01\n",
      "   1.00000e+00 6.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00010e+04 2.90000e+01 3.00000e+01 3.10000e+01\n",
      "   1.00000e+00 6.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00010e+04 3.00000e+01 3.10000e+01 3.20000e+01\n",
      "   1.00000e+00 6.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01810e+05 2.00020e+04 2.35000e+01 2.45000e+01 2.55000e+01\n",
      "   0.00000e+00 7.65000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00020e+04 2.45000e+01 2.55000e+01 2.65000e+01\n",
      "   0.00000e+00 7.95000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00020e+04 2.55000e+01 2.65000e+01 2.75000e+01\n",
      "   0.00000e+00 8.25000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00020e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   0.00000e+00 8.55000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00020e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   0.00000e+00 8.85000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00020e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   0.00000e+00 9.15000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00020e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   0.00000e+00 9.45000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00020e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   0.00000e+00 9.75000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00020e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   0.00000e+00 1.00500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01810e+05 2.00030e+04 2.15000e+01 2.25000e+01 2.35000e+01\n",
      "   1.00000e+00 4.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00030e+04 2.25000e+01 2.35000e+01 2.45000e+01\n",
      "   1.00000e+00 4.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00030e+04 2.35000e+01 2.45000e+01 2.55000e+01\n",
      "   1.00000e+00 5.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00030e+04 2.45000e+01 2.55000e+01 2.65000e+01\n",
      "   1.00000e+00 5.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00030e+04 2.55000e+01 2.65000e+01 2.75000e+01\n",
      "   1.00000e+00 5.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00030e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   1.00000e+00 5.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00030e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   1.00000e+00 5.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00030e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   1.00000e+00 6.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00030e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   1.00000e+00 6.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01810e+05 2.00040e+04 2.20000e+02 2.30000e+02 2.40000e+02\n",
      "   0.00000e+00 1.20000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00040e+04 2.30000e+02 2.40000e+02 2.50000e+02\n",
      "   0.00000e+00 1.25000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00040e+04 2.40000e+02 2.50000e+02 2.60000e+02\n",
      "   0.00000e+00 1.30000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00040e+04 2.50000e+02 2.60000e+02 2.70000e+02\n",
      "   0.00000e+00 1.35000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00040e+04 2.60000e+02 2.70000e+02 2.80000e+02\n",
      "   0.00000e+00 1.40000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00040e+04 2.70000e+02 2.80000e+02 2.90000e+02\n",
      "   0.00000e+00 1.45000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00040e+04 2.80000e+02 2.90000e+02 3.00000e+02\n",
      "   0.00000e+00 1.50000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00040e+04 2.90000e+02 3.00000e+02 3.10000e+02\n",
      "   0.00000e+00 1.55000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00040e+04 3.00000e+02 3.10000e+02 3.20000e+02\n",
      "   0.00000e+00 1.60000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01810e+05 2.00050e+04 6.80000e+01 7.30000e+01 7.80000e+01\n",
      "   1.00000e+00 7.80000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01811e+05 2.00050e+04 7.10000e+01 7.60000e+01 8.10000e+01\n",
      "   1.00000e+00 8.10000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01812e+05 2.00050e+04 7.40000e+01 7.90000e+01 8.40000e+01\n",
      "   1.00000e+00 8.40000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01901e+05 2.00050e+04 7.70000e+01 8.20000e+01 8.70000e+01\n",
      "   1.00000e+00 8.70000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01902e+05 2.00050e+04 8.00000e+01 8.50000e+01 9.00000e+01\n",
      "   1.00000e+00 9.00000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01903e+05 2.00050e+04 8.30000e+01 8.80000e+01 9.30000e+01\n",
      "   1.00000e+00 9.30000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01904e+05 2.00050e+04 8.60000e+01 9.10000e+01 9.60000e+01\n",
      "   1.00000e+00 9.60000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01905e+05 2.00050e+04 8.90000e+01 9.40000e+01 9.90000e+01\n",
      "   1.00000e+00 9.90000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01906e+05 2.00050e+04 9.20000e+01 9.70000e+01 1.02000e+02\n",
      "   1.00000e+00 1.02000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(166, 15, 13) (166,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001         0.0\n",
      "1       20002         0.0\n",
      "2       20003         0.0\n",
      "3       20004         0.0\n",
      "4       20005         0.0\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:50:29,076] Trial 40 finished with value: 1.0 and parameters: {'n_layers': 1, 'n_neurons': 162, 'n_steps': 15, 'dropout_threshold': 0.21384599978919802}. Best is trial 28 with value: 0.052441262810946444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10526315789473684, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.657894736842104, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(2.857142857142857, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5263157894736842, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01473684210526316, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 312 neurons, 1 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 162240.5000 - val_loss: 114550.1797\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 119412.1094 - val_loss: 92660.5469\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 94947.1250 - val_loss: 108529.1797\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 92524.3906 - val_loss: 78473.0312\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 81914.3125 - val_loss: 74324.1172\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 77990.7891 - val_loss: 89882.9531\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 81161.6250 - val_loss: 68615.9375\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 70102.5312 - val_loss: 91145.5078\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 89318.6328 - val_loss: 125224.2578\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 90237.9766 - val_loss: 67338.4531\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 76911.3750 - val_loss: 78636.1562\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 90447.4766 - val_loss: 112166.5391\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 70673.6562 - val_loss: 69140.4141\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 70595.3906 - val_loss: 68370.4141\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 76260.2188 - val_loss: 65329.1289\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 62147.6836 - val_loss: 54472.9531\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 55433.1289 - val_loss: 56029.1133\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 52007.5625 - val_loss: 51113.9688\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 52900.7812 - val_loss: 55729.4219\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 50987.2539 - val_loss: 50372.9766\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 47788.5078 - val_loss: 52978.0078\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 55542.7578 - val_loss: 71532.5078\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 57202.3750 - val_loss: 50505.9531\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51413.6250 - val_loss: 48074.7109\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 47513.4648 - val_loss: 45223.3867\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 60110.0117 - val_loss: 53887.1016\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 44967.9336 - val_loss: 39474.6836\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40471.9570 - val_loss: 43082.1523\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 41322.7891 - val_loss: 37272.6602\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 37410.3555 - val_loss: 41332.2539\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40097.1406 - val_loss: 38794.6914\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35416.7070 - val_loss: 36701.5859\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 36388.0508 - val_loss: 33579.0312\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31628.1211 - val_loss: 33317.7227\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31958.9609 - val_loss: 32280.8887\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30800.4062 - val_loss: 34495.7305\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31952.9785 - val_loss: 30896.4473\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29491.5605 - val_loss: 45741.5000\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 36031.0078 - val_loss: 48142.7578\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 49567.5547 - val_loss: 36630.4102\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32512.3945 - val_loss: 48183.4805\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43608.1133 - val_loss: 25498.8086\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34456.7109 - val_loss: 46097.6367\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 36445.7227 - val_loss: 29046.9590\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29427.3203 - val_loss: 32714.7676\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 34939.2617 - val_loss: 64706.9180\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 57492.5820 - val_loss: 23542.1641\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29202.9395 - val_loss: 45411.6523\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30769.4277 - val_loss: 21532.7715\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22291.4727 - val_loss: 20676.2539\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 22634.5977 - val_loss: 29945.9609\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24337.6191 - val_loss: 18645.2871\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17985.4961 - val_loss: 19489.2441\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19162.4570 - val_loss: 16912.1445\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18807.4883 - val_loss: 16651.4902\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17661.1934 - val_loss: 20778.7715\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17521.0039 - val_loss: 16830.6855\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15732.3262 - val_loss: 17476.5625\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16403.8340 - val_loss: 24516.9844\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20903.1328 - val_loss: 14422.5527\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16921.5371 - val_loss: 13414.2012\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13697.1045 - val_loss: 12909.9678\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13582.6094 - val_loss: 20500.6406\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18089.5410 - val_loss: 23119.7930\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20925.5176 - val_loss: 12324.2803\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13705.9941 - val_loss: 18433.0449\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15747.1924 - val_loss: 24688.3770\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17737.0000 - val_loss: 13275.7793\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11690.1855 - val_loss: 9775.8584\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10677.5234 - val_loss: 17357.9492\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14762.2881 - val_loss: 14177.9072\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10527.5820 - val_loss: 8638.7529\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8369.6807 - val_loss: 8807.6475\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8665.7266 - val_loss: 10299.3408\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9196.7783 - val_loss: 8735.7646\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10087.0967 - val_loss: 7430.2910\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9466.0469 - val_loss: 24435.5312\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16176.5088 - val_loss: 13729.5840\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9651.1846 - val_loss: 13073.1201\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11153.5557 - val_loss: 6690.6104\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6273.9868 - val_loss: 6388.0117\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5827.2627 - val_loss: 8443.9971\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7924.4053 - val_loss: 9791.0947\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6333.5864 - val_loss: 7382.3169\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6424.5835 - val_loss: 7423.3042\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6408.3569 - val_loss: 4666.5698\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5188.0967 - val_loss: 5780.0562\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5192.6396 - val_loss: 5876.1235\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4967.3545 - val_loss: 4036.7493\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4131.8296 - val_loss: 8768.0850\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7990.2168 - val_loss: 6189.4551\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6225.3037 - val_loss: 3424.3970\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4373.8403 - val_loss: 3290.0933\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5275.1001 - val_loss: 3083.2603\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3703.7014 - val_loss: 3668.5430\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3371.5625 - val_loss: 3251.7913\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2900.8030 - val_loss: 4294.0210\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4053.6702 - val_loss: 3212.2542\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3450.5417 - val_loss: 2363.3110\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3007.7551 - val_loss: 4394.2202\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3769.8696 - val_loss: 2450.9971\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2846.8137 - val_loss: 4274.3535\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3884.3110 - val_loss: 3765.5747\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3058.0027 - val_loss: 3451.1140\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2649.6560 - val_loss: 2331.4290\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1891.4227 - val_loss: 1570.6864\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1586.5703 - val_loss: 1622.0835\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2045.2781 - val_loss: 1727.5121\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1833.1964 - val_loss: 1814.2070\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1708.4382 - val_loss: 1551.1449\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2120.1018 - val_loss: 2030.3829\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1247.0669 - val_loss: 1607.9465\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1327.9296 - val_loss: 1049.2791\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1024.2052 - val_loss: 1382.2670\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1496.9106 - val_loss: 1533.7747\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1496.9155 - val_loss: 1076.7607\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 904.3072 - val_loss: 1004.1360\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1035.5917 - val_loss: 1016.8705\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1472.9745 - val_loss: 2575.4485\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1940.4917 - val_loss: 2347.4346\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1408.5420 - val_loss: 830.4170\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 798.4290 - val_loss: 584.7729\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 628.5071 - val_loss: 573.5069\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 598.9133 - val_loss: 567.3400\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 561.7484 - val_loss: 829.0934\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 708.3734 - val_loss: 1369.2424\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 929.4110 - val_loss: 601.2631\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 726.2269 - val_loss: 467.4298\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 515.0459 - val_loss: 500.3625\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 513.9570 - val_loss: 381.0006\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 507.9060 - val_loss: 363.3785\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 522.7212 - val_loss: 975.3093\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 827.2188 - val_loss: 491.7618\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 451.7246 - val_loss: 316.0830\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 534.3130 - val_loss: 286.1279\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 360.7827 - val_loss: 269.6106\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 288.7643 - val_loss: 246.0269\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 337.0199 - val_loss: 355.6970\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 347.4779 - val_loss: 1682.3325\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 822.5797 - val_loss: 431.2312\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 443.6134 - val_loss: 199.9926\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 366.9188 - val_loss: 212.0958\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 202.0917 - val_loss: 218.2631\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 236.7458 - val_loss: 520.6193\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 335.8810 - val_loss: 207.9814\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 249.9117 - val_loss: 342.8484\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 403.3018 - val_loss: 633.6048\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 363.3128 - val_loss: 199.3948\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 216.1300 - val_loss: 156.7459\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 151.3331 - val_loss: 140.0503\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 155.0331 - val_loss: 149.9064\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 161.8464 - val_loss: 187.0546\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 158.4565 - val_loss: 749.0057\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 458.7293 - val_loss: 123.6603\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 306.1413 - val_loss: 556.5442\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 337.8661 - val_loss: 324.1128\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 238.0577 - val_loss: 307.3209\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 271.4714 - val_loss: 113.0679\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 187.8705 - val_loss: 128.7638\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 147.0611 - val_loss: 113.1329\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 128.8530 - val_loss: 140.2759\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 153.3846 - val_loss: 106.4486\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 115.6576 - val_loss: 143.5944\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 133.8033 - val_loss: 217.6356\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 160.2652 - val_loss: 166.9780\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 157.2316 - val_loss: 121.5827\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 125.7297 - val_loss: 101.3371\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 112.8648 - val_loss: 115.6221\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 157.4634 - val_loss: 215.1615\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 163.8332 - val_loss: 246.9102\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 191.2560 - val_loss: 141.4977\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 131.9135 - val_loss: 98.3974\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 104.2083 - val_loss: 108.2036\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 122.2570 - val_loss: 152.5770\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 158.1949 - val_loss: 356.8913\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 209.8245 - val_loss: 181.0731\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 202.4333 - val_loss: 101.0774\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 115.2242 - val_loss: 100.5887\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 102.6636 - val_loss: 142.0410\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 112.2729 - val_loss: 112.4207\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 126.7249 - val_loss: 428.2047\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 237.5945 - val_loss: 117.5116\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:50:34,851] Trial 41 finished with value: 0.07671340340122684 and parameters: {'n_layers': 1, 'n_neurons': 312, 'n_steps': 1, 'dropout_threshold': 0.18242816145965232}. Best is trial 28 with value: 0.052441262810946444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(180, 1, 13) (180,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001    86.010666\n",
      "1       20002   130.475281\n",
      "2       20003    84.831528\n",
      "3       20004   205.409561\n",
      "4       20005  1280.078857\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10256410256410256, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.333333333333334, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(2.857142857142857, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5128205128205128, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01435897435897436, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 300 neurons, 2 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 22974120.0000 - val_loss: 8288400.5000\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 10895326.0000 - val_loss: 2997503.2500\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1126324.2500 - val_loss: 3226601.2500\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3083213.2500 - val_loss: 569130.1875\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 395472.2812 - val_loss: 1245617.1250\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 962843.4375 - val_loss: 96373.7500\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208892.5156 - val_loss: 450419.8750\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 266457.0938 - val_loss: 104427.1250\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 196327.8125 - val_loss: 151291.2500\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 93015.3906 - val_loss: 146352.7344\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 131557.7500 - val_loss: 73420.2031\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 81772.5391 - val_loss: 91037.7422\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 70889.5547 - val_loss: 84614.7188\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 81951.1641 - val_loss: 70141.0156\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 68757.8203 - val_loss: 77564.3672\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 71099.3359 - val_loss: 70090.4844\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 71782.1328 - val_loss: 69021.9531\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 62685.1758 - val_loss: 69218.8750\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 68004.6562 - val_loss: 65779.9453\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 71505.0391 - val_loss: 67598.4453\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 64528.1836 - val_loss: 70573.5078\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 63938.0664 - val_loss: 65494.8750\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 62959.0742 - val_loss: 61717.9062\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 61508.2383 - val_loss: 61623.4844\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 60184.0234 - val_loss: 60438.1016\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 59443.1289 - val_loss: 58772.1016\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 57651.5430 - val_loss: 57928.6875\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 58706.2500 - val_loss: 56036.6055\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 57172.4023 - val_loss: 56216.9922\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 53355.0234 - val_loss: 54138.5078\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51997.7070 - val_loss: 54490.3203\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51384.1172 - val_loss: 52148.3320\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 50798.7578 - val_loss: 51379.9062\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 48904.7148 - val_loss: 51171.7812\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 49153.9336 - val_loss: 51998.9570\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 58678.3672 - val_loss: 48826.6211\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 48271.6758 - val_loss: 48658.3516\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 45820.8008 - val_loss: 46159.6094\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 44684.6836 - val_loss: 46406.6680\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43452.8359 - val_loss: 45381.0703\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43234.3203 - val_loss: 43890.8477\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 45053.4141 - val_loss: 43467.0664\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 44947.0195 - val_loss: 41596.6641\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 39975.7969 - val_loss: 40476.6133\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40864.3398 - val_loss: 39957.5273\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 39909.6367 - val_loss: 38474.0586\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 38874.7969 - val_loss: 37640.0938\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 37523.7266 - val_loss: 37265.3711\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35159.7422 - val_loss: 36608.5508\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35593.6719 - val_loss: 35217.4180\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 34041.3672 - val_loss: 34338.5430\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33266.3828 - val_loss: 35505.3359\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 38202.6680 - val_loss: 32498.4648\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31112.0449 - val_loss: 32033.5859\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30748.2324 - val_loss: 32634.0391\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35935.7734 - val_loss: 35343.8477\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35560.7969 - val_loss: 31589.2402\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 36368.9336 - val_loss: 31881.4199\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32000.5039 - val_loss: 31646.7109\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32386.6250 - val_loss: 29890.6621\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29524.7500 - val_loss: 27328.0742\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25610.1523 - val_loss: 26365.8691\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25206.1836 - val_loss: 25416.5625\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23813.1289 - val_loss: 25838.3438\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 24646.9414 - val_loss: 24989.0762\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24359.7480 - val_loss: 23501.0938\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22734.1074 - val_loss: 23701.4883\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22409.1094 - val_loss: 22036.8438\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21292.8691 - val_loss: 21976.3945\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21879.0898 - val_loss: 21137.2656\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 22076.9414 - val_loss: 20852.8691\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 19426.8691 - val_loss: 19488.2402\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18552.1914 - val_loss: 19413.0918\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19597.9043 - val_loss: 18584.2520\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18295.7402 - val_loss: 17969.8906\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17815.2461 - val_loss: 18389.2578\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18427.6504 - val_loss: 20222.5703\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19553.9336 - val_loss: 23572.7715\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20846.9160 - val_loss: 19163.2539\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18677.2305 - val_loss: 15761.8105\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14724.4180 - val_loss: 14977.4053\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14690.3428 - val_loss: 14935.2451\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15732.0195 - val_loss: 17169.4316\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16765.1777 - val_loss: 14800.4834\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13932.5322 - val_loss: 13382.8760\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13437.3447 - val_loss: 12894.1895\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12232.6709 - val_loss: 14238.0430\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13551.4482 - val_loss: 12421.4258\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12176.5537 - val_loss: 11722.4375\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11048.8955 - val_loss: 12965.7266\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12213.8438 - val_loss: 15579.2451\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13420.0488 - val_loss: 13293.6963\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12841.9131 - val_loss: 10511.4092\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11181.8799 - val_loss: 11148.9033\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11770.2783 - val_loss: 11112.5166\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10269.1680 - val_loss: 9309.5303\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9432.1055 - val_loss: 9697.9463\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10271.7217 - val_loss: 9313.8770\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8586.2734 - val_loss: 8416.2969\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9283.0068 - val_loss: 8181.5088\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8479.0557 - val_loss: 9074.0566\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8691.7949 - val_loss: 8369.5840\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7894.9712 - val_loss: 7607.4985\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9003.8838 - val_loss: 11322.3301\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9766.8213 - val_loss: 8324.0107\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6891.4580 - val_loss: 8912.5166\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8369.6084 - val_loss: 6424.8105\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6640.6777 - val_loss: 6182.8394\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6067.0747 - val_loss: 6501.6753\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6389.9834 - val_loss: 7113.5132\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5839.3027 - val_loss: 8495.0322\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6527.1675 - val_loss: 5708.7817\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5826.4678 - val_loss: 5367.6655\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5860.9775 - val_loss: 5035.8921\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5477.4575 - val_loss: 5689.3877\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5023.4634 - val_loss: 6120.2104\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5025.3359 - val_loss: 5572.9365\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5134.2021 - val_loss: 4393.7847\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4751.9507 - val_loss: 6990.5000\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6389.4023 - val_loss: 5559.0664\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5369.1816 - val_loss: 4136.2617\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4930.8081 - val_loss: 4442.6294\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3849.1482 - val_loss: 4459.1763\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3914.4431 - val_loss: 3532.0056\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3364.0068 - val_loss: 3310.2500\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3225.8779 - val_loss: 3341.5063\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3194.8577 - val_loss: 4054.0208\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3704.0647 - val_loss: 3155.1443\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3211.9817 - val_loss: 2860.2009\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2773.9866 - val_loss: 3519.3616\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3139.6433 - val_loss: 3525.3979\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2988.7585 - val_loss: 2653.5344\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2973.7043 - val_loss: 3099.0654\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2786.0999 - val_loss: 3770.7400\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2812.2678 - val_loss: 4027.4197\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3362.8096 - val_loss: 2414.2336\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2661.3064 - val_loss: 2928.5164\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3005.0259 - val_loss: 2212.1833\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2233.7686 - val_loss: 2242.6528\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2523.2227 - val_loss: 2657.4829\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2197.4783 - val_loss: 1869.5725\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2274.6787 - val_loss: 2330.3381\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2186.6497 - val_loss: 1697.4495\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2116.3252 - val_loss: 2425.1311\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1898.8295 - val_loss: 1675.6694\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1828.2018 - val_loss: 1559.1431\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1541.7848 - val_loss: 1460.0253\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1503.7958 - val_loss: 1445.0319\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1375.5728 - val_loss: 1543.9733\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1485.8534 - val_loss: 1330.1226\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1344.2528 - val_loss: 1260.3873\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1227.1796 - val_loss: 1300.8666\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1335.4694 - val_loss: 1166.4534\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1294.6409 - val_loss: 1169.4561\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1187.1315 - val_loss: 1120.7788\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1102.8544 - val_loss: 1104.0515\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1118.9470 - val_loss: 1351.3192\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1406.8955 - val_loss: 1036.6660\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1311.6906 - val_loss: 1700.0664\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1316.4946 - val_loss: 1697.6301\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1145.8549 - val_loss: 1298.4004\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1129.0568 - val_loss: 862.1253\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 987.8790 - val_loss: 1706.6096\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1150.9458 - val_loss: 984.1992\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1034.0879 - val_loss: 905.2051\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 808.2625 - val_loss: 814.8182\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 725.9819 - val_loss: 700.2160\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 747.2747 - val_loss: 686.4680\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 701.4120 - val_loss: 653.6610\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 657.8984 - val_loss: 650.4890\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 666.5831 - val_loss: 610.2756\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 626.9022 - val_loss: 761.9389\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 724.2747 - val_loss: 602.6611\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 647.0989 - val_loss: 793.5108\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 718.7513 - val_loss: 839.4999\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 696.3469 - val_loss: 695.8190\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 679.4977 - val_loss: 1216.4252\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 835.3690 - val_loss: 496.0470\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 714.4409 - val_loss: 574.5588\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 570.2215 - val_loss: 542.6287\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 482.7423 - val_loss: 466.7224\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 505.8780 - val_loss: 646.7425\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 546.2679 - val_loss: 439.0529\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 458.8885 - val_loss: 547.9460\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 490.0534 - val_loss: 484.2070\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 509.7192 - val_loss: 709.4680\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 544.8658 - val_loss: 583.6306\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 537.9099 - val_loss: 646.8652\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 559.8000 - val_loss: 542.0687\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 454.6961 - val_loss: 440.0413\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 414.5589 - val_loss: 357.5579\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 358.5195 - val_loss: 384.6240\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 363.7633 - val_loss: 408.4448\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 382.5552 - val_loss: 402.5376\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 396.9675 - val_loss: 320.1750\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 312.5316 - val_loss: 298.9625\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 301.4429 - val_loss: 335.7579\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 339.0009 - val_loss: 286.9365\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 290.2279 - val_loss: 292.6869\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 311.8599 - val_loss: 280.6829\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:50:42,198] Trial 42 finished with value: 0.07541220430223006 and parameters: {'n_layers': 1, 'n_neurons': 300, 'n_steps': 2, 'dropout_threshold': 0.18383037254054913}. Best is trial 28 with value: 0.052441262810946444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(179, 2, 13) (179,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001    85.448372\n",
      "1       20002   129.415176\n",
      "2       20003    84.159134\n",
      "3       20004   225.386322\n",
      "4       20005  1260.237549\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.1, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.025, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(2.857142857142857, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.014000000000000002, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 270 neurons, 3 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 12063560.0000 - val_loss: 8495624.0000\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4126988.2500 - val_loss: 558982.9375\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1631445.7500 - val_loss: 991166.8750\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 414090.6875 - val_loss: 724365.3750\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 747172.9375 - val_loss: 78377.0547\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 231845.6406 - val_loss: 431447.3438\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 200540.0781 - val_loss: 161687.5938\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 230041.4219 - val_loss: 90868.0078\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 100518.4141 - val_loss: 153581.3438\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 104030.2969 - val_loss: 78999.2266\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 82968.7891 - val_loss: 65039.5234\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 70238.2188 - val_loss: 73611.2500\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 67890.9922 - val_loss: 73073.8906\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 63574.4805 - val_loss: 71068.2500\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 69199.3984 - val_loss: 62294.5352\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 62197.2148 - val_loss: 61525.8438\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 60657.8086 - val_loss: 64714.0234\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 60599.3867 - val_loss: 60534.7188\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 58867.2812 - val_loss: 60906.3984\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 57538.6055 - val_loss: 58593.0703\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 55874.3984 - val_loss: 59040.7852\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 60603.6562 - val_loss: 57108.0820\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 55174.5352 - val_loss: 56564.3281\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 54409.8633 - val_loss: 56119.0938\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 59027.0078 - val_loss: 55737.8203\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 52832.1094 - val_loss: 55872.5000\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 52488.8477 - val_loss: 52929.8281\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 50644.5078 - val_loss: 52393.4688\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51135.5039 - val_loss: 51438.0898\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 49011.4648 - val_loss: 53395.8086\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 52373.8594 - val_loss: 49299.1133\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 47414.7656 - val_loss: 54124.8398\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 64695.9883 - val_loss: 52001.7031\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 66098.9375 - val_loss: 46682.8828\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 49745.7227 - val_loss: 46220.0781\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 50555.8359 - val_loss: 46028.1367\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 51510.0078 - val_loss: 44234.3203\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 48820.0664 - val_loss: 43128.5000\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 41674.3359 - val_loss: 43008.5703\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 43255.5234 - val_loss: 41655.8086\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 40639.9805 - val_loss: 42080.0742\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 42588.9609 - val_loss: 42428.4336\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 44007.7773 - val_loss: 42751.9805\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 48461.0156 - val_loss: 39116.9297\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 37658.5117 - val_loss: 40402.5742\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 43603.1367 - val_loss: 40704.0781\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 39116.2930 - val_loss: 36083.6289\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35915.6797 - val_loss: 39826.4375\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 46435.2070 - val_loss: 45485.3633\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 46092.5508 - val_loss: 38353.4922\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 35484.0664 - val_loss: 40272.8984\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 36331.4609 - val_loss: 37425.4102\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 34829.8555 - val_loss: 38735.5117\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32415.3281 - val_loss: 42857.5312\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 36122.8672 - val_loss: 37991.3711\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 33108.3516 - val_loss: 30006.9668\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 28636.3438 - val_loss: 28833.2656\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30822.4961 - val_loss: 32066.9277\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 34574.1133 - val_loss: 45790.8164\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 41276.0117 - val_loss: 52824.7734\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40932.6836 - val_loss: 64983.1172\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 47478.7422 - val_loss: 55877.0469\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 41165.6641 - val_loss: 39695.5703\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 32657.6406 - val_loss: 26523.9160\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24504.3320 - val_loss: 23311.3867\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 23961.5957 - val_loss: 22745.4531\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 21717.9238 - val_loss: 22368.3906\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22130.0859 - val_loss: 25359.0957\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 25338.5098 - val_loss: 31477.4023\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26439.7148 - val_loss: 26305.6660\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21765.3301 - val_loss: 21721.7305\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21112.3242 - val_loss: 19543.0723\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 20109.8477 - val_loss: 19913.3496\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19159.3105 - val_loss: 21018.0840\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 18877.4961 - val_loss: 19349.6289\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 19736.1973 - val_loss: 17675.1738\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 16902.1328 - val_loss: 17741.9297\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 16959.4863 - val_loss: 16805.0137\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 16423.5156 - val_loss: 16531.4961\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 16831.0117 - val_loss: 16470.9199\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 15913.5596 - val_loss: 18004.2891\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 16089.8252 - val_loss: 31860.7168\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 24789.0625 - val_loss: 31533.3984\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 23671.4629 - val_loss: 15202.2676\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 18519.5723 - val_loss: 14467.9854\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 14838.3799 - val_loss: 14154.9492\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 13180.1719 - val_loss: 18948.9160\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 16669.5469 - val_loss: 13046.2861\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 18694.0449 - val_loss: 17782.3203\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 14388.0449 - val_loss: 12423.7363\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 13501.8760 - val_loss: 15962.9229\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15186.7344 - val_loss: 20643.5977\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 13630.9375 - val_loss: 12245.0967\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 13473.7578 - val_loss: 10801.8418\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 10245.4258 - val_loss: 10077.8750\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9900.5420 - val_loss: 10229.3584\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9800.8184 - val_loss: 11630.1982\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 10292.7832 - val_loss: 10060.3984\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9694.9609 - val_loss: 8822.6074\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8805.4922 - val_loss: 9245.3047\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9173.2119 - val_loss: 14160.3525\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10828.2988 - val_loss: 12860.7646\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 11420.1064 - val_loss: 9017.0762\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10482.7656 - val_loss: 9668.3457\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9572.7402 - val_loss: 9813.5215\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 11053.7383 - val_loss: 7424.6416\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7719.7690 - val_loss: 7844.5820\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7648.7148 - val_loss: 6696.7085\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8086.1074 - val_loss: 9462.4434\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7215.3828 - val_loss: 7726.1548\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6185.0039 - val_loss: 6026.7095\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6305.1455 - val_loss: 5724.6753\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5491.3745 - val_loss: 5503.8291\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5398.5024 - val_loss: 5551.3154\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5580.7603 - val_loss: 7196.6709\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6248.6572 - val_loss: 5073.6846\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 5316.8638 - val_loss: 7659.2632\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5820.6543 - val_loss: 5457.4155\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4916.2031 - val_loss: 5020.0854\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 5171.6895 - val_loss: 7810.6831\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6180.8242 - val_loss: 4107.4785\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5149.6567 - val_loss: 8867.0674\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6121.7886 - val_loss: 5037.5601\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4359.9180 - val_loss: 4240.1719\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4294.6953 - val_loss: 4100.5273\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3956.2388 - val_loss: 4920.9272\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4893.7456 - val_loss: 4068.2507\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4102.2993 - val_loss: 6790.2871\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5828.7988 - val_loss: 5319.9150\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5107.7705 - val_loss: 2948.7705\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2849.5254 - val_loss: 2960.4487\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2900.7563 - val_loss: 3254.8762\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3178.4661 - val_loss: 4501.8447\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3415.3965 - val_loss: 2720.2964\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2487.6174 - val_loss: 2480.5991\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2404.0508 - val_loss: 2475.9485\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2206.9968 - val_loss: 3192.4985\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2741.6250 - val_loss: 2266.8464\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2671.1191 - val_loss: 2398.6753\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2340.9531 - val_loss: 3234.4966\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3369.6182 - val_loss: 1912.0225\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2457.4424 - val_loss: 4356.1440\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3004.0442 - val_loss: 1847.0256\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1849.8774 - val_loss: 2994.7788\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2389.1223 - val_loss: 2185.1533\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1998.4637 - val_loss: 2350.1870\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2270.8962 - val_loss: 1556.7039\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2810.6885 - val_loss: 2971.8347\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2141.0486 - val_loss: 2158.2571\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1984.8138 - val_loss: 1812.2198\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1642.5065 - val_loss: 1626.5471\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1696.2870 - val_loss: 1325.6577\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1519.3645 - val_loss: 2453.5925\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1901.9105 - val_loss: 1249.1224\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1607.6230 - val_loss: 2056.5818\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1358.2997 - val_loss: 1119.9905\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1224.5092 - val_loss: 1114.1111\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1125.1106 - val_loss: 1816.0518\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1579.2328 - val_loss: 980.9318\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1159.5360 - val_loss: 1456.4235\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1344.8892 - val_loss: 1687.7596\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1455.0156 - val_loss: 964.3419\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1332.8105 - val_loss: 864.3086\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 984.2278 - val_loss: 866.1259\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 879.6228 - val_loss: 836.0281\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 748.8706 - val_loss: 1565.6198\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1401.3745 - val_loss: 1576.3467\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1417.2422 - val_loss: 931.1597\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1804.7343 - val_loss: 899.6458\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1562.2185 - val_loss: 3199.8374\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1693.5682 - val_loss: 735.2757\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 819.7270 - val_loss: 606.6786\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 637.0140 - val_loss: 703.9507\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 696.3558 - val_loss: 778.1007\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 720.6592 - val_loss: 558.3038\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 568.0459 - val_loss: 685.1541\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 624.2574 - val_loss: 517.1994\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 516.4612 - val_loss: 515.2631\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 546.8769 - val_loss: 643.2212\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 695.6245 - val_loss: 787.7166\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 674.2496 - val_loss: 469.3971\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 491.1616 - val_loss: 472.2916\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 544.7555 - val_loss: 453.6375\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 483.6065 - val_loss: 427.6385\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 541.3924 - val_loss: 587.0392\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 895.2501 - val_loss: 454.6957\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 680.5974 - val_loss: 726.6843\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 833.8411 - val_loss: 672.3085\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 605.5217 - val_loss: 968.1091\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 653.6296 - val_loss: 1020.7518\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 965.5728 - val_loss: 370.4185\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 539.5619 - val_loss: 952.3892\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 618.8699 - val_loss: 366.7982\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 405.8249 - val_loss: 544.1003\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 519.8343 - val_loss: 602.4133\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 580.4681 - val_loss: 366.1487\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 473.3900 - val_loss: 381.9383\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 427.8730 - val_loss: 605.9815\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 444.1376 - val_loss: 512.8165\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 496.5061 - val_loss: 415.6847\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:50:49,430] Trial 43 finished with value: 0.12102854499690466 and parameters: {'n_layers': 1, 'n_neurons': 270, 'n_steps': 3, 'dropout_threshold': 0.11914633857074829}. Best is trial 28 with value: 0.052441262810946444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(178, 3, 13) (178,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001    98.555000\n",
      "1       20002   142.836212\n",
      "2       20003    97.181488\n",
      "3       20004   237.549561\n",
      "4       20005  1284.224609\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0975609756097561, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(11.731707317073171, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(2.857142857142857, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4878048780487805, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013658536585365855, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 357 neurons, 12 steps.\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 37ms/step - loss: 806055872.0000 - val_loss: 533595872.0000\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 156389104.0000 - val_loss: 30711312.0000\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 113763672.0000 - val_loss: 45133876.0000\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 30325182.0000 - val_loss: 9983878.0000\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 89379320.0000 - val_loss: 61633808.0000\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 45368156.0000 - val_loss: 1464143.8750\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9308184.0000 - val_loss: 15552976.0000\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 216687792.0000 - val_loss: 10113506.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 39168068.0000 - val_loss: 166181120.0000\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 66499532.0000 - val_loss: 353209.8438\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2017624.8750 - val_loss: 22836766.0000\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7117260.5000 - val_loss: 140935.9062\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3653919.5000 - val_loss: 3911428.7500\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2026039.1250 - val_loss: 1953103.8750\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3084840.7500 - val_loss: 141699712.0000\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 58102952.0000 - val_loss: 373275968.0000\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 92351856.0000 - val_loss: 24130876.0000\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 16882358.0000 - val_loss: 6942191.0000\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5603695.0000 - val_loss: 208525.7031\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6006254.5000 - val_loss: 5202515.5000\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2183403.2500 - val_loss: 16573607.0000\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7676847.5000 - val_loss: 7110712.5000\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01901e+05 2.00010e+04 2.50000e+01 2.60000e+01 2.70000e+01\n",
      "   1.00000e+00 5.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00010e+04 2.60000e+01 2.70000e+01 2.80000e+01\n",
      "   1.00000e+00 5.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00010e+04 2.70000e+01 2.80000e+01 2.90000e+01\n",
      "   1.00000e+00 5.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00010e+04 2.80000e+01 2.90000e+01 3.00000e+01\n",
      "   1.00000e+00 6.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00010e+04 2.90000e+01 3.00000e+01 3.10000e+01\n",
      "   1.00000e+00 6.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00010e+04 3.00000e+01 3.10000e+01 3.20000e+01\n",
      "   1.00000e+00 6.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01901e+05 2.00020e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   0.00000e+00 8.55000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00020e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   0.00000e+00 8.85000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00020e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   0.00000e+00 9.15000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00020e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   0.00000e+00 9.45000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00020e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   0.00000e+00 9.75000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00020e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   0.00000e+00 1.00500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01901e+05 2.00030e+04 2.45000e+01 2.55000e+01 2.65000e+01\n",
      "   1.00000e+00 5.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00030e+04 2.55000e+01 2.65000e+01 2.75000e+01\n",
      "   1.00000e+00 5.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00030e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   1.00000e+00 5.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00030e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   1.00000e+00 5.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00030e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   1.00000e+00 6.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00030e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   1.00000e+00 6.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:50:51,730] Trial 44 finished with value: 0.9006214929151406 and parameters: {'n_layers': 1, 'n_neurons': 357, 'n_steps': 12, 'dropout_threshold': 0.15546419528383326}. Best is trial 28 with value: 0.052441262810946444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01901e+05 2.00040e+04 2.50000e+02 2.60000e+02 2.70000e+02\n",
      "   0.00000e+00 1.35000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00040e+04 2.60000e+02 2.70000e+02 2.80000e+02\n",
      "   0.00000e+00 1.40000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00040e+04 2.70000e+02 2.80000e+02 2.90000e+02\n",
      "   0.00000e+00 1.45000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00040e+04 2.80000e+02 2.90000e+02 3.00000e+02\n",
      "   0.00000e+00 1.50000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00040e+04 2.90000e+02 3.00000e+02 3.10000e+02\n",
      "   0.00000e+00 1.55000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00040e+04 3.00000e+02 3.10000e+02 3.20000e+02\n",
      "   0.00000e+00 1.60000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01901e+05 2.00050e+04 7.70000e+01 8.20000e+01 8.70000e+01\n",
      "   1.00000e+00 8.70000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01902e+05 2.00050e+04 8.00000e+01 8.50000e+01 9.00000e+01\n",
      "   1.00000e+00 9.00000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01903e+05 2.00050e+04 8.30000e+01 8.80000e+01 9.30000e+01\n",
      "   1.00000e+00 9.30000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01904e+05 2.00050e+04 8.60000e+01 9.10000e+01 9.60000e+01\n",
      "   1.00000e+00 9.60000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01905e+05 2.00050e+04 8.90000e+01 9.40000e+01 9.90000e+01\n",
      "   1.00000e+00 9.90000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01906e+05 2.00050e+04 9.20000e+01 9.70000e+01 1.02000e+02\n",
      "   1.00000e+00 1.02000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(169, 12, 13) (169,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  126.527550\n",
      "1       20002  116.810829\n",
      "2       20003  127.364647\n",
      "3       20004    0.000000\n",
      "4       20005    0.000000\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09523809523809523, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(11.452380952380953, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(2.857142857142857, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.47619047619047616, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013333333333333334, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 2 layers, 295 neurons, 4 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 37ms/step - loss: 18795922.0000 - val_loss: 2334061.7500\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 12288581.0000 - val_loss: 525882.9375\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 12461231.0000 - val_loss: 11444105.0000\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 11796346.0000 - val_loss: 8102071.0000\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 10792232.0000 - val_loss: 2822407.2500\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 10727692.0000 - val_loss: 28524942.0000\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9342805.0000 - val_loss: 5422715.5000\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 10129069.0000 - val_loss: 20011186.0000\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9627654.0000 - val_loss: 2138261.7500\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8845395.0000 - val_loss: 11247396.0000\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8051185.0000 - val_loss: 3363571.2500\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6129840.0000 - val_loss: 129343.1172\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4768266.5000 - val_loss: 4296737.5000\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5337502.5000 - val_loss: 1213762.1250\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3039780.2500 - val_loss: 1407005.3750\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3388049.7500 - val_loss: 5212472.5000\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3291843.7500 - val_loss: 1522634.2500\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2567290.0000 - val_loss: 1259450.0000\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2906566.0000 - val_loss: 155375.3594\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2215266.0000 - val_loss: 165766.4375\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2332198.5000 - val_loss: 1867131.3750\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1664552.7500 - val_loss: 789191.3125\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:50:54,395] Trial 45 finished with value: 1.0 and parameters: {'n_layers': 2, 'n_neurons': 295, 'n_steps': 4, 'dropout_threshold': 0.23360796386693428}. Best is trial 28 with value: 0.052441262810946444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(177, 4, 13) (177,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001         0.0\n",
      "1       20002         0.0\n",
      "2       20003         0.0\n",
      "3       20004         0.0\n",
      "4       20005         0.0\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09302325581395349, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(11.186046511627907, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(2.857142857142857, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.46511627906976744, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013023255813953489, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 333 neurons, 2 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 30ms/step - loss: 2545134.5000 - val_loss: 510637.3438\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 252081.6562 - val_loss: 230817.2500\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 158130.6406 - val_loss: 97848.3047\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 113392.1094 - val_loss: 118137.1484\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 160562.6562 - val_loss: 203207.6719\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 142828.8594 - val_loss: 86584.1875\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 85709.5781 - val_loss: 81255.4297\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 96611.3438 - val_loss: 149354.9375\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 105191.9297 - val_loss: 76247.4688\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 102593.8438 - val_loss: 136422.3438\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 109140.9531 - val_loss: 73464.1406\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 71798.4531 - val_loss: 79175.1094\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 80163.7578 - val_loss: 100238.2734\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 91245.1484 - val_loss: 68738.8672\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 68297.4922 - val_loss: 82201.4453\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 74338.9688 - val_loss: 70705.8125\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 63367.9570 - val_loss: 69925.0625\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 67249.3125 - val_loss: 63900.0820\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 65862.1016 - val_loss: 80720.0859\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 74379.5703 - val_loss: 64972.4961\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 64192.5430 - val_loss: 58878.7812\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 57781.6055 - val_loss: 59889.7344\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 59093.7500 - val_loss: 61501.4453\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 61896.6523 - val_loss: 123810.2031\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 102207.8203 - val_loss: 77956.4375\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 57064.2305 - val_loss: 59708.1641\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 60580.2031 - val_loss: 50877.1602\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 71085.2422 - val_loss: 85295.7656\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 81505.6016 - val_loss: 64458.2734\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 62382.3672 - val_loss: 68200.9922\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 54633.5859 - val_loss: 45784.6406\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 45996.3555 - val_loss: 64042.5391\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 59891.1953 - val_loss: 124066.7656\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 82296.2500 - val_loss: 59506.0820\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 47535.5742 - val_loss: 41675.2188\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 39342.7305 - val_loss: 52741.6328\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 45547.7227 - val_loss: 64303.5547\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 58669.8047 - val_loss: 42262.6289\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 55183.7812 - val_loss: 36068.4336\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 41786.5000 - val_loss: 88192.4453\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 93441.4219 - val_loss: 116968.5469\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 101686.4297 - val_loss: 139753.4219\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 101460.6016 - val_loss: 102628.3906\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 74257.2266 - val_loss: 32211.1836\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 45879.4766 - val_loss: 44349.0781\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 63166.8594 - val_loss: 113894.0391\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 79590.0391 - val_loss: 59302.5312\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 39240.6992 - val_loss: 61013.3828\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 50375.7930 - val_loss: 102233.4453\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 55607.3711 - val_loss: 48137.5039\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 36503.2148 - val_loss: 25250.4297\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 31120.1426 - val_loss: 42073.2148\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 38128.7812 - val_loss: 29063.8672\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 26392.6973 - val_loss: 37524.6992\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 35754.0508 - val_loss: 22520.8887\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 27532.7324 - val_loss: 36498.0078\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 30372.4355 - val_loss: 45518.0117\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 27784.1914 - val_loss: 24343.1660\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 28962.9902 - val_loss: 39573.2773\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 39886.3672 - val_loss: 55470.4727\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 37587.5078 - val_loss: 18414.7207\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20584.7109 - val_loss: 18231.1387\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 20748.4277 - val_loss: 16341.3789\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 17171.0156 - val_loss: 16575.2695\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 16564.1719 - val_loss: 16730.5508\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17272.1152 - val_loss: 14303.1299\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 14814.4404 - val_loss: 13788.7441\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 13129.6250 - val_loss: 12952.6123\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 12426.5908 - val_loss: 15891.1689\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 15582.8096 - val_loss: 34294.9297\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 22879.1387 - val_loss: 11475.3330\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 15067.6719 - val_loss: 32379.4707\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 19571.7188 - val_loss: 41548.0391\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 29322.5371 - val_loss: 12277.7734\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 18997.8086 - val_loss: 64772.2422\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 31971.3848 - val_loss: 37085.6680\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18062.6250 - val_loss: 10967.7002\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 12422.4873 - val_loss: 12103.9346\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 13039.3916 - val_loss: 10491.5439\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10520.2471 - val_loss: 14495.2666\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 13958.2959 - val_loss: 7797.6938\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 13177.1143 - val_loss: 19395.6758\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 12100.0664 - val_loss: 36919.9805\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 18604.9062 - val_loss: 6994.3291\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 10217.2773 - val_loss: 7732.1611\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 7143.9453 - val_loss: 7007.1030\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6133.4194 - val_loss: 10428.1943\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 7546.2578 - val_loss: 6301.7544\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6590.1470 - val_loss: 6182.0234\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5782.6699 - val_loss: 5360.4639\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5586.2910 - val_loss: 13097.1729\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9640.9062 - val_loss: 4983.1289\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 11377.8926 - val_loss: 24206.3203\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 16426.1777 - val_loss: 27527.4785\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 16549.2070 - val_loss: 6787.8003\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 12795.8340 - val_loss: 41972.3320\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 22216.0918 - val_loss: 6918.7397\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 12742.0781 - val_loss: 16370.5605\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 14415.9785 - val_loss: 7195.1982\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5031.3174 - val_loss: 3449.3081\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4620.9844 - val_loss: 10206.6670\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5219.7612 - val_loss: 4409.6724\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3912.9348 - val_loss: 5253.2349\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4059.1882 - val_loss: 9447.3672\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4977.0259 - val_loss: 5921.1636\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 5096.5303 - val_loss: 2474.9062\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2860.2483 - val_loss: 5000.2949\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4403.4731 - val_loss: 4284.3330\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3009.5259 - val_loss: 4781.1650\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3739.9546 - val_loss: 2144.5901\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3483.9954 - val_loss: 5224.2529\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 4980.1172 - val_loss: 5627.7300\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3926.4163 - val_loss: 3023.6631\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 4406.6401 - val_loss: 11137.9219\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6445.0996 - val_loss: 4986.9956\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3833.8694 - val_loss: 1859.0201\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3501.4873 - val_loss: 2178.9941\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2159.5149 - val_loss: 1558.5193\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1305.7297 - val_loss: 1940.3746\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1912.5135 - val_loss: 1228.3866\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1244.3912 - val_loss: 1323.3754\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1449.5753 - val_loss: 1093.5148\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1296.6799 - val_loss: 1182.9766\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1067.6377 - val_loss: 994.2886\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 975.1938 - val_loss: 2302.8860\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1965.0385 - val_loss: 1153.9019\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1477.8677 - val_loss: 4031.4075\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1580.8105 - val_loss: 3417.9119\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2360.5012 - val_loss: 1156.8535\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 919.1551 - val_loss: 1167.7539\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1066.2720 - val_loss: 817.5818\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 902.1834 - val_loss: 666.7404\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 824.2441 - val_loss: 3190.9075\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2261.8657 - val_loss: 2872.0415\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1637.0211 - val_loss: 1470.8915\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1038.5818 - val_loss: 2315.5127\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1625.0461 - val_loss: 4322.4463\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1847.0774 - val_loss: 2650.3486\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1464.7544 - val_loss: 9449.8984\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 5972.5288 - val_loss: 8470.5635\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 5523.1689 - val_loss: 5451.9707\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3156.9653 - val_loss: 919.9506\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:51:02,946] Trial 46 finished with value: 0.05197422617205154 and parameters: {'n_layers': 1, 'n_neurons': 333, 'n_steps': 2, 'dropout_threshold': 0.19656820478562106}. Best is trial 46 with value: 0.05197422617205154.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(179, 2, 13) (179,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001    88.684402\n",
      "1       20002   130.315491\n",
      "2       20003    87.276421\n",
      "3       20004   231.022156\n",
      "4       20005  1208.452759\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09090909090909091, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(10.931818181818182, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(2.857142857142857, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.45454545454545453, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.012727272727272728, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 2 layers, 396 neurons, 2 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 41ms/step - loss: 2285699.5000 - val_loss: 1441561.5000\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 987327.4375 - val_loss: 789037.2500\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 958448.5625 - val_loss: 1757690.3750\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 753707.7500 - val_loss: 419432.5312\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 575017.1250 - val_loss: 679201.0625\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 447533.1250 - val_loss: 195107.1875\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 363918.9688 - val_loss: 92282.2188\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 283021.1875 - val_loss: 122613.2891\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 272626.2188 - val_loss: 91052.3047\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 200304.8594 - val_loss: 85241.8906\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 191969.1094 - val_loss: 87567.8281\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 196734.7031 - val_loss: 84795.0625\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 148128.0000 - val_loss: 121435.6719\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 144730.5312 - val_loss: 89636.0703\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 156651.7031 - val_loss: 89153.0938\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 141567.8750 - val_loss: 85567.0625\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 158185.3906 - val_loss: 117200.5703\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 179177.5469 - val_loss: 97030.9609\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 212744.3438 - val_loss: 261653.2969\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 258353.2656 - val_loss: 157296.6562\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207653.9531 - val_loss: 140783.2656\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 143115.8125 - val_loss: 84123.5625\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 144984.8594 - val_loss: 152648.9531\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 146999.1562 - val_loss: 88136.5078\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 121553.0234 - val_loss: 85318.6016\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 121963.3672 - val_loss: 83689.7422\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 123593.9609 - val_loss: 83877.4062\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 114405.3203 - val_loss: 83641.7422\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 104172.3750 - val_loss: 83826.7734\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 101480.0234 - val_loss: 94057.8906\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 109248.1953 - val_loss: 88990.1719\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 117877.8750 - val_loss: 85189.4922\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 95798.1562 - val_loss: 86328.1875\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 116162.8281 - val_loss: 84072.4531\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 102787.2344 - val_loss: 83810.4297\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 129947.5703 - val_loss: 84175.8438\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 117879.8203 - val_loss: 85184.3281\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 112367.9766 - val_loss: 84243.8438\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:51:06,522] Trial 47 finished with value: 0.8000182277935749 and parameters: {'n_layers': 2, 'n_neurons': 396, 'n_steps': 2, 'dropout_threshold': 0.19487221896193224}. Best is trial 46 with value: 0.05197422617205154.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(179, 2, 13) (179,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001  197.135147\n",
      "1       20002  197.352448\n",
      "2       20003  197.121887\n",
      "3       20004  198.539764\n",
      "4       20005  203.018997\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08888888888888889, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(10.688888888888888, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(2.857142857142857, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4444444444444444, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.012444444444444445, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 1 layers, 20 neurons, 5 steps.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 23ms/step - loss: 162611856.0000 - val_loss: 28773136.0000\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9600009.0000 - val_loss: 7497018.5000\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16351125.0000 - val_loss: 23110652.0000\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17618516.0000 - val_loss: 4939851.5000\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1814735.8750 - val_loss: 989696.0000\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2505387.0000 - val_loss: 3882764.0000\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2936412.5000 - val_loss: 732737.3750\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 263540.6562 - val_loss: 362446.3438\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 632156.1250 - val_loss: 688752.1250\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 437292.8750 - val_loss: 72107.7578\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 88190.4922 - val_loss: 209688.3125\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 199642.0156 - val_loss: 117276.6016\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 78317.8828 - val_loss: 66078.0469\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 76845.7656 - val_loss: 76547.0156\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 63389.5078 - val_loss: 59397.8438\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 62493.1758 - val_loss: 70699.8203\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 63926.9453 - val_loss: 59064.2305\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 55046.4180 - val_loss: 57423.7969\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 55215.1992 - val_loss: 58822.4922\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 63050.3594 - val_loss: 74768.8828\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 76864.6250 - val_loss: 62882.4102\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 57288.5820 - val_loss: 65392.0898\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 65287.0234 - val_loss: 61980.5586\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 55270.5820 - val_loss: 54933.8906\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 53847.0195 - val_loss: 54434.3438\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 53206.1250 - val_loss: 56497.8086\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 53752.9453 - val_loss: 54086.8906\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 51544.6836 - val_loss: 53029.2031\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 50602.3750 - val_loss: 52531.0234\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 50294.1328 - val_loss: 52274.5430\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 50133.2695 - val_loss: 52658.3203\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 50415.4766 - val_loss: 51840.1211\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 49462.9453 - val_loss: 51206.4141\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 49381.9141 - val_loss: 51166.8789\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 48718.8906 - val_loss: 50564.9219\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 48679.9180 - val_loss: 50410.8672\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 48125.2656 - val_loss: 49735.4844\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 47358.9766 - val_loss: 51588.6875\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 69280.6250 - val_loss: 62810.5547\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 50467.4805 - val_loss: 58027.8008\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 58463.3398 - val_loss: 47203.3242\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 65022.8125 - val_loss: 78826.3828\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 57280.8086 - val_loss: 54365.9414\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 60604.3125 - val_loss: 54473.9219\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 47168.4883 - val_loss: 62184.6680\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 79314.3750 - val_loss: 62197.9414\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 49697.6602 - val_loss: 61115.4570\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 61481.8555 - val_loss: 45023.7734\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 43257.9375 - val_loss: 52885.8242\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51269.6758 - val_loss: 42419.0273\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 45079.9453 - val_loss: 51802.5742\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 45596.7227 - val_loss: 41536.8477\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 39825.6211 - val_loss: 41243.7930\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 39730.2969 - val_loss: 40756.1367\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 41489.5078 - val_loss: 41855.9375\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38555.8008 - val_loss: 44727.8789\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 42906.3828 - val_loss: 40241.7109\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 39340.5000 - val_loss: 39274.4805\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 40170.7734 - val_loss: 41910.2852\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 38580.5977 - val_loss: 38303.0039\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 36906.8945 - val_loss: 37973.5352\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 36715.6602 - val_loss: 37707.1250\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 37180.8438 - val_loss: 37438.0977\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35697.3164 - val_loss: 40903.5000\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 51338.6562 - val_loss: 58402.0898\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51572.2656 - val_loss: 36204.9648\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 41130.0977 - val_loss: 41951.4688\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35722.4023 - val_loss: 37464.7188\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35738.7148 - val_loss: 34503.8633\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 34084.1914 - val_loss: 34656.8633\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32471.5273 - val_loss: 33064.9141\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 31895.4805 - val_loss: 33190.8906\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32270.4883 - val_loss: 32314.4434\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31797.3633 - val_loss: 33814.2734\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 31573.5273 - val_loss: 31569.5957\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30293.9883 - val_loss: 32965.4453\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 39900.4766 - val_loss: 38519.9648\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32970.6992 - val_loss: 32897.3555\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32169.6953 - val_loss: 30240.4766\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28501.5332 - val_loss: 29772.1953\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28704.5332 - val_loss: 29224.2324\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30203.0527 - val_loss: 29175.1777\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29694.6113 - val_loss: 31738.7500\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29043.8691 - val_loss: 28335.8516\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27252.1582 - val_loss: 28017.3789\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27374.0840 - val_loss: 28420.5508\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27079.8730 - val_loss: 26995.7461\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26073.6738 - val_loss: 26889.2949\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25876.7539 - val_loss: 26447.1387\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25458.6426 - val_loss: 25999.2285\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25644.5820 - val_loss: 26091.1934\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24783.0293 - val_loss: 25366.6836\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24371.0410 - val_loss: 25173.6738\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 24317.0762 - val_loss: 25113.7832\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23956.6074 - val_loss: 25287.0625\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27307.5879 - val_loss: 24221.1953\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24151.7559 - val_loss: 27426.8379\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24566.4785 - val_loss: 23673.0625\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23095.8379 - val_loss: 23716.2051\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 22977.9512 - val_loss: 22816.6445\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 21973.9434 - val_loss: 24967.4961\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33434.7656 - val_loss: 22421.0859\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26669.4414 - val_loss: 27472.9277\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22499.8438 - val_loss: 23820.3066\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21333.3887 - val_loss: 23097.1621\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22634.6855 - val_loss: 20670.8828\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19780.4941 - val_loss: 20332.2559\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19495.8770 - val_loss: 20477.0957\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19826.9336 - val_loss: 19852.2715\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19152.7500 - val_loss: 19542.8770\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18817.4453 - val_loss: 19248.1953\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18617.5801 - val_loss: 19077.5938\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18368.0566 - val_loss: 18862.5215\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18807.1621 - val_loss: 18431.0430\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17797.4082 - val_loss: 18159.3105\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17464.1914 - val_loss: 18146.9043\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17698.7930 - val_loss: 17618.5059\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16933.9922 - val_loss: 17349.3906\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 16749.2129 - val_loss: 18621.9199\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 22892.6562 - val_loss: 16755.8262\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21553.1758 - val_loss: 19063.1230\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17322.8457 - val_loss: 18311.9297\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16117.0527 - val_loss: 18804.1172\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17391.0547 - val_loss: 15945.2051\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15430.9316 - val_loss: 15621.0244\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15121.5947 - val_loss: 16899.8066\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22022.7285 - val_loss: 14902.5576\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17505.7109 - val_loss: 17348.7832\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15294.4639 - val_loss: 15316.4248\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13879.5049 - val_loss: 15937.9609\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14668.9453 - val_loss: 15670.9023\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17593.4141 - val_loss: 13748.4424\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14872.8242 - val_loss: 15286.8672\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14171.5127 - val_loss: 13473.1719\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 12661.6973 - val_loss: 14063.7646\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13428.2920 - val_loss: 12812.6084\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12403.9414 - val_loss: 12809.2754\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12343.7354 - val_loss: 12423.6846\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12102.6816 - val_loss: 12429.4893\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12224.6377 - val_loss: 12105.3770\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 11665.4150 - val_loss: 12045.5059\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 12406.7363 - val_loss: 11734.0625\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11335.3496 - val_loss: 11917.8828\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11473.2920 - val_loss: 11270.4912\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10987.3164 - val_loss: 11373.2539\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11153.4932 - val_loss: 10991.3730\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10735.7832 - val_loss: 12253.7012\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13595.6953 - val_loss: 10979.4834\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14758.5186 - val_loss: 10434.0576\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10676.1074 - val_loss: 16218.4141\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15771.8184 - val_loss: 11118.5693\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13956.2559 - val_loss: 9708.1592\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10040.4531 - val_loss: 14492.5869\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14842.2656 - val_loss: 10565.7031\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12073.1172 - val_loss: 8944.6172\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8885.7090 - val_loss: 12461.1084\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13295.3818 - val_loss: 9740.4893\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 11200.1865 - val_loss: 8342.2021\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 8622.1572 - val_loss: 8170.2207\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8992.2959 - val_loss: 8291.6113\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8100.5498 - val_loss: 12001.8457\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12686.6875 - val_loss: 8114.9248\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7891.1499 - val_loss: 7618.3477\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7555.2910 - val_loss: 7194.1499\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7175.4858 - val_loss: 7016.7109\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7044.3066 - val_loss: 7081.2983\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6760.6392 - val_loss: 6783.1348\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6528.2451 - val_loss: 6978.2681\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6830.6885 - val_loss: 7703.0400\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8786.0361 - val_loss: 6925.6777\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8931.1895 - val_loss: 6286.1265\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7048.1299 - val_loss: 6182.7085\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7288.9951 - val_loss: 7450.7305\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11720.8164 - val_loss: 6324.9746\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5974.7129 - val_loss: 6326.9531\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5784.0776 - val_loss: 5757.7900\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5673.4141 - val_loss: 5769.1626\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5502.8159 - val_loss: 6815.4692\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7645.5776 - val_loss: 5860.6753\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7347.6157 - val_loss: 7542.4072\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13783.7764 - val_loss: 6106.5732\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10337.4639 - val_loss: 4952.4424\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6273.2231 - val_loss: 4868.2490\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6022.3857 - val_loss: 5014.7344\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4810.7896 - val_loss: 4773.6108\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4838.9985 - val_loss: 4747.8164\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4607.5459 - val_loss: 4504.4150\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4439.5454 - val_loss: 4871.5850\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5165.8413 - val_loss: 4407.9653\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4367.9932 - val_loss: 5287.1685\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6396.1768 - val_loss: 4712.3052\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5637.4062 - val_loss: 4137.2627\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4284.6206 - val_loss: 4052.2585\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4642.6870 - val_loss: 5144.1846\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7655.4326 - val_loss: 3835.4041\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3891.5698 - val_loss: 3753.3926\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3672.3433 - val_loss: 3665.3376\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3658.4390 - val_loss: 4568.5308\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5940.6680 - val_loss: 4614.7114\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7246.2681 - val_loss: 3904.5320\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:51:12,630] Trial 48 finished with value: 0.1746122584927689 and parameters: {'n_layers': 1, 'n_neurons': 20, 'n_steps': 5, 'dropout_threshold': 0.2538654962810635}. Best is trial 46 with value: 0.05197422617205154.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(176, 5, 13) (176,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id   prediction\n",
      "0       20001   126.805107\n",
      "1       20002   165.991486\n",
      "2       20003   125.351448\n",
      "3       20004   284.376862\n",
      "4       20005  1153.255859\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08695652173913043, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(10.456521739130435, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(2.857142857142857, 20.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.43478260869565216, 20.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 20)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.012173913043478262, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "Training model with 2 layers, 446 neurons, 18 steps.\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 74ms/step - loss: 525330496.0000 - val_loss: 1079200000.0000\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 626008064.0000 - val_loss: 587550464.0000\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 1159751168.0000 - val_loss: 674884352.0000\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 938077440.0000 - val_loss: 2288906752.0000\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 798152896.0000 - val_loss: 276722528.0000\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 767518592.0000 - val_loss: 555931456.0000\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 1577594624.0000 - val_loss: 3465897728.0000\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 1412616448.0000 - val_loss: 1630281984.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 1202908160.0000 - val_loss: 1680188032.0000\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 1206548224.0000 - val_loss: 1170783872.0000\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 1036730368.0000 - val_loss: 399208224.0000\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 1456514816.0000 - val_loss: 1499431168.0000\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 2689562368.0000 - val_loss: 717543936.0000\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 1654714240.0000 - val_loss: 93540104.0000\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 1286484992.0000 - val_loss: 261542176.0000\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 1090529280.0000 - val_loss: 5912769536.0000\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 2454023424.0000 - val_loss: 4080318464.0000\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 4163930624.0000 - val_loss: 2321079296.0000\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 2552579328.0000 - val_loss: 3767846144.0000\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 2142438912.0000 - val_loss: 680023232.0000\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 533588192.0000 - val_loss: 172998288.0000\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 430647296.0000 - val_loss: 1509685504.0000\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 1147325184.0000 - val_loss: 1673747200.0000\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 690182528.0000 - val_loss: 179708704.0000\n",
      "Predicting 201904 for product 20001 (1/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01807e+05 2.00010e+04 1.90000e+01 2.00000e+01 2.10000e+01\n",
      "   1.00000e+00 4.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01808e+05 2.00010e+04 2.00000e+01 2.10000e+01 2.20000e+01\n",
      "   1.00000e+00 4.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00010e+04 2.10000e+01 2.20000e+01 2.30000e+01\n",
      "   1.00000e+00 4.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00010e+04 2.20000e+01 2.30000e+01 2.40000e+01\n",
      "   1.00000e+00 4.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00010e+04 2.30000e+01 2.40000e+01 2.50000e+01\n",
      "   1.00000e+00 5.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00010e+04 2.40000e+01 2.50000e+01 2.60000e+01\n",
      "   1.00000e+00 5.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00010e+04 2.50000e+01 2.60000e+01 2.70000e+01\n",
      "   1.00000e+00 5.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00010e+04 2.60000e+01 2.70000e+01 2.80000e+01\n",
      "   1.00000e+00 5.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00010e+04 2.70000e+01 2.80000e+01 2.90000e+01\n",
      "   1.00000e+00 5.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00010e+04 2.80000e+01 2.90000e+01 3.00000e+01\n",
      "   1.00000e+00 6.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00010e+04 2.90000e+01 3.00000e+01 3.10000e+01\n",
      "   1.00000e+00 6.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00010e+04 3.00000e+01 3.10000e+01 3.20000e+01\n",
      "   1.00000e+00 6.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00010e+04 3.10000e+01 3.20000e+01 3.30000e+01\n",
      "   1.00000e+00 6.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00010e+04 3.20000e+01 3.30000e+01 3.40000e+01\n",
      "   1.00000e+00 6.80000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00010e+04 3.30000e+01 3.40000e+01 3.50000e+01\n",
      "   1.00000e+00 7.00000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00010e+04 3.40000e+01 3.50000e+01 3.60000e+01\n",
      "   1.00000e+00 7.20000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00010e+04 3.50000e+01 3.60000e+01 3.70000e+01\n",
      "   1.00000e+00 7.40000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00010e+04 3.60000e+01 3.70000e+01 3.80000e+01\n",
      "   1.00000e+00 7.60000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20002 (2/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01807e+05 2.00020e+04 2.05000e+01 2.15000e+01 2.25000e+01\n",
      "   0.00000e+00 6.75000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01808e+05 2.00020e+04 2.15000e+01 2.25000e+01 2.35000e+01\n",
      "   0.00000e+00 7.05000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00020e+04 2.25000e+01 2.35000e+01 2.45000e+01\n",
      "   0.00000e+00 7.35000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00020e+04 2.35000e+01 2.45000e+01 2.55000e+01\n",
      "   0.00000e+00 7.65000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00020e+04 2.45000e+01 2.55000e+01 2.65000e+01\n",
      "   0.00000e+00 7.95000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00020e+04 2.55000e+01 2.65000e+01 2.75000e+01\n",
      "   0.00000e+00 8.25000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00020e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   0.00000e+00 8.55000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00020e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   0.00000e+00 8.85000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00020e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   0.00000e+00 9.15000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00020e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   0.00000e+00 9.45000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00020e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   0.00000e+00 9.75000e+01 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00020e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   0.00000e+00 1.00500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00020e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   0.00000e+00 1.03500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00020e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   0.00000e+00 1.06500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00020e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   0.00000e+00 1.09500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00020e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   0.00000e+00 1.12500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00020e+04 3.65000e+01 3.75000e+01 3.85000e+01\n",
      "   0.00000e+00 1.15500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00020e+04 3.75000e+01 3.85000e+01 3.95000e+01\n",
      "   0.00000e+00 1.18500e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "   1.00000e+00 0.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20003 (3/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01807e+05 2.00030e+04 1.85000e+01 1.95000e+01 2.05000e+01\n",
      "   1.00000e+00 4.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01808e+05 2.00030e+04 1.95000e+01 2.05000e+01 2.15000e+01\n",
      "   1.00000e+00 4.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00030e+04 2.05000e+01 2.15000e+01 2.25000e+01\n",
      "   1.00000e+00 4.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00030e+04 2.15000e+01 2.25000e+01 2.35000e+01\n",
      "   1.00000e+00 4.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00030e+04 2.25000e+01 2.35000e+01 2.45000e+01\n",
      "   1.00000e+00 4.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00030e+04 2.35000e+01 2.45000e+01 2.55000e+01\n",
      "   1.00000e+00 5.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00030e+04 2.45000e+01 2.55000e+01 2.65000e+01\n",
      "   1.00000e+00 5.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00030e+04 2.55000e+01 2.65000e+01 2.75000e+01\n",
      "   1.00000e+00 5.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00030e+04 2.65000e+01 2.75000e+01 2.85000e+01\n",
      "   1.00000e+00 5.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00030e+04 2.75000e+01 2.85000e+01 2.95000e+01\n",
      "   1.00000e+00 5.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00030e+04 2.85000e+01 2.95000e+01 3.05000e+01\n",
      "   1.00000e+00 6.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00030e+04 2.95000e+01 3.05000e+01 3.15000e+01\n",
      "   1.00000e+00 6.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00030e+04 3.05000e+01 3.15000e+01 3.25000e+01\n",
      "   1.00000e+00 6.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00030e+04 3.15000e+01 3.25000e+01 3.35000e+01\n",
      "   1.00000e+00 6.70000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00030e+04 3.25000e+01 3.35000e+01 3.45000e+01\n",
      "   1.00000e+00 6.90000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00030e+04 3.35000e+01 3.45000e+01 3.55000e+01\n",
      "   1.00000e+00 7.10000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00030e+04 3.45000e+01 3.55000e+01 3.65000e+01\n",
      "   1.00000e+00 7.30000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00030e+04 3.55000e+01 3.65000e+01 3.75000e+01\n",
      "   1.00000e+00 7.50000e+01 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "Predicting 201904 for product 20004 (4/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01807e+05 2.00040e+04 1.90000e+02 2.00000e+02 2.10000e+02\n",
      "   0.00000e+00 1.05000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01808e+05 2.00040e+04 2.00000e+02 2.10000e+02 2.20000e+02\n",
      "   0.00000e+00 1.10000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01809e+05 2.00040e+04 2.10000e+02 2.20000e+02 2.30000e+02\n",
      "   0.00000e+00 1.15000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01810e+05 2.00040e+04 2.20000e+02 2.30000e+02 2.40000e+02\n",
      "   0.00000e+00 1.20000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01811e+05 2.00040e+04 2.30000e+02 2.40000e+02 2.50000e+02\n",
      "   0.00000e+00 1.25000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01812e+05 2.00040e+04 2.40000e+02 2.50000e+02 2.60000e+02\n",
      "   0.00000e+00 1.30000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01901e+05 2.00040e+04 2.50000e+02 2.60000e+02 2.70000e+02\n",
      "   0.00000e+00 1.35000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01902e+05 2.00040e+04 2.60000e+02 2.70000e+02 2.80000e+02\n",
      "   0.00000e+00 1.40000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01903e+05 2.00040e+04 2.70000e+02 2.80000e+02 2.90000e+02\n",
      "   0.00000e+00 1.45000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01904e+05 2.00040e+04 2.80000e+02 2.90000e+02 3.00000e+02\n",
      "   0.00000e+00 1.50000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01905e+05 2.00040e+04 2.90000e+02 3.00000e+02 3.10000e+02\n",
      "   0.00000e+00 1.55000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01906e+05 2.00040e+04 3.00000e+02 3.10000e+02 3.20000e+02\n",
      "   0.00000e+00 1.60000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01907e+05 2.00040e+04 3.10000e+02 3.20000e+02 3.30000e+02\n",
      "   0.00000e+00 1.65000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01908e+05 2.00040e+04 3.20000e+02 3.30000e+02 3.40000e+02\n",
      "   0.00000e+00 1.70000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01909e+05 2.00040e+04 3.30000e+02 3.40000e+02 3.50000e+02\n",
      "   0.00000e+00 1.75000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01910e+05 2.00040e+04 3.40000e+02 3.50000e+02 3.60000e+02\n",
      "   0.00000e+00 1.80000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01911e+05 2.00040e+04 3.50000e+02 3.60000e+02 3.70000e+02\n",
      "   0.00000e+00 1.85000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]\n",
      "  [2.01912e+05 2.00040e+04 3.60000e+02 3.70000e+02 3.80000e+02\n",
      "   0.00000e+00 1.90000e+02 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "   0.00000e+00 1.00000e+00 0.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 19:51:18,545] Trial 49 finished with value: 1.0 and parameters: {'n_layers': 2, 'n_neurons': 446, 'n_steps': 18, 'dropout_threshold': 0.1797248303070684}. Best is trial 46 with value: 0.05197422617205154.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 20005 (5/5))\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "[[[2.01807e+05 2.00050e+04 5.90000e+01 6.40000e+01 6.90000e+01\n",
      "   1.00000e+00 6.90000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01808e+05 2.00050e+04 6.20000e+01 6.70000e+01 7.20000e+01\n",
      "   1.00000e+00 7.20000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01809e+05 2.00050e+04 6.50000e+01 7.00000e+01 7.50000e+01\n",
      "   1.00000e+00 7.50000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01810e+05 2.00050e+04 6.80000e+01 7.30000e+01 7.80000e+01\n",
      "   1.00000e+00 7.80000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01811e+05 2.00050e+04 7.10000e+01 7.60000e+01 8.10000e+01\n",
      "   1.00000e+00 8.10000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01812e+05 2.00050e+04 7.40000e+01 7.90000e+01 8.40000e+01\n",
      "   1.00000e+00 8.40000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01901e+05 2.00050e+04 7.70000e+01 8.20000e+01 8.70000e+01\n",
      "   1.00000e+00 8.70000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01902e+05 2.00050e+04 8.00000e+01 8.50000e+01 9.00000e+01\n",
      "   1.00000e+00 9.00000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01903e+05 2.00050e+04 8.30000e+01 8.80000e+01 9.30000e+01\n",
      "   1.00000e+00 9.30000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01904e+05 2.00050e+04 8.60000e+01 9.10000e+01 9.60000e+01\n",
      "   1.00000e+00 9.60000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01905e+05 2.00050e+04 8.90000e+01 9.40000e+01 9.90000e+01\n",
      "   1.00000e+00 9.90000e+02 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01906e+05 2.00050e+04 9.20000e+01 9.70000e+01 1.02000e+02\n",
      "   1.00000e+00 1.02000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01907e+05 2.00050e+04 9.50000e+01 1.00000e+02 1.05000e+02\n",
      "   1.00000e+00 1.05000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01908e+05 2.00050e+04 9.80000e+01 1.03000e+02 1.08000e+02\n",
      "   1.00000e+00 1.08000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01909e+05 2.00050e+04 1.01000e+02 1.06000e+02 1.11000e+02\n",
      "   1.00000e+00 1.11000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01910e+05 2.00050e+04 1.04000e+02 1.09000e+02 1.14000e+02\n",
      "   1.00000e+00 1.14000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01911e+05 2.00050e+04 1.07000e+02 1.12000e+02 1.17000e+02\n",
      "   1.00000e+00 1.17000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]\n",
      "  [2.01912e+05 2.00050e+04 1.10000e+02 1.15000e+02 1.20000e+02\n",
      "   1.00000e+00 1.20000e+03 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "   0.00000e+00 0.00000e+00 1.00000e+00]]]\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "/////////// INPUT ///////////////////////////\n",
      "{}\n",
      "()\n",
      "//////////////////////////////////////\n",
      "(163, 18, 13) (163,) (5, 2)\n",
      "//////////////////////////////////////\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "   product_id  prediction\n",
      "0       20001         0.0\n",
      "1       20002         0.0\n",
      "2       20003         0.0\n",
      "3       20004         0.0\n",
      "4       20005         0.0\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "******************************************************************************************\n",
      "Best trial:\n",
      "n_layers: 1\n",
      "n_neurons: 333\n",
      "n_steps: 2\n",
      "dropout_threshold: 0.19656820478562106\n"
     ]
    }
   ],
   "source": [
    "# Create a study object and specify the optimization direction\n",
    "study = optuna.create_study(direction='minimize', study_name=\"lstm_8_BO_test\")\n",
    "\n",
    "# Perform optimization\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Extract the best trial\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print('Best trial:')\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
