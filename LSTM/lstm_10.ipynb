{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 21:52:20.703192: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-25 21:52:20.908271: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-25 21:52:21.529807: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usandor Tensorflow version 2.13.1\n",
      "Usando GPU: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 21:52:22.356721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 21:52:22.458837: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 21:52:22.459107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 21:52:22.951374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 21:52:22.951483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 21:52:22.951540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 21:52:22.951809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /device:GPU:0 with 3248 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2023-11-25 21:52:22.952468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 21:52:22.952545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 21:52:22.952604: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 21:52:22.952668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 21:52:22.952723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 21:52:22.952767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /device:GPU:0 with 3248 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# set tensorflow to be less verbose\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "print(\"Usandor Tensorflow version \" + tf.__version__)\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "  print('Usando GPU: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "  print(\"Usando CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "import pandas as pd\n",
    "from empresa4.datasets import get_dataset, nombres_datasets\n",
    "from keras.callbacks import EarlyStopping\n",
    "from datetime import datetime\n",
    "from empresa4.core import calculate_error_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>product_id</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>product_category</th>\n",
       "      <th>cat2</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "      <td>479.0</td>\n",
       "      <td>937.72717</td>\n",
       "      <td>934.77222</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201701</td>\n",
       "      <td>20002</td>\n",
       "      <td>391.0</td>\n",
       "      <td>555.18654</td>\n",
       "      <td>550.15707</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201701</td>\n",
       "      <td>20003</td>\n",
       "      <td>438.0</td>\n",
       "      <td>1067.81543</td>\n",
       "      <td>1063.45835</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>475.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201701</td>\n",
       "      <td>20004</td>\n",
       "      <td>339.0</td>\n",
       "      <td>569.37394</td>\n",
       "      <td>555.91614</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201701</td>\n",
       "      <td>20005</td>\n",
       "      <td>249.0</td>\n",
       "      <td>494.60084</td>\n",
       "      <td>494.27011</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33691</th>\n",
       "      <td>201902</td>\n",
       "      <td>21235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>PC</td>\n",
       "      <td>PIEL1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33692</th>\n",
       "      <td>201902</td>\n",
       "      <td>21236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>PC</td>\n",
       "      <td>PIEL1</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33693</th>\n",
       "      <td>201902</td>\n",
       "      <td>21115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>PC</td>\n",
       "      <td>DEOS</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33694</th>\n",
       "      <td>201902</td>\n",
       "      <td>20734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>PC</td>\n",
       "      <td>CABELLO</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33695</th>\n",
       "      <td>201902</td>\n",
       "      <td>21243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>PC</td>\n",
       "      <td>DEOS</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33696 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       periodo  product_id  cust_request_qty  cust_request_tn          tn  \\\n",
       "0       201701       20001             479.0        937.72717   934.77222   \n",
       "1       201701       20002             391.0        555.18654   550.15707   \n",
       "2       201701       20003             438.0       1067.81543  1063.45835   \n",
       "3       201701       20004             339.0        569.37394   555.91614   \n",
       "4       201701       20005             249.0        494.60084   494.27011   \n",
       "...        ...         ...               ...              ...         ...   \n",
       "33691   201902       21235               0.0          0.00000     0.00000   \n",
       "33692   201902       21236               0.0          0.00000     0.00000   \n",
       "33693   201902       21115               0.0          0.00000     0.00000   \n",
       "33694   201902       20734               0.0          0.00000     0.00000   \n",
       "33695   201902       21243               0.0          0.00000     0.00000   \n",
       "\n",
       "      product_category         cat2  sku_size  plan_precios_cuidados  \n",
       "0                   HC  ROPA LAVADO    3000.0                      0  \n",
       "1                   HC  ROPA LAVADO    3000.0                      0  \n",
       "2                FOODS     ADEREZOS     475.0                      0  \n",
       "3                FOODS     ADEREZOS     240.0                      0  \n",
       "4                FOODS     ADEREZOS     120.0                      0  \n",
       "...                ...          ...       ...                    ...  \n",
       "33691               PC        PIEL1     200.0                      0  \n",
       "33692               PC        PIEL1     400.0                      0  \n",
       "33693               PC         DEOS      89.0                      0  \n",
       "33694               PC      CABELLO     400.0                      0  \n",
       "33695               PC         DEOS      70.0                      0  \n",
       "\n",
       "[33696 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "df = get_dataset(\"02_productos_todos_anti_leak\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>201701_norm</th>\n",
       "      <th>201702_norm</th>\n",
       "      <th>201703_norm</th>\n",
       "      <th>201704_norm</th>\n",
       "      <th>201705_norm</th>\n",
       "      <th>201706_norm</th>\n",
       "      <th>201707_norm</th>\n",
       "      <th>201708_norm</th>\n",
       "      <th>201709_norm</th>\n",
       "      <th>...</th>\n",
       "      <th>201806_norm</th>\n",
       "      <th>201807_norm</th>\n",
       "      <th>201808_norm</th>\n",
       "      <th>201809_norm</th>\n",
       "      <th>201810_norm</th>\n",
       "      <th>201811_norm</th>\n",
       "      <th>201812_norm</th>\n",
       "      <th>201901_norm</th>\n",
       "      <th>201902_norm</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001</td>\n",
       "      <td>0.091342</td>\n",
       "      <td>0.347663</td>\n",
       "      <td>0.567846</td>\n",
       "      <td>0.466153</td>\n",
       "      <td>0.654484</td>\n",
       "      <td>0.662267</td>\n",
       "      <td>0.449035</td>\n",
       "      <td>0.552176</td>\n",
       "      <td>0.573766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501371</td>\n",
       "      <td>0.640632</td>\n",
       "      <td>0.784656</td>\n",
       "      <td>0.626804</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.789908</td>\n",
       "      <td>0.647724</td>\n",
       "      <td>0.555827</td>\n",
       "      <td>0.548559</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20002</td>\n",
       "      <td>0.035110</td>\n",
       "      <td>0.286313</td>\n",
       "      <td>0.472443</td>\n",
       "      <td>0.295634</td>\n",
       "      <td>0.477368</td>\n",
       "      <td>0.547960</td>\n",
       "      <td>0.478475</td>\n",
       "      <td>0.350738</td>\n",
       "      <td>0.602969</td>\n",
       "      <td>...</td>\n",
       "      <td>0.585130</td>\n",
       "      <td>0.553193</td>\n",
       "      <td>0.657610</td>\n",
       "      <td>0.540080</td>\n",
       "      <td>0.780210</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571334</td>\n",
       "      <td>0.716985</td>\n",
       "      <td>0.590329</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20003</td>\n",
       "      <td>0.375239</td>\n",
       "      <td>0.383889</td>\n",
       "      <td>0.468175</td>\n",
       "      <td>0.268330</td>\n",
       "      <td>0.316705</td>\n",
       "      <td>0.379824</td>\n",
       "      <td>0.400778</td>\n",
       "      <td>0.441167</td>\n",
       "      <td>0.748375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337259</td>\n",
       "      <td>0.334958</td>\n",
       "      <td>0.488019</td>\n",
       "      <td>0.465740</td>\n",
       "      <td>0.670507</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.392968</td>\n",
       "      <td>0.492512</td>\n",
       "      <td>0.387094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20004</td>\n",
       "      <td>0.164645</td>\n",
       "      <td>0.400644</td>\n",
       "      <td>0.386223</td>\n",
       "      <td>0.403683</td>\n",
       "      <td>0.428377</td>\n",
       "      <td>0.465553</td>\n",
       "      <td>0.449286</td>\n",
       "      <td>0.822024</td>\n",
       "      <td>0.993244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353047</td>\n",
       "      <td>0.515169</td>\n",
       "      <td>0.738344</td>\n",
       "      <td>0.748157</td>\n",
       "      <td>0.638388</td>\n",
       "      <td>0.632612</td>\n",
       "      <td>0.461655</td>\n",
       "      <td>0.403118</td>\n",
       "      <td>0.348204</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20005</td>\n",
       "      <td>0.179476</td>\n",
       "      <td>0.441811</td>\n",
       "      <td>0.451804</td>\n",
       "      <td>0.530902</td>\n",
       "      <td>0.413083</td>\n",
       "      <td>0.423504</td>\n",
       "      <td>0.501455</td>\n",
       "      <td>0.855840</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438761</td>\n",
       "      <td>0.402467</td>\n",
       "      <td>0.701053</td>\n",
       "      <td>0.610395</td>\n",
       "      <td>0.716162</td>\n",
       "      <td>0.375956</td>\n",
       "      <td>0.298511</td>\n",
       "      <td>0.291258</td>\n",
       "      <td>0.328378</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>21295</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>21296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>21297</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>21298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>21299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1296 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      product_id  201701_norm  201702_norm  201703_norm  201704_norm  \\\n",
       "0          20001     0.091342     0.347663     0.567846     0.466153   \n",
       "1          20002     0.035110     0.286313     0.472443     0.295634   \n",
       "2          20003     0.375239     0.383889     0.468175     0.268330   \n",
       "3          20004     0.164645     0.400644     0.386223     0.403683   \n",
       "4          20005     0.179476     0.441811     0.451804     0.530902   \n",
       "...          ...          ...          ...          ...          ...   \n",
       "1291       21295     1.000000     0.000000     0.000000     0.000000   \n",
       "1292       21296     0.000000     0.000000     0.000000     0.000000   \n",
       "1293       21297     1.000000     0.000000     0.000000     0.000000   \n",
       "1294       21298     0.000000     0.000000     0.000000     0.000000   \n",
       "1295       21299     0.000000     0.000000     0.000000     0.000000   \n",
       "\n",
       "      201705_norm  201706_norm  201707_norm  201708_norm  201709_norm  ...  \\\n",
       "0        0.654484     0.662267     0.449035     0.552176     0.573766  ...   \n",
       "1        0.477368     0.547960     0.478475     0.350738     0.602969  ...   \n",
       "2        0.316705     0.379824     0.400778     0.441167     0.748375  ...   \n",
       "3        0.428377     0.465553     0.449286     0.822024     0.993244  ...   \n",
       "4        0.413083     0.423504     0.501455     0.855840     1.000000  ...   \n",
       "...           ...          ...          ...          ...          ...  ...   \n",
       "1291     0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "1292     0.000000     0.000000     0.000000     1.000000     0.000000  ...   \n",
       "1293     0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "1294     0.000000     0.000000     0.000000     1.000000     0.000000  ...   \n",
       "1295     0.000000     0.000000     0.000000     1.000000     0.000000  ...   \n",
       "\n",
       "      201806_norm  201807_norm  201808_norm  201809_norm  201810_norm  \\\n",
       "0        0.501371     0.640632     0.784656     0.626804     1.000000   \n",
       "1        0.585130     0.553193     0.657610     0.540080     0.780210   \n",
       "2        0.337259     0.334958     0.488019     0.465740     0.670507   \n",
       "3        0.353047     0.515169     0.738344     0.748157     0.638388   \n",
       "4        0.438761     0.402467     0.701053     0.610395     0.716162   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1291     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "1292     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "1293     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "1294     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "1295     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "\n",
       "      201811_norm  201812_norm  201901_norm  201902_norm  cluster  \n",
       "0        0.789908     0.647724     0.555827     0.548559        0  \n",
       "1        1.000000     0.571334     0.716985     0.590329        0  \n",
       "2        0.616162     0.392968     0.492512     0.387094        0  \n",
       "3        0.632612     0.461655     0.403118     0.348204        0  \n",
       "4        0.375956     0.298511     0.291258     0.328378        0  \n",
       "...           ...          ...          ...          ...      ...  \n",
       "1291     0.000000     0.000000     0.000000     0.000000        3  \n",
       "1292     0.000000     0.000000     0.000000     0.000000        1  \n",
       "1293     0.000000     0.000000     0.000000     0.000000        3  \n",
       "1294     0.000000     0.000000     0.000000     0.000000        1  \n",
       "1295     0.000000     0.000000     0.000000     0.000000        1  \n",
       "\n",
       "[1296 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_df = pd.read_csv(\"product_ids_clusters.csv\")\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add column cluster from cluster_df to df\n",
    "df = df.merge(cluster_df[[\"product_id\", \"cluster\"]], on='product_id', how='left')\n",
    "df[\"cluster\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "equiv = {\n",
    "    201701: 201703,\n",
    "    201702: 201704,\n",
    "    201703: 201705,\n",
    "    201704: 201706,\n",
    "    201705: 201707,\n",
    "    201706: 201708,\n",
    "    201707: 201709,\n",
    "    201708: 201710,\n",
    "    201709: 201711,\n",
    "    201710: 201712,\n",
    "    201711: 201801,\n",
    "    201712: 201802,\n",
    "    201801: 201803,\n",
    "    201802: 201804,\n",
    "    201803: 201805,\n",
    "    201804: 201806,\n",
    "    201805: 201807,\n",
    "    201806: 201808,\n",
    "    201807: 201809,\n",
    "    201808: 201810,\n",
    "    201809: 201811,\n",
    "    201810: 201812,\n",
    "    201811: 201901,\n",
    "    201812: 201902,\n",
    "    201901: 201903,\n",
    "    201902: 201904,\n",
    "    201903: 201905,\n",
    "    201904: 201906,\n",
    "    201905: 201907,\n",
    "    201906: 201908,\n",
    "    201907: 201909,\n",
    "    201908: 201910,\n",
    "    201909: 201911,\n",
    "    201910: 201912,\n",
    "    201911: 202001,\n",
    "    201912: 202002,\n",
    "    202001: 202003,\n",
    "    202002: 202004,\n",
    "}\n",
    "target_df = get_dataset(\"02_productos_todos\")\n",
    "\n",
    "\n",
    "# Filter data up to 201902\n",
    "def lag_target_class(row):\n",
    "    # from the column \"periodo\" and \"product_id\" of this row, get the equivalen periodo in equiv and get the tn column from \"target\" df for this product_id and the equiv periodo\n",
    "    product_id = row[\"product_id\"]\n",
    "    periodo = row[\"periodo\"]\n",
    "    periodo_equiv = equiv.get(periodo)\n",
    "    if periodo_equiv is None:\n",
    "        return None\n",
    "    value = target_df[(target_df[\"product_id\"] == product_id) & (target_df[\"periodo\"] == periodo_equiv)][\"tn\"].values[0]\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>product_id</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>product_category</th>\n",
       "      <th>cat2</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>cluster</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "      <td>479.0</td>\n",
       "      <td>937.72717</td>\n",
       "      <td>934.77222</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1303.35771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201701</td>\n",
       "      <td>20002</td>\n",
       "      <td>391.0</td>\n",
       "      <td>555.18654</td>\n",
       "      <td>550.15707</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>834.73521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201701</td>\n",
       "      <td>20003</td>\n",
       "      <td>438.0</td>\n",
       "      <td>1067.81543</td>\n",
       "      <td>1063.45835</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>475.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>917.16548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201701</td>\n",
       "      <td>20004</td>\n",
       "      <td>339.0</td>\n",
       "      <td>569.37394</td>\n",
       "      <td>555.91614</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>489.91328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201701</td>\n",
       "      <td>20005</td>\n",
       "      <td>249.0</td>\n",
       "      <td>494.60084</td>\n",
       "      <td>494.27011</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>ADEREZOS</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>563.89955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33691</th>\n",
       "      <td>201902</td>\n",
       "      <td>21235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>PC</td>\n",
       "      <td>PIEL1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33692</th>\n",
       "      <td>201902</td>\n",
       "      <td>21236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>PC</td>\n",
       "      <td>PIEL1</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33693</th>\n",
       "      <td>201902</td>\n",
       "      <td>21115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>PC</td>\n",
       "      <td>DEOS</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33694</th>\n",
       "      <td>201902</td>\n",
       "      <td>20734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>PC</td>\n",
       "      <td>CABELLO</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33695</th>\n",
       "      <td>201902</td>\n",
       "      <td>21243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>PC</td>\n",
       "      <td>DEOS</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33696 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       periodo  product_id  cust_request_qty  cust_request_tn          tn  \\\n",
       "0       201701       20001             479.0        937.72717   934.77222   \n",
       "1       201701       20002             391.0        555.18654   550.15707   \n",
       "2       201701       20003             438.0       1067.81543  1063.45835   \n",
       "3       201701       20004             339.0        569.37394   555.91614   \n",
       "4       201701       20005             249.0        494.60084   494.27011   \n",
       "...        ...         ...               ...              ...         ...   \n",
       "33691   201902       21235               0.0          0.00000     0.00000   \n",
       "33692   201902       21236               0.0          0.00000     0.00000   \n",
       "33693   201902       21115               0.0          0.00000     0.00000   \n",
       "33694   201902       20734               0.0          0.00000     0.00000   \n",
       "33695   201902       21243               0.0          0.00000     0.00000   \n",
       "\n",
       "      product_category         cat2  sku_size  plan_precios_cuidados  cluster  \\\n",
       "0                   HC  ROPA LAVADO    3000.0                      0        0   \n",
       "1                   HC  ROPA LAVADO    3000.0                      0        0   \n",
       "2                FOODS     ADEREZOS     475.0                      0        0   \n",
       "3                FOODS     ADEREZOS     240.0                      0        0   \n",
       "4                FOODS     ADEREZOS     120.0                      0        0   \n",
       "...                ...          ...       ...                    ...      ...   \n",
       "33691               PC        PIEL1     200.0                      0        1   \n",
       "33692               PC        PIEL1     400.0                      0        1   \n",
       "33693               PC         DEOS      89.0                      0        1   \n",
       "33694               PC      CABELLO     400.0                      0        1   \n",
       "33695               PC         DEOS      70.0                      0        1   \n",
       "\n",
       "           target  \n",
       "0      1303.35771  \n",
       "1       834.73521  \n",
       "2       917.16548  \n",
       "3       489.91328  \n",
       "4       563.89955  \n",
       "...           ...  \n",
       "33691     0.00000  \n",
       "33692     0.00000  \n",
       "33693     0.00000  \n",
       "33694     0.00000  \n",
       "33695     0.00000  \n",
       "\n",
       "[33696 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"target\"] = df.apply(lag_target_class, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by `periodo` just to be sure\n",
    "product_data = df.sort_values([\"product_id\", \"periodo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "# Using scikit-learn, ONE HOT ENCODE the categorical variables: product_category\n",
    "product_data = pd.get_dummies(product_data, columns=[\"product_category\", \"cat2\"])\n",
    "\n",
    "# sort columns so that 'lag_tn' is the last column in the dataframe\n",
    "product_data = product_data[[col for col in product_data.columns if col != 'target'] + ['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = product_data.columns\n",
    "product_category_columns = [col for col in columns if col.startswith(\"product_category\")]\n",
    "\n",
    "# convert boolean columns to int\n",
    "product_data = product_data.astype({col: 'int32' for col in product_category_columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cat2_CABELLO                  int32\n",
       "cat2_OTROS                    int32\n",
       "cat2_HOGAR                    int32\n",
       "cat2_DEOS                     int32\n",
       "cat2_DENTAL                   int32\n",
       "cat2_unknown                  int32\n",
       "cat2_ADEREZOS                 int32\n",
       "product_category_unknown      int32\n",
       "product_category_REF          int32\n",
       "cat2_PIEL1                    int32\n",
       "product_category_PC           int32\n",
       "product_category_FOODS        int32\n",
       "cat2_PROFESIONAL              int32\n",
       "cat2_ROPA ACONDICIONADOR      int32\n",
       "cat2_ROPA LAVADO              int32\n",
       "cat2_ROPA MANCHAS             int32\n",
       "cat2_SOPAS Y CALDOS           int32\n",
       "cat2_TE                       int32\n",
       "cat2_VAJILLA                  int32\n",
       "product_category_HC           int32\n",
       "cat2_PIEL2                    int32\n",
       "periodo                       int64\n",
       "plan_precios_cuidados         int64\n",
       "product_id                    int64\n",
       "cluster                       int64\n",
       "sku_size                    float64\n",
       "tn                          float64\n",
       "cust_request_tn             float64\n",
       "cust_request_qty            float64\n",
       "target                      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert bool columns to int\n",
    "cat2_columns = [col for col in columns if col.startswith(\"cat2\")]\n",
    "\n",
    "product_data = product_data.astype({col: 'int32' for col in cat2_columns})\n",
    "product_data.dtypes.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "periodo                     0\n",
       "product_id                  0\n",
       "cust_request_qty            0\n",
       "cust_request_tn             0\n",
       "tn                          0\n",
       "sku_size                    0\n",
       "plan_precios_cuidados       0\n",
       "cluster                     0\n",
       "product_category_FOODS      0\n",
       "product_category_HC         0\n",
       "product_category_PC         0\n",
       "product_category_REF        0\n",
       "product_category_unknown    0\n",
       "cat2_ADEREZOS               0\n",
       "cat2_CABELLO                0\n",
       "cat2_DENTAL                 0\n",
       "cat2_DEOS                   0\n",
       "cat2_HOGAR                  0\n",
       "cat2_OTROS                  0\n",
       "cat2_PIEL1                  0\n",
       "cat2_PIEL2                  0\n",
       "cat2_PROFESIONAL            0\n",
       "cat2_ROPA ACONDICIONADOR    0\n",
       "cat2_ROPA LAVADO            0\n",
       "cat2_ROPA MANCHAS           0\n",
       "cat2_SOPAS Y CALDOS         0\n",
       "cat2_TE                     0\n",
       "cat2_VAJILLA                0\n",
       "cat2_unknown                0\n",
       "target                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['periodo', 'product_id', 'cust_request_qty', 'cust_request_tn', 'tn',\n",
       "       'sku_size', 'plan_precios_cuidados', 'cluster',\n",
       "       'product_category_FOODS', 'product_category_HC', 'product_category_PC',\n",
       "       'product_category_REF', 'product_category_unknown', 'cat2_ADEREZOS',\n",
       "       'cat2_CABELLO', 'cat2_DENTAL', 'cat2_DEOS', 'cat2_HOGAR', 'cat2_OTROS',\n",
       "       'cat2_PIEL1', 'cat2_PIEL2', 'cat2_PROFESIONAL',\n",
       "       'cat2_ROPA ACONDICIONADOR', 'cat2_ROPA LAVADO', 'cat2_ROPA MANCHAS',\n",
       "       'cat2_SOPAS Y CALDOS', 'cat2_TE', 'cat2_VAJILLA', 'cat2_unknown',\n",
       "       'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "periodo\n",
       "201701    1296\n",
       "201702    1296\n",
       "201901    1296\n",
       "201812    1296\n",
       "201811    1296\n",
       "201810    1296\n",
       "201809    1296\n",
       "201808    1296\n",
       "201807    1296\n",
       "201806    1296\n",
       "201805    1296\n",
       "201804    1296\n",
       "201803    1296\n",
       "201802    1296\n",
       "201801    1296\n",
       "201712    1296\n",
       "201711    1296\n",
       "201710    1296\n",
       "201709    1296\n",
       "201708    1296\n",
       "201707    1296\n",
       "201706    1296\n",
       "201705    1296\n",
       "201704    1296\n",
       "201703    1296\n",
       "201902    1296\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_data[\"periodo\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "train_df = product_data[product_data[\"periodo\"] <= 201811].copy()\n",
    "validation_df = product_data[product_data[\"periodo\"] == 201812].copy()\n",
    "test_df = product_data[product_data[\"periodo\"] == 201902].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29808, 30), (1296, 30), (1296, 30))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, validation_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': 'mergesort', 'order': None}\n",
      "()\n",
      "{'axis': -1, 'mode': 'raise', 'out': None}\n",
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),)\n"
     ]
    }
   ],
   "source": [
    "# standarize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_train = StandardScaler()\n",
    "scaler_validation = StandardScaler()\n",
    "scaler_test_product_id = StandardScaler()\n",
    "scaler_test_target = StandardScaler()\n",
    "scaler_test_rest = StandardScaler()\n",
    "\n",
    "train_df[train_df.columns] = scaler_train.fit_transform(train_df[train_df.columns])\n",
    "validation_df[validation_df.columns] = scaler_validation.fit_transform(validation_df[validation_df.columns])\n",
    "\n",
    "rest = [col for col in test_df.columns if col not in [\"product_id\", \"target\"]]\n",
    "\n",
    "test_df[rest] = scaler_test_rest.fit_transform(test_df[rest])\n",
    "test_df[\"target\"] = scaler_test_target.fit_transform(test_df[[\"target\"]])\n",
    "test_df[\"product_id\"] = scaler_test_product_id.fit_transform(test_df[[\"product_id\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterations Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_neurons(n_layer, n_neurons, embudo_1, embudo_2, embudo_3):\n",
    "    if n_layer == 1:\n",
    "        return n_neurons\n",
    "    elif n_layer == 2:\n",
    "        return int(n_neurons // embudo_1)\n",
    "    elif n_layer == 3:\n",
    "        return int(n_neurons // embudo_2)\n",
    "    elif n_layer == 4:\n",
    "        return int(n_neurons // embudo_3)\n",
    "    else:\n",
    "        return int(n_neurons // embudo_3)\n",
    "\n",
    "def train_model(n_layers, n_neurons, dropout_threshold, embudo_1, embudo_2, embudo_3, X_train, y_train, X_validate, y_validate):\n",
    "    print(f\"Training model with {n_layers} layers, {n_neurons} neurons, {embudo_1} embudo_1, {embudo_2} embudo_2, and {embudo_3} embudo_3.\")\n",
    "\n",
    "    # Reshape the input data to have three dimensions: [samples, time steps, features]\n",
    "    X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_validate = X_validate.values.reshape((X_validate.shape[0], 1, X_validate.shape[1]))\n",
    "\n",
    "    # Number of features (excluding the target variable)\n",
    "    n_features = X_train.shape[2]\n",
    "    print(f\"Número de features: {n_features}\")\n",
    "\n",
    "    # Define the LSTM model structure based on the number of layers\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == n_layers - 1: # Last layer\n",
    "            model.add(LSTM(n_neurons, activation=\"relu\", input_shape=(1, n_features)))\n",
    "        else: # Intermediate layers\n",
    "            final_n_neurons = get_n_neurons(i, n_neurons, embudo_1, embudo_2, embudo_3)\n",
    "            model.add(LSTM(final_n_neurons, activation=\"relu\", return_sequences=True, input_shape=(1, n_features)))\n",
    "            model.add(Dropout(dropout_threshold))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    # Define EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, epochs=200, validation_data=(X_validate, y_validate), callbacks=[early_stopping], verbose=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, X_test, trial_number, cluster = None):\n",
    "    rows = []\n",
    "    # for each product_id, generate a prediction for 201904\n",
    "    for i, product in enumerate(X_test[\"product_id\"].unique()):\n",
    "        print(\n",
    "            f\"Predicting 201904 for product {product} ({i+1}/{len(X_test['product_id'].unique())}))\"\n",
    "        )\n",
    "\n",
    "        this_X_test = X_test[X_test[\"product_id\"] == product]\n",
    "        this_X_test = this_X_test.values.reshape((this_X_test.shape[0], 1, this_X_test.shape[1]))\n",
    "\n",
    "\n",
    "        # Make prediction\n",
    "        yhat = model.predict(this_X_test, verbose=0)\n",
    "        if yhat[0][0] < 1:\n",
    "            yhat[0][0] = 0\n",
    "\n",
    "        # Append to final output DataFrame\n",
    "        rows.append(\n",
    "            {\n",
    "                \"product_id\": product,\n",
    "                \"target\": yhat[0][0],\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    final_output = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\"product_id\", \"target\"],\n",
    "    )\n",
    "    \n",
    "    final_output[\"product_id\"] = scaler_test_product_id.inverse_transform(final_output[[\"product_id\"]])\n",
    "    final_output[\"target\"] = scaler_test_target.inverse_transform(final_output[[\"target\"]])\n",
    "    final_output[\"product_id\"] = final_output[\"product_id\"].astype(int)\n",
    "\n",
    "    final_output = final_output.sort_values(\"product_id\", ascending=True)\n",
    "    timestamp = datetime.now().timestamp()\n",
    "    final_output.to_csv(f\"./output/output_lstm_{version}_BO_{cluster if cluster else 'TODO'}_{trial_number}_{timestamp}.csv\", index=False)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(predictions):\n",
    "    # revert the standarization for the output\n",
    "\n",
    "    \n",
    "    predictions.rename(columns={\"target\": \"prediction\"}, inplace=True)\n",
    "    \n",
    "    return calculate_error_2(predictions, 201904)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gero/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "from optuna import Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: Trial, cluster:int = None):\n",
    "    # get trial number\n",
    "    trial_number = trial.number\n",
    "\n",
    "    # Define the search space using Optuna\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 4)\n",
    "    n_neurons = trial.suggest_int('n_neurons', 20, 500)\n",
    "    embudo_1 = trial.suggest_float('embudo_1', 0.9, 1.5)\n",
    "    embudo_2 = trial.suggest_float('embudo_2', 0.8, 2.5)\n",
    "    embudo_3 = trial.suggest_float('embudo_3', 0.7, 3.5)\n",
    "    dropout_threshold = trial.suggest_float('dropout_threshold', 0.1, 0.66)\n",
    "\n",
    "    if cluster is not None:\n",
    "        train_df_cluster = train_df[train_df[\"cluster\"] == cluster].copy()\n",
    "        validation_df_cluster = validation_df[validation_df[\"cluster\"] == cluster].copy()\n",
    "        test_df_cluster = test_df[test_df[\"cluster\"] == cluster].copy()\n",
    "\n",
    "        X_train, y_train = train_df_cluster.drop([\"target\"], axis=1), train_df_cluster[\"target\"]\n",
    "        X_validate, y_validate = validation_df_cluster.drop([\"target\"], axis=1), validation_df_cluster[\"target\"]\n",
    "        X_test = test_df_cluster.drop([\"target\"], axis=1)\n",
    "    else:\n",
    "        X_train, y_train = train_df.drop([\"target\"], axis=1), train_df[\"target\"]\n",
    "        X_validate, y_validate = validation_df.drop([\"target\"], axis=1), validation_df[\"target\"]\n",
    "        X_test = test_df.drop([\"target\"], axis=1)\n",
    "\n",
    "    model = train_model(n_layers, n_neurons, dropout_threshold, embudo_1, embudo_2, embudo_3, X_train, y_train, X_validate, y_validate)\n",
    "    predictions = make_predictions(model, X_test, trial_number, cluster)\n",
    "    score = calculate_score(predictions)\n",
    "\n",
    "    # Save the results\n",
    "    result = {\n",
    "        \"n_layers\": n_layers,\n",
    "        \"n_neurons\": n_neurons,\n",
    "        \"embudo_1\": embudo_1,\n",
    "        \"embudo_2\": embudo_2,\n",
    "        \"embudo_3\": embudo_3,\n",
    "        \"dropout_threshold\": dropout_threshold,\n",
    "        \"score\": score\n",
    "    }\n",
    "    g_s_df = pd.DataFrame([result])\n",
    "    g_s_df.to_csv(f\"./output/lstm_{version}_optuna_{n_layers}_{n_neurons}_{dropout_threshold}_{score}.csv\", mode='a', index=False)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bo(cluster=None):\n",
    "\n",
    "    # Create a study object and specify the optimization direction\n",
    "    study = optuna.create_study(direction='minimize', study_name=f\"lstm_{version}_BO_{cluster if cluster else 'TODO'}\")\n",
    "\n",
    "    # Perform optimization\n",
    "    study.optimize(lambda x: objective(x, cluster), n_trials=50)\n",
    "\n",
    "    # Extract the best trial\n",
    "    best_trial = study.best_trial\n",
    "\n",
    "    print(f\"Best trial{' for cluster' + str(cluster) if cluster else ''}:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.8078684 , -0.02005994,  0.76774853,  2.34336546,  1.55555699])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"cluster\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 21:56:00,002] A new study created in memory with name: lstm_10_BO_-0.020059937789255886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for cluster -0.020059937789255886\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 434 neurons, 1.4262970651291207 embudo_1, 2.4372122423922833 embudo_2, and 1.226984633715104 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 21:56:00.016076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 21:56:00.016209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 21:56:00.016268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 21:56:00.016443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 21:56:00.016507: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 21:56:00.016574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 21:56:00.016666: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 21:56:00.016719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 21:56:00.016767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3248 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 21:56:02.712627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-11-25 21:56:02.725656: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f35a40017a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-25 21:56:02.725666: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9\n",
      "2023-11-25 21:56:02.734253: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-25 21:56:02.860896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8906\n",
      "2023-11-25 21:56:02.951734: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191/191 [==============================] - 4s 6ms/step - loss: 0.0045 - val_loss: 0.0429\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.3808e-04 - val_loss: 0.0429\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.4244e-04 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 8.6110e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 5.7621e-05 - val_loss: 0.0428\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 3.4855e-05 - val_loss: 0.0428\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.5716e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.9598e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.9102e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8091e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8582e-05 - val_loss: 0.0428\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8897e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7839e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.9154e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7820e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8518e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.0452e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7908e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.9173e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7873e-05 - val_loss: 0.0427\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8029e-05 - val_loss: 0.0427\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.8281e-05 - val_loss: 0.0427\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7566e-05 - val_loss: 0.0427\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.8014e-05 - val_loss: 0.0427\n",
      "Epoch 25/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7733e-05 - val_loss: 0.0427\n",
      "Epoch 26/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7626e-05 - val_loss: 0.0427\n",
      "Epoch 27/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7302e-05 - val_loss: 0.0427\n",
      "Epoch 28/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7148e-05 - val_loss: 0.0427\n",
      "Epoch 29/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7541e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 21:56:36,193] Trial 0 finished with value: 9.284959958354843 and parameters: {'n_layers': 3, 'n_neurons': 434, 'embudo_1': 1.4262970651291207, 'embudo_2': 2.4372122423922833, 'embudo_3': 1.226984633715104, 'dropout_threshold': 0.4535291792003888}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 286 neurons, 1.498221356102266 embudo_1, 1.2638020005084938 embudo_2, and 3.292473929456645 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 7ms/step - loss: 0.0071 - val_loss: 0.0429\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.3912e-04 - val_loss: 0.0427\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 8.2109e-05 - val_loss: 0.0428\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 4.6616e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.7718e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.0147e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7534e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8135e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6865e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6672e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7022e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7007e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7877e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7465e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 21:57:00,681] Trial 1 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 286, 'embudo_1': 1.498221356102266, 'embudo_2': 1.2638020005084938, 'embudo_3': 3.292473929456645, 'dropout_threshold': 0.1818793644283666}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 156 neurons, 1.374514168879929 embudo_1, 0.8712078736381931 embudo_2, and 2.142820399793039 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 3s 5ms/step - loss: 0.0081 - val_loss: 0.0428\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 3.4422e-04 - val_loss: 0.0427\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.2782e-04 - val_loss: 0.0428\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 5.5386e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.6423e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.8635e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6844e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7427e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6607e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7426e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6990e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6735e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7655e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6952e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7003e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7569e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 21:57:23,107] Trial 2 finished with value: 9.284959958354843 and parameters: {'n_layers': 3, 'n_neurons': 156, 'embudo_1': 1.374514168879929, 'embudo_2': 0.8712078736381931, 'embudo_3': 2.142820399793039, 'dropout_threshold': 0.386754424761911}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 458 neurons, 1.0096391274204592 embudo_1, 2.2721009807929935 embudo_2, and 2.208663495920029 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 7ms/step - loss: 0.0054 - val_loss: 0.0428\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 9.5449e-05 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 6.2884e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 4.7126e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 3.4764e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.8437e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.3219e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.0947e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8541e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8694e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8278e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7171e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7197e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7227e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8492e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7666e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.6994e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 21:57:52,439] Trial 3 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 458, 'embudo_1': 1.0096391274204592, 'embudo_2': 2.2721009807929935, 'embudo_3': 2.208663495920029, 'dropout_threshold': 0.21606685181502727}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 490 neurons, 1.0376336676470979 embudo_1, 1.934590706625347 embudo_2, and 2.530174106708089 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 7ms/step - loss: 0.0051 - val_loss: 0.0429\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 9.8141e-05 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 4.7387e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.9009e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.0990e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8201e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7624e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8699e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9014e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8121e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8023e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7740e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7463e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9238e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8729e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9071e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8129e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8146e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8894e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 21:58:24,479] Trial 4 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 490, 'embudo_1': 1.0376336676470979, 'embudo_2': 1.934590706625347, 'embudo_3': 2.530174106708089, 'dropout_threshold': 0.273963746237737}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 255 neurons, 1.1147617307823263 embudo_1, 1.4519221577776933 embudo_2, and 3.3965837194883672 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 3s 6ms/step - loss: 0.0058 - val_loss: 0.0430\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6405e-04 - val_loss: 0.0427\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 8.7606e-05 - val_loss: 0.0428\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 6.9626e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 5.8306e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 4.3239e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 3.2286e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.2884e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.0928e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7957e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7727e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7180e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 21:58:44,470] Trial 5 finished with value: 9.284959958354843 and parameters: {'n_layers': 3, 'n_neurons': 255, 'embudo_1': 1.1147617307823263, 'embudo_2': 1.4519221577776933, 'embudo_3': 3.3965837194883672, 'dropout_threshold': 0.2889880067048426}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 376 neurons, 1.4547195120384213 embudo_1, 1.1493438036836217 embudo_2, and 2.8675212599587185 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 7ms/step - loss: 0.0063 - val_loss: 0.0428\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7283e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 6.0237e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.3028e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7751e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7465e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7206e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7526e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7062e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7477e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7484e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8257e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7318e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7748e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7825e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7043e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7799e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8105e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7810e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8069e-05 - val_loss: 0.0427\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8499e-05 - val_loss: 0.0427\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8742e-05 - val_loss: 0.0427\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8218e-05 - val_loss: 0.0427\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7319e-05 - val_loss: 0.0428\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 21:59:19,680] Trial 6 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 376, 'embudo_1': 1.4547195120384213, 'embudo_2': 1.1493438036836217, 'embudo_3': 2.8675212599587185, 'dropout_threshold': 0.3734523579340643}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 165 neurons, 1.2481139297672899 embudo_1, 2.2829756299723205 embudo_2, and 3.1790519214537714 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 7ms/step - loss: 0.0093 - val_loss: 0.0427\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.1230e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.1930e-04 - val_loss: 0.0428\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 5.6151e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.7984e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8808e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7419e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6752e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7159e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7211e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.6987e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7410e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.6794e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6901e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7534e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7672e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6683e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7297e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8114e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8226e-05 - val_loss: 0.0427\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8008e-05 - val_loss: 0.0427\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8038e-05 - val_loss: 0.0427\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7556e-05 - val_loss: 0.0427\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7609e-05 - val_loss: 0.0427\n",
      "Epoch 25/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7544e-05 - val_loss: 0.0427\n",
      "Epoch 26/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8482e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 21:59:57,019] Trial 7 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 165, 'embudo_1': 1.2481139297672899, 'embudo_2': 2.2829756299723205, 'embudo_3': 3.1790519214537714, 'dropout_threshold': 0.2946073874922296}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 40 neurons, 1.0454328518306213 embudo_1, 1.0287527723070204 embudo_2, and 1.9945021098082734 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 2s 4ms/step - loss: 0.0185 - val_loss: 0.0424\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 4.7517e-04 - val_loss: 0.0426\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 4.9953e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 2.6950e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 2.1610e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 2.0066e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9041e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8774e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8229e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.7895e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.7708e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:00:12,531] Trial 8 finished with value: 9.284959958354843 and parameters: {'n_layers': 2, 'n_neurons': 40, 'embudo_1': 1.0454328518306213, 'embudo_2': 1.0287527723070204, 'embudo_3': 1.9945021098082734, 'dropout_threshold': 0.4954514786675225}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 241 neurons, 1.493758359916746 embudo_1, 1.4458410928380023 embudo_2, and 3.28723983874165 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 2s 4ms/step - loss: 0.0067 - val_loss: 0.0429\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 3.3407e-04 - val_loss: 0.0427\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 2.8691e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8437e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8129e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8009e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.7859e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8007e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8200e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8001e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.7741e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8856e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.7515e-05 - val_loss: 0.0428\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:00:29,108] Trial 9 finished with value: 9.284959958354843 and parameters: {'n_layers': 2, 'n_neurons': 241, 'embudo_1': 1.493758359916746, 'embudo_2': 1.4458410928380023, 'embudo_3': 3.28723983874165, 'dropout_threshold': 0.523973078115165}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.3333333333333333, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.36363636363636365, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(160.33333333333334, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(43.72727272727273, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.19999999999999998, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05454545454545454, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5666666666666667, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15454545454545454, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.9333333333333332, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2545454545454545, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.18666666666666668, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05090909090909091, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 383 neurons, 1.3018009894170581 embudo_1, 2.456428805274096 embudo_2, and 1.0581437202289703 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 2s 3ms/step - loss: 0.0033 - val_loss: 0.0428\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.0876e-05 - val_loss: 0.0427\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.0866e-05 - val_loss: 0.0428\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 1.8073e-05 - val_loss: 0.0428\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 1.9606e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 1.9161e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 3.9253e-05 - val_loss: 0.0428\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 1.9339e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 3.4388e-05 - val_loss: 0.0435\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 4.2149e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 1.9251e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.0620e-05 - val_loss: 0.0426\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.8731e-05 - val_loss: 0.0426\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 6.0746e-05 - val_loss: 0.0428\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.6641e-05 - val_loss: 0.0425\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.5763e-05 - val_loss: 0.0426\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 5.2769e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.3490e-05 - val_loss: 0.0426\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 3.5166e-05 - val_loss: 0.0430\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.3630e-05 - val_loss: 0.0427\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.3420e-05 - val_loss: 0.0427\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 3.0179e-05 - val_loss: 0.0427\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 3.5359e-05 - val_loss: 0.0427\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.2632e-05 - val_loss: 0.0429\n",
      "Epoch 25/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 3.3623e-05 - val_loss: 0.0425\n",
      "Epoch 26/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 3.1593e-05 - val_loss: 0.0427\n",
      "Epoch 27/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 6.9040e-05 - val_loss: 0.0427\n",
      "Epoch 28/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.0789e-05 - val_loss: 0.0426\n",
      "Epoch 29/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.6482e-05 - val_loss: 0.0427\n",
      "Epoch 30/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.1793e-05 - val_loss: 0.0426\n",
      "Epoch 31/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.3536e-05 - val_loss: 0.0427\n",
      "Epoch 32/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.9223e-05 - val_loss: 0.0427\n",
      "Epoch 33/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 3.3278e-05 - val_loss: 0.0428\n",
      "Epoch 34/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.6293e-05 - val_loss: 0.0426\n",
      "Epoch 35/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.1039e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:00:52,771] Trial 10 finished with value: 9.284959958354843 and parameters: {'n_layers': 1, 'n_neurons': 383, 'embudo_1': 1.3018009894170581, 'embudo_2': 2.456428805274096, 'embudo_3': 1.0581437202289703, 'dropout_threshold': 0.6191088437436241}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.36363636363636365, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(43.72727272727273, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05454545454545454, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15454545454545454, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2545454545454545, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05090909090909091, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 356 neurons, 1.3844797496315902 embudo_1, 1.8940130598708462 embudo_2, and 1.3298449705221915 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 3s 6ms/step - loss: 0.0048 - val_loss: 0.0429\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 9.9437e-05 - val_loss: 0.0429\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 8.5643e-05 - val_loss: 0.0430\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 7.1510e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 5.9751e-05 - val_loss: 0.0428\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 4.9848e-05 - val_loss: 0.0429\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 4.3651e-05 - val_loss: 0.0428\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 3.9024e-05 - val_loss: 0.0428\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 3.3761e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 3.1190e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.8407e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.5227e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.4647e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.0475e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:01:14,572] Trial 11 finished with value: 9.284959958354843 and parameters: {'n_layers': 3, 'n_neurons': 356, 'embudo_1': 1.3844797496315902, 'embudo_2': 1.8940130598708462, 'embudo_3': 1.3298449705221915, 'dropout_threshold': 0.10897998786556806}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.3333333333333333, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(40.083333333333336, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.049999999999999996, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14166666666666666, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2333333333333333, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04666666666666667, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 327 neurons, 1.4694561830008663 embudo_1, 1.7205465438274135 embudo_2, and 1.4819707015537373 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_37 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_39 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 3s 6ms/step - loss: 0.0047 - val_loss: 0.0429\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.0648e-04 - val_loss: 0.0429\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 8.8066e-05 - val_loss: 0.0429\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 7.4578e-05 - val_loss: 0.0428\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 6.0396e-05 - val_loss: 0.0428\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 5.3286e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 4.5120e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 3.9845e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 3.4827e-05 - val_loss: 0.0428\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 3.1849e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.9192e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.6322e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.5120e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.2232e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.2960e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.0237e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.9821e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.0235e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8098e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7990e-05 - val_loss: 0.0427\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8007e-05 - val_loss: 0.0427\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.0922e-05 - val_loss: 0.0427\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7831e-05 - val_loss: 0.0427\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7475e-05 - val_loss: 0.0427\n",
      "Epoch 25/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7854e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:01:45,539] Trial 12 finished with value: 9.284959958354843 and parameters: {'n_layers': 3, 'n_neurons': 327, 'embudo_1': 1.4694561830008663, 'embudo_2': 1.7205465438274135, 'embudo_3': 1.4819707015537373, 'dropout_threshold': 0.1086593089918532}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.3076923076923077, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(37.0, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04615384615384615, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13076923076923078, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.21538461538461537, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04307692307692308, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 433 neurons, 1.3672825193065927 embudo_1, 1.418436962225174 embudo_2, and 1.7162952532878741 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_40 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_41 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 3s 4ms/step - loss: 0.0035 - val_loss: 0.0430\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 3.0460e-04 - val_loss: 0.0432\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.6621e-04 - val_loss: 0.0429\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 8.1896e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 3.9732e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 2.2542e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.9863e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.8307e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9165e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.8938e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 2.0559e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8365e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 2.1944e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.9517e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.0371e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:02:04,424] Trial 13 finished with value: 9.284959958354843 and parameters: {'n_layers': 2, 'n_neurons': 433, 'embudo_1': 1.3672825193065927, 'embudo_2': 1.418436962225174, 'embudo_3': 1.7162952532878741, 'dropout_threshold': 0.4227280834803949}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2857142857142857, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(34.357142857142854, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04285714285714286, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12142857142857143, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.19999999999999998, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 292 neurons, 1.2063862161614824 embudo_1, 2.065740770431022 embudo_2, and 0.8293036290333393 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_42 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_44 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 7ms/step - loss: 0.0066 - val_loss: 0.0428\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.5450e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.0786e-04 - val_loss: 0.0426\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 7.3318e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 4.3890e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.6534e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.0213e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7841e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7217e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7511e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7341e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7718e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7806e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:02:28,759] Trial 14 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 292, 'embudo_1': 1.2063862161614824, 'embudo_2': 2.065740770431022, 'embudo_3': 0.8293036290333393, 'dropout_threshold': 0.2011433732780576}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.26666666666666666, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(32.06666666666667, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11333333333333333, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.18666666666666665, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.037333333333333336, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 189 neurons, 0.9321098017035767 embudo_1, 2.4808332427228437 embudo_2, and 2.740355106950748 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_46 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_47 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 3s 5ms/step - loss: 0.0077 - val_loss: 0.0429\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 3.0652e-04 - val_loss: 0.0429\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.5233e-04 - val_loss: 0.0428\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 9.7403e-05 - val_loss: 0.0428\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 5.6931e-05 - val_loss: 0.0428\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.5737e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7529e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7156e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6651e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6661e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7024e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6638e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7261e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7027e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7065e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7250e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7338e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7255e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7128e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7044e-05 - val_loss: 0.0427\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7406e-05 - val_loss: 0.0427\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7523e-05 - val_loss: 0.0427\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7431e-05 - val_loss: 0.0427\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7012e-05 - val_loss: 0.0427\n",
      "Epoch 25/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7432e-05 - val_loss: 0.0427\n",
      "Epoch 26/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7167e-05 - val_loss: 0.0427\n",
      "Epoch 27/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7473e-05 - val_loss: 0.0427\n",
      "Epoch 28/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7992e-05 - val_loss: 0.0427\n",
      "Epoch 29/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7186e-05 - val_loss: 0.0427\n",
      "Epoch 30/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.8013e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:03:03,077] Trial 15 finished with value: 9.284959958354843 and parameters: {'n_layers': 3, 'n_neurons': 189, 'embudo_1': 0.9321098017035767, 'embudo_2': 2.4808332427228437, 'embudo_3': 2.740355106950748, 'dropout_threshold': 0.45751680495395364}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.25, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(30.0625, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0375, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10625, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.175, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.035, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 71 neurons, 1.499788248601192 embudo_1, 1.6796027831171267 embudo_2, and 0.7267560163543775 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 0.0062 - val_loss: 0.0429\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 4.1991e-05 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.6922e-05 - val_loss: 0.0428\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.2525e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.1637e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.0350e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 1.9579e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 1.9988e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 1.9225e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.0118e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.3308e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 1.8953e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.1727e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.7491e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 3.2293e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.0898e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.0435e-05 - val_loss: 0.0426\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.0215e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.0954e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.2635e-05 - val_loss: 0.0426\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.6516e-05 - val_loss: 0.0427\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 5.5600e-05 - val_loss: 0.0426\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.5017e-05 - val_loss: 0.0427\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.1855e-05 - val_loss: 0.0427\n",
      "Epoch 25/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 3.3833e-05 - val_loss: 0.0427\n",
      "Epoch 26/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.5007e-05 - val_loss: 0.0426\n",
      "Epoch 27/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.3420e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:03:21,957] Trial 16 finished with value: 9.284959958354843 and parameters: {'n_layers': 1, 'n_neurons': 71, 'embudo_1': 1.499788248601192, 'embudo_2': 1.6796027831171267, 'embudo_3': 0.7267560163543775, 'dropout_threshold': 0.3445424876427593}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.23529411764705882, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(28.294117647058822, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03529411764705882, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.16470588235294117, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03294117647058824, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 415 neurons, 1.4311537846080007 embudo_1, 1.1996229585692135 embudo_2, and 1.7531246807596557 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_50 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_51 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_52 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_53 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 7ms/step - loss: 0.0058 - val_loss: 0.0428\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.2490e-04 - val_loss: 0.0427\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.4380e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8262e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9344e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8998e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.0269e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8396e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.0086e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8419e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8157e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9383e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.9123e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8405e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.0107e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.9093e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7562e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.0487e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8608e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8414e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:03:53,478] Trial 17 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 415, 'embudo_1': 1.4311537846080007, 'embudo_2': 1.1996229585692135, 'embudo_3': 1.7531246807596557, 'dropout_threshold': 0.561210307834829}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2222222222222222, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(26.72222222222222, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03333333333333333, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09444444444444444, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15555555555555556, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.031111111111111114, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 320 neurons, 1.3067454780072303 embudo_1, 0.8037074274652003 embudo_2, and 3.454366581089158 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_54 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_55 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_56 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 3s 6ms/step - loss: 0.0058 - val_loss: 0.0430\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.8808e-04 - val_loss: 0.0429\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.0303e-04 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 6.6140e-05 - val_loss: 0.0428\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 3.9711e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.5201e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8861e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7655e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7944e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7690e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7651e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7338e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8224e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.8389e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7296e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7062e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.8408e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.9086e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7676e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7407e-05 - val_loss: 0.0427\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7893e-05 - val_loss: 0.0427\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7275e-05 - val_loss: 0.0427\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8412e-05 - val_loss: 0.0427\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.8197e-05 - val_loss: 0.0427\n",
      "Epoch 25/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7651e-05 - val_loss: 0.0427\n",
      "Epoch 26/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7908e-05 - val_loss: 0.0427\n",
      "Epoch 27/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7309e-05 - val_loss: 0.0427\n",
      "Epoch 28/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6956e-05 - val_loss: 0.0427\n",
      "Epoch 29/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7159e-05 - val_loss: 0.0427\n",
      "Epoch 30/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7154e-05 - val_loss: 0.0427\n",
      "Epoch 31/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7338e-05 - val_loss: 0.0427\n",
      "Epoch 32/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7481e-05 - val_loss: 0.0427\n",
      "Epoch 33/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7212e-05 - val_loss: 0.0427\n",
      "Epoch 34/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6514e-05 - val_loss: 0.0427\n",
      "Epoch 35/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7008e-05 - val_loss: 0.0427\n",
      "Epoch 36/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7108e-05 - val_loss: 0.0427\n",
      "Epoch 37/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7934e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:04:35,037] Trial 18 finished with value: 9.284959958354843 and parameters: {'n_layers': 3, 'n_neurons': 320, 'embudo_1': 1.3067454780072303, 'embudo_2': 0.8037074274652003, 'embudo_3': 3.454366581089158, 'dropout_threshold': 0.4298315510785254}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.21052631578947367, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(25.31578947368421, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.031578947368421054, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08947368421052632, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14736842105263157, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02947368421052632, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 120 neurons, 1.4107742830807748 embudo_1, 1.5341992908035702 embudo_2, and 2.5449962823773937 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_57 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_58 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 2s 4ms/step - loss: 0.0103 - val_loss: 0.0429\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 3.9253e-04 - val_loss: 0.0427\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 3.2385e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 2.3737e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 2.1493e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 2.1475e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 2.0579e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9882e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 2.0404e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9106e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9172e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9016e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8504e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8444e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:04:52,564] Trial 19 finished with value: 9.284959958354843 and parameters: {'n_layers': 2, 'n_neurons': 120, 'embudo_1': 1.4107742830807748, 'embudo_2': 1.5341992908035702, 'embudo_3': 2.5449962823773937, 'dropout_threshold': 0.6426899985393405}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(24.05, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08499999999999999, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13999999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.028000000000000004, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 211 neurons, 1.329536058157584 embudo_1, 1.2845815267947094 embudo_2, and 3.12226915690501 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_59 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_60 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_61 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_62 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 6ms/step - loss: 0.0082 - val_loss: 0.0428\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.6701e-04 - val_loss: 0.0427\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.8271e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7957e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8031e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7768e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7549e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7869e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8143e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7944e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7791e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7626e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8375e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8421e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7955e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7753e-05 - val_loss: 0.0428\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.2623e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8655e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7625e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8666e-05 - val_loss: 0.0427\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8663e-05 - val_loss: 0.0427\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8509e-05 - val_loss: 0.0427\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8658e-05 - val_loss: 0.0427\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7922e-05 - val_loss: 0.0427\n",
      "Epoch 25/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8165e-05 - val_loss: 0.0427\n",
      "Epoch 26/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8491e-05 - val_loss: 0.0427\n",
      "Epoch 27/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8947e-05 - val_loss: 0.0427\n",
      "Epoch 28/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.9378e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:05:31,826] Trial 20 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 211, 'embudo_1': 1.329536058157584, 'embudo_2': 1.2845815267947094, 'embudo_3': 3.12226915690501, 'dropout_threshold': 0.47460697940236807}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(24.05, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08499999999999999, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13999999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.028000000000000004, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 127 neurons, 1.41380798932696 embudo_1, 0.9704124707676349 embudo_2, and 2.176177501641418 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_63 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_64 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_65 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 3s 5ms/step - loss: 0.0088 - val_loss: 0.0430\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 3.6146e-04 - val_loss: 0.0427\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6959e-04 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.0579e-04 - val_loss: 0.0428\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 5.9705e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.7191e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7593e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6798e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6544e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7131e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6691e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7233e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:05:50,504] Trial 21 finished with value: 9.284959958354843 and parameters: {'n_layers': 3, 'n_neurons': 127, 'embudo_1': 1.41380798932696, 'embudo_2': 0.9704124707676349, 'embudo_3': 2.176177501641418, 'dropout_threshold': 0.4001694638749039}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.19047619047619047, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(22.904761904761905, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02857142857142857, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08095238095238096, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13333333333333333, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02666666666666667, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 148 neurons, 1.3601861825365695 embudo_1, 0.909903359768919 embudo_2, and 1.2287094741541955 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_66 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_67 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_68 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 3s 6ms/step - loss: 0.0075 - val_loss: 0.0428\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 3.5361e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7317e-04 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.1422e-04 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 7.6971e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 5.6161e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 4.1089e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 3.0802e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.3040e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.9115e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7419e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6966e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6906e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7161e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6620e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7294e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7257e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:06:14,360] Trial 22 finished with value: 9.284959958354843 and parameters: {'n_layers': 3, 'n_neurons': 148, 'embudo_1': 1.3601861825365695, 'embudo_2': 0.909903359768919, 'embudo_3': 1.2287094741541955, 'dropout_threshold': 0.3549674829310535}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.18181818181818182, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(21.863636363636363, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02727272727272727, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07727272727272727, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12727272727272726, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.025454545454545455, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 213 neurons, 1.4450400754110466 embudo_1, 1.0967702483557682 embudo_2, and 2.3611917774137026 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_69 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_70 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_71 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 3s 6ms/step - loss: 0.0068 - val_loss: 0.0428\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.6245e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.3552e-04 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 9.1199e-05 - val_loss: 0.0428\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 6.9568e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 4.9330e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 3.5301e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.4562e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.8875e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7483e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7351e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6707e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.8618e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7545e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6582e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7454e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6493e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7207e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7569e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.8035e-05 - val_loss: 0.0427\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7489e-05 - val_loss: 0.0427\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.8975e-05 - val_loss: 0.0427\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7441e-05 - val_loss: 0.0427\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7788e-05 - val_loss: 0.0427\n",
      "Epoch 25/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7440e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:06:44,724] Trial 23 finished with value: 9.284959958354843 and parameters: {'n_layers': 3, 'n_neurons': 213, 'embudo_1': 1.4450400754110466, 'embudo_2': 1.0967702483557682, 'embudo_3': 2.3611917774137026, 'dropout_threshold': 0.4006228427471998}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.17391304347826086, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20.91304347826087, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02608695652173913, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07391304347826087, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.1217391304347826, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.024347826086956525, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 280 neurons, 1.397257958865604 embudo_1, 1.214969000807456 embudo_2, and 1.9098370709800854 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_72 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_73 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 2s 4ms/step - loss: 0.0044 - val_loss: 0.0429\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 3.8317e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.4123e-04 - val_loss: 0.0428\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 4.0470e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 2.0321e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.7905e-05 - val_loss: 0.0428\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9458e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8399e-05 - val_loss: 0.0428\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8910e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 2.0206e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.9364e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9603e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9566e-05 - val_loss: 0.0428\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9809e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8624e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9521e-05 - val_loss: 0.0428\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 2.2274e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9334e-05 - val_loss: 0.0428\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9973e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9812e-05 - val_loss: 0.0427\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.9350e-05 - val_loss: 0.0427\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 2.0078e-05 - val_loss: 0.0427\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8695e-05 - val_loss: 0.0427\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8676e-05 - val_loss: 0.0428\n",
      "Epoch 25/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9227e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:07:09,012] Trial 24 finished with value: 9.284959958354843 and parameters: {'n_layers': 2, 'n_neurons': 280, 'embudo_1': 1.397257958865604, 'embudo_2': 1.214969000807456, 'embudo_3': 1.9098370709800854, 'dropout_threshold': 0.4516473507684235}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.16666666666666666, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20.041666666666668, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.024999999999999998, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07083333333333333, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11666666666666665, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.023333333333333334, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 86 neurons, 1.4498407909305864 embudo_1, 1.3125290013789583 embudo_2, and 2.950681958078478 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_74 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_75 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_76 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 3s 7ms/step - loss: 0.0111 - val_loss: 0.0431\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 3.4872e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.3914e-04 - val_loss: 0.0428\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 6.3118e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.4149e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7982e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7451e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7838e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7376e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7546e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7980e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.8292e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6896e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7513e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:07:29,924] Trial 25 finished with value: 9.284959958354843 and parameters: {'n_layers': 3, 'n_neurons': 86, 'embudo_1': 1.4498407909305864, 'embudo_2': 1.3125290013789583, 'embudo_3': 2.950681958078478, 'dropout_threshold': 0.33822206816201583}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.16, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(19.24, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.024, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.068, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11199999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.022400000000000003, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 322 neurons, 1.3458802512260912 embudo_1, 0.872906507779533 embudo_2, and 1.640389772911345 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_77 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_78 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_79 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_80 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 7ms/step - loss: 0.0068 - val_loss: 0.0427\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.2184e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.1273e-04 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 6.3219e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 3.4309e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1218e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7691e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6773e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6821e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7091e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7034e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6795e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7452e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7739e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7783e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7032e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7623e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8038e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7162e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7899e-05 - val_loss: 0.0427\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7248e-05 - val_loss: 0.0427\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7165e-05 - val_loss: 0.0427\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7813e-05 - val_loss: 0.0427\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7941e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:08:05,988] Trial 26 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 322, 'embudo_1': 1.3458802512260912, 'embudo_2': 0.872906507779533, 'embudo_3': 1.640389772911345, 'dropout_threshold': 0.39397405836796406}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15384615384615385, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(18.5, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.023076923076923075, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06538461538461539, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10769230769230768, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02153846153846154, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 23 neurons, 1.4830447042543031 embudo_1, 1.0373327092094795 embudo_2, and 1.9666309753795883 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_81 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_82 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_83 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 3s 5ms/step - loss: 0.0228 - val_loss: 0.0433\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 6.7281e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 8.1611e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.3443e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7355e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6906e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6845e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6440e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6372e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6508e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6306e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6332e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6484e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6365e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:08:26,254] Trial 27 finished with value: 9.284959958354843 and parameters: {'n_layers': 3, 'n_neurons': 23, 'embudo_1': 1.4830447042543031, 'embudo_2': 1.0373327092094795, 'embudo_3': 1.9666309753795883, 'dropout_threshold': 0.5036910345134037}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14814814814814814, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(17.814814814814813, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.022222222222222223, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06296296296296296, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.1037037037037037, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.020740740740740744, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 468 neurons, 1.2825503642941773 embudo_1, 1.00014186505975 embudo_2, and 2.344030893778262 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_84 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_85 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 2s 6ms/step - loss: 0.0044 - val_loss: 0.0430\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 3.1566e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.3769e-04 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 3.0820e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 2.0142e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9217e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.9934e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.9969e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.9327e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8508e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8711e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8320e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.0918e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.0007e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9401e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9455e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.2976e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.9053e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 2.2095e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9694e-05 - val_loss: 0.0428\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9439e-05 - val_loss: 0.0427\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8335e-05 - val_loss: 0.0427\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8930e-05 - val_loss: 0.0427\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.8580e-05 - val_loss: 0.0427\n",
      "Epoch 25/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8370e-05 - val_loss: 0.0427\n",
      "Epoch 26/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8564e-05 - val_loss: 0.0427\n",
      "Epoch 27/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9287e-05 - val_loss: 0.0427\n",
      "Epoch 28/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7766e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:08:53,617] Trial 28 finished with value: 9.284959958354843 and parameters: {'n_layers': 2, 'n_neurons': 468, 'embudo_1': 1.2825503642941773, 'embudo_2': 1.00014186505975, 'embudo_3': 2.344030893778262, 'dropout_threshold': 0.44239511754676314}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14285714285714285, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(17.178571428571427, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02142857142857143, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.060714285714285714, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 427 neurons, 1.4062256625375222 embudo_1, 1.1020257706408618 embudo_2, and 2.1809137420706026 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_86 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_87 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_88 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_89 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 7ms/step - loss: 0.0059 - val_loss: 0.0428\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 8.9053e-05 - val_loss: 0.0427\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 6.9343e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 5.1170e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 4.2353e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 3.5837e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.9956e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.7052e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.6085e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.3168e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1725e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.0755e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.0058e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8596e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:09:19,467] Trial 29 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 427, 'embudo_1': 1.4062256625375222, 'embudo_2': 1.1020257706408618, 'embudo_3': 2.1809137420706026, 'dropout_threshold': 0.16256611642910226}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13793103448275862, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(16.586206896551722, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.020689655172413793, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05862068965517241, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09655172413793103, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.019310344827586208, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 238 neurons, 1.3646792430653274 embudo_1, 0.8126160674271279 embudo_2, and 1.503978208817612 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_90 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_91 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_92 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_93 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 7ms/step - loss: 0.0082 - val_loss: 0.0429\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7808e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 7.8796e-05 - val_loss: 0.0428\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 3.3650e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.0666e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7989e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7194e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7206e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7401e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7343e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7128e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7284e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7781e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7557e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7594e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7016e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8033e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7607e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8279e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.6800e-05 - val_loss: 0.0427\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7819e-05 - val_loss: 0.0427\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8365e-05 - val_loss: 0.0427\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7297e-05 - val_loss: 0.0427\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8410e-05 - val_loss: 0.0427\n",
      "Epoch 25/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7799e-05 - val_loss: 0.0427\n",
      "Epoch 26/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7549e-05 - val_loss: 0.0427\n",
      "Epoch 27/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8971e-05 - val_loss: 0.0427\n",
      "Epoch 28/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8396e-05 - val_loss: 0.0427\n",
      "Epoch 29/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7186e-05 - val_loss: 0.0427\n",
      "Epoch 30/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8227e-05 - val_loss: 0.0427\n",
      "Epoch 31/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8254e-05 - val_loss: 0.0427\n",
      "Epoch 32/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7219e-05 - val_loss: 0.0428\n",
      "Epoch 33/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.0225e-05 - val_loss: 0.0427\n",
      "Epoch 34/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8151e-05 - val_loss: 0.0427\n",
      "Epoch 35/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8477e-05 - val_loss: 0.0427\n",
      "Epoch 36/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8298e-05 - val_loss: 0.0427\n",
      "Epoch 37/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7639e-05 - val_loss: 0.0427\n",
      "Epoch 38/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8705e-05 - val_loss: 0.0427\n",
      "Epoch 39/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7791e-05 - val_loss: 0.0427\n",
      "Epoch 40/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8544e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:10:10,781] Trial 30 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 238, 'embudo_1': 1.3646792430653274, 'embudo_2': 0.8126160674271279, 'embudo_3': 1.503978208817612, 'dropout_threshold': 0.23169150335213096}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13793103448275862, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(16.586206896551722, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.020689655172413793, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05862068965517241, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09655172413793103, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.019310344827586208, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 475 neurons, 1.1452849991909773 embudo_1, 2.2761653237003587 embudo_2, and 2.6104910028294688 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_94 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_95 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_96 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_97 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 8ms/step - loss: 0.0055 - val_loss: 0.0428\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.0445e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 6.4849e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 4.2238e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.8614e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1597e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.0037e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7525e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7578e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7416e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9042e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8662e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7451e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7701e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7561e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8474e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8175e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7970e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7800e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8325e-05 - val_loss: 0.0427\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8084e-05 - val_loss: 0.0427\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8270e-05 - val_loss: 0.0427\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8458e-05 - val_loss: 0.0427\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8401e-05 - val_loss: 0.0427\n",
      "Epoch 25/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9220e-05 - val_loss: 0.0427\n",
      "Epoch 26/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8865e-05 - val_loss: 0.0427\n",
      "Epoch 27/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7978e-05 - val_loss: 0.0427\n",
      "Epoch 28/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7228e-05 - val_loss: 0.0427\n",
      "Epoch 29/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.0583e-05 - val_loss: 0.0427\n",
      "Epoch 30/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8946e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:10:54,959] Trial 31 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 475, 'embudo_1': 1.1452849991909773, 'embudo_2': 2.2761653237003587, 'embudo_3': 2.6104910028294688, 'dropout_threshold': 0.248712371186162}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13333333333333333, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(16.033333333333335, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.056666666666666664, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333332, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.018666666666666668, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 498 neurons, 0.9045534420551378 embudo_1, 1.8071031193931906 embudo_2, and 2.3853163828583934 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_98 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_99 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_100 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_101 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 9ms/step - loss: 0.0047 - val_loss: 0.0429\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 9.1404e-05 - val_loss: 0.0427\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 4.6073e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 3.1199e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.4115e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9997e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9172e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 1.7389e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8965e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8398e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7530e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7672e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9283e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8095e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8352e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:11:23,467] Trial 32 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 498, 'embudo_1': 0.9045534420551378, 'embudo_2': 1.8071031193931906, 'embudo_3': 2.3853163828583934, 'dropout_threshold': 0.2790392473558444}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12903225806451613, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(15.516129032258064, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01935483870967742, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.054838709677419356, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09032258064516129, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01806451612903226, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 457 neurons, 1.453453842743088 embudo_1, 2.0014055677160183 embudo_2, and 3.489492888103004 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_102 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_103 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_104 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_105 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 7ms/step - loss: 0.0055 - val_loss: 0.0427\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.1500e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 4.8593e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.5376e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9205e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7616e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7678e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7947e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7003e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7820e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7919e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8562e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8208e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8715e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7739e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7935e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7829e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9344e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8104e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8980e-05 - val_loss: 0.0427\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8324e-05 - val_loss: 0.0427\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8660e-05 - val_loss: 0.0427\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8649e-05 - val_loss: 0.0427\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8382e-05 - val_loss: 0.0427\n",
      "Epoch 25/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7997e-05 - val_loss: 0.0428\n",
      "Epoch 26/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.2319e-05 - val_loss: 0.0427\n",
      "Epoch 27/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9389e-05 - val_loss: 0.0427\n",
      "Epoch 28/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8664e-05 - val_loss: 0.0427\n",
      "Epoch 29/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9079e-05 - val_loss: 0.0427\n",
      "Epoch 30/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8468e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:12:07,024] Trial 33 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 457, 'embudo_1': 1.453453842743088, 'embudo_2': 2.0014055677160183, 'embudo_3': 3.489492888103004, 'dropout_threshold': 0.32502923613316276}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.125, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(15.03125, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01875, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.053125, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0875, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0175, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 400 neurons, 1.2612163538482057 embudo_1, 1.5538091217472896 embudo_2, and 2.7842652636910046 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_106 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_107 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_108 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 3s 6ms/step - loss: 0.0050 - val_loss: 0.0429\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6804e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 8.7838e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 5.4894e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 3.9904e-05 - val_loss: 0.0428\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 3.2105e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.5174e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.1931e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.9680e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8805e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7922e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7487e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8243e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8320e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:12:28,348] Trial 34 finished with value: 9.284959958354843 and parameters: {'n_layers': 3, 'n_neurons': 400, 'embudo_1': 1.2612163538482057, 'embudo_2': 1.5538091217472896, 'embudo_3': 2.7842652636910046, 'dropout_threshold': 0.2972091444400448}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12121212121212122, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(14.575757575757576, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01818181818181818, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.051515151515151514, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08484848484848484, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01696969696969697, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 449 neurons, 1.219680486239951 embudo_1, 2.3347623466301597 embudo_2, and 2.157132804126244 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_109 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_110 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_111 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_112 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 8ms/step - loss: 0.0055 - val_loss: 0.0429\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.0200e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 3.7351e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1406e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8643e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8525e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8274e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7874e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8512e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9298e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7338e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9833e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8390e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7249e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8767e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.0592e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7875e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7220e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9003e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9390e-05 - val_loss: 0.0427\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8818e-05 - val_loss: 0.0427\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8578e-05 - val_loss: 0.0427\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9237e-05 - val_loss: 0.0427\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9232e-05 - val_loss: 0.0427\n",
      "Epoch 25/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8671e-05 - val_loss: 0.0427\n",
      "Epoch 26/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8882e-05 - val_loss: 0.0427\n",
      "Epoch 27/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7972e-05 - val_loss: 0.0427\n",
      "Epoch 28/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9348e-05 - val_loss: 0.0427\n",
      "Epoch 29/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9443e-05 - val_loss: 0.0427\n",
      "Epoch 30/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7731e-05 - val_loss: 0.0427\n",
      "Epoch 31/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8388e-05 - val_loss: 0.0429\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:13:13,890] Trial 35 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 449, 'embudo_1': 1.219680486239951, 'embudo_2': 2.3347623466301597, 'embudo_3': 2.157132804126244, 'dropout_threshold': 0.37292064015021253}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11764705882352941, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(14.147058823529411, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01764705882352941, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.049999999999999996, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08235294117647059, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01647058823529412, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 369 neurons, 1.1232796723211516 embudo_1, 2.083618913095722 embudo_2, and 2.997788993367829 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_113 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_114 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_115 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 3s 6ms/step - loss: 0.0049 - val_loss: 0.0430\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.4599e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 8.1741e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 5.8048e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 4.6329e-05 - val_loss: 0.0428\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 4.3075e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 3.6388e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 3.5321e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 3.3226e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.8572e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.6406e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.4542e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.2791e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.0708e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:13:35,513] Trial 36 finished with value: 9.284959958354843 and parameters: {'n_layers': 3, 'n_neurons': 369, 'embudo_1': 1.1232796723211516, 'embudo_2': 2.083618913095722, 'embudo_3': 2.997788993367829, 'dropout_threshold': 0.1653567426835414}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11428571428571428, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(13.742857142857142, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.017142857142857144, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04857142857142857, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.016, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 268 neurons, 1.4981032698377688 embudo_1, 2.201008842239299 embudo_2, and 3.200971649725632 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_116 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_117 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_118 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_119 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 7ms/step - loss: 0.0077 - val_loss: 0.0428\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7319e-04 - val_loss: 0.0427\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 6.9858e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.5901e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9112e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7515e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7123e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6947e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7190e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7224e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7319e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7278e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6943e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7413e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7431e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:14:01,079] Trial 37 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 268, 'embudo_1': 1.4981032698377688, 'embudo_2': 2.201008842239299, 'embudo_3': 3.200971649725632, 'dropout_threshold': 0.30494744998317913}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.1111111111111111, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(13.36111111111111, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.016666666666666666, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04722222222222222, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07777777777777778, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015555555555555557, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 355 neurons, 1.1776204527558602 embudo_1, 2.42370439868334 embudo_2, and 2.6841763226824553 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_120 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_121 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_122 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_123 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 7ms/step - loss: 0.0060 - val_loss: 0.0427\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.4062e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 6.3921e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 3.0214e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.0762e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8147e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7581e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7938e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7956e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8395e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7563e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8219e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7611e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8263e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8351e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:14:27,481] Trial 38 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 355, 'embudo_1': 1.1776204527558602, 'embudo_2': 2.42370439868334, 'embudo_3': 2.6841763226824553, 'dropout_threshold': 0.3142972840285529}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10810810810810811, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(13.0, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.016216216216216217, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04594594594594594, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07567567567567567, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015135135135135137, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 498 neurons, 1.077342594536164 embudo_1, 2.174647691994908 embudo_2, and 2.4738386751098282 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_124 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_125 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_126 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 3s 6ms/step - loss: 0.0041 - val_loss: 0.0429\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.3165e-04 - val_loss: 0.0427\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 7.7162e-05 - val_loss: 0.0428\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 4.9251e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 3.5090e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.7935e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.3241e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.1914e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.9705e-05 - val_loss: 0.0428\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.9704e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7555e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8052e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8270e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8340e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8696e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7942e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7634e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8754e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8322e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7666e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:14:55,476] Trial 39 finished with value: 9.284959958354843 and parameters: {'n_layers': 3, 'n_neurons': 498, 'embudo_1': 1.077342594536164, 'embudo_2': 2.174647691994908, 'embudo_3': 2.4738386751098282, 'dropout_threshold': 0.23792510518684584}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10526315789473684, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.657894736842104, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015789473684210527, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04473684210526316, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07368421052631578, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01473684210526316, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 164 neurons, 1.4597717135651442 embudo_1, 2.3974480292716986 embudo_2, and 2.057513852105131 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_127 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_128 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 2s 4ms/step - loss: 0.0065 - val_loss: 0.0433\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 4.5914e-04 - val_loss: 0.0430\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.7881e-04 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 4.8311e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.9738e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8643e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8379e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8093e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.7981e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8381e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8826e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8157e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.7627e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8829e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.7210e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8550e-05 - val_loss: 0.0428\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.7853e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8117e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.8335e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 1.8119e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:15:16,536] Trial 40 finished with value: 9.284959958354843 and parameters: {'n_layers': 2, 'n_neurons': 164, 'embudo_1': 1.4597717135651442, 'embudo_2': 2.3974480292716986, 'embudo_3': 2.057513852105131, 'dropout_threshold': 0.35444191998206453}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10526315789473684, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.657894736842104, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015789473684210527, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04473684210526316, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07368421052631578, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01473684210526316, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 391 neurons, 1.0103343554487603 embudo_1, 2.4924369189024356 embudo_2, and 2.8468611924678977 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_129 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_130 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_131 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_132 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 7ms/step - loss: 0.0058 - val_loss: 0.0427\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.2453e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 7.8568e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 5.5615e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 4.3988e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 3.5162e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.5743e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1288e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9231e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8032e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7457e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7474e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7357e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7133e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7136e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:15:43,512] Trial 41 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 391, 'embudo_1': 1.0103343554487603, 'embudo_2': 2.4924369189024356, 'embudo_3': 2.8468611924678977, 'dropout_threshold': 0.2589963094013661}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10256410256410256, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.333333333333334, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015384615384615384, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04358974358974359, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07179487179487179, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01435897435897436, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 436 neurons, 1.011757104331851 embudo_1, 1.9338338057926603 embudo_2, and 3.300408267052001 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_133 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_134 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_135 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_136 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 7ms/step - loss: 0.0059 - val_loss: 0.0429\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.0090e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 7.3453e-05 - val_loss: 0.0428\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 5.4769e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 4.4744e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 3.8245e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 3.3424e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 3.1070e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.7752e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.5407e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.3445e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1683e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.0955e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9841e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9200e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:16:10,919] Trial 42 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 436, 'embudo_1': 1.011757104331851, 'embudo_2': 1.9338338057926603, 'embudo_3': 3.300408267052001, 'dropout_threshold': 0.21546776214829996}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.1, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.025, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.042499999999999996, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06999999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.014000000000000002, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 479 neurons, 1.4233318685994776 embudo_1, 1.799821641747531 embudo_2, and 2.2740812594712674 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_137 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_138 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_139 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_140 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 7ms/step - loss: 0.0055 - val_loss: 0.0429\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.2661e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 7.3372e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 4.2013e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.8137e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.0342e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8519e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7830e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7120e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7813e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7541e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7346e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7127e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7836e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7251e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:16:38,432] Trial 43 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 479, 'embudo_1': 1.4233318685994776, 'embudo_2': 1.799821641747531, 'embudo_3': 2.2740812594712674, 'dropout_threshold': 0.27073734536353145}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0975609756097561, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(11.731707317073171, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.014634146341463414, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.041463414634146344, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06829268292682926, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013658536585365855, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 417 neurons, 1.46979327348346 embudo_1, 2.3218344998027445 embudo_2, and 2.529725677543637 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_141 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 1s 3ms/step - loss: 0.0029 - val_loss: 0.0428\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.3222e-05 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.0415e-05 - val_loss: 0.0428\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 8.4968e-05 - val_loss: 0.0428\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 1.9157e-05 - val_loss: 0.0428\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 1.8662e-05 - val_loss: 0.0428\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.2609e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 4.0670e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.2847e-05 - val_loss: 0.0429\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.5308e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.6920e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 3.4534e-05 - val_loss: 0.0428\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.7499e-05 - val_loss: 0.0428\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 5.3319e-05 - val_loss: 0.0428\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.0500e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.6019e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.3977e-05 - val_loss: 0.0428\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 4.2389e-05 - val_loss: 0.0426\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 3.3330e-05 - val_loss: 0.0428\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.4409e-05 - val_loss: 0.0427\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.8147e-05 - val_loss: 0.0427\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 4.0072e-05 - val_loss: 0.0427\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 3.0025e-05 - val_loss: 0.0427\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.8929e-05 - val_loss: 0.0427\n",
      "Epoch 25/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.6571e-05 - val_loss: 0.0427\n",
      "Epoch 26/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.5755e-05 - val_loss: 0.0426\n",
      "Epoch 27/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.3536e-05 - val_loss: 0.0427\n",
      "Epoch 28/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.1061e-05 - val_loss: 0.0426\n",
      "Epoch 29/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 3.7801e-05 - val_loss: 0.0427\n",
      "Epoch 30/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 4.1462e-05 - val_loss: 0.0428\n",
      "Epoch 31/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.5363e-05 - val_loss: 0.0427\n",
      "Epoch 32/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.1049e-05 - val_loss: 0.0426\n",
      "Epoch 33/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.4192e-05 - val_loss: 0.0427\n",
      "Epoch 34/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.8075e-05 - val_loss: 0.0425\n",
      "Epoch 35/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.4436e-05 - val_loss: 0.0427\n",
      "Epoch 36/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.5100e-05 - val_loss: 0.0429\n",
      "Epoch 37/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 3.3645e-05 - val_loss: 0.0426\n",
      "Epoch 38/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.4205e-05 - val_loss: 0.0430\n",
      "Epoch 39/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.1938e-05 - val_loss: 0.0426\n",
      "Epoch 40/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.4259e-05 - val_loss: 0.0428\n",
      "Epoch 41/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.3507e-05 - val_loss: 0.0427\n",
      "Epoch 42/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.4229e-05 - val_loss: 0.0427\n",
      "Epoch 43/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.1551e-05 - val_loss: 0.0427\n",
      "Epoch 44/200\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 2.0488e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:17:04,646] Trial 44 finished with value: 9.284959958354843 and parameters: {'n_layers': 1, 'n_neurons': 417, 'embudo_1': 1.46979327348346, 'embudo_2': 2.3218344998027445, 'embudo_3': 2.529725677543637, 'dropout_threshold': 0.18822567299015683}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09523809523809523, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(11.452380952380953, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.014285714285714285, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04047619047619048, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06666666666666667, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013333333333333334, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 352 neurons, 1.3776996489230122 embudo_1, 2.3831837953761497 embudo_2, and 1.870042385866022 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_142 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_143 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_144 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 3s 5ms/step - loss: 0.0050 - val_loss: 0.0428\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7699e-04 - val_loss: 0.0427\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 8.2193e-05 - val_loss: 0.0428\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 3.8131e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.5511e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.0605e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8520e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8188e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7604e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7555e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7232e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7973e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:17:24,679] Trial 45 finished with value: 9.284959958354843 and parameters: {'n_layers': 3, 'n_neurons': 352, 'embudo_1': 1.3776996489230122, 'embudo_2': 2.3831837953761497, 'embudo_3': 1.870042385866022, 'dropout_threshold': 0.28912542525624096}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09302325581395349, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(11.186046511627907, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013953488372093023, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03953488372093023, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06511627906976744, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013023255813953489, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 444 neurons, 1.325350463699679 embudo_1, 2.2310981257521108 embudo_2, and 2.053814679001564 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_145 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_146 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_147 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_148 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 7ms/step - loss: 0.0054 - val_loss: 0.0428\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 9.5814e-05 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 6.5988e-05 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 4.4157e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 3.1985e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.3491e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9790e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8899e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7763e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7621e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.6846e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7076e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.6519e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7052e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.6930e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.6384e-05 - val_loss: 0.0427\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7131e-05 - val_loss: 0.0427\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8327e-05 - val_loss: 0.0427\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7303e-05 - val_loss: 0.0427\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7969e-05 - val_loss: 0.0427\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7109e-05 - val_loss: 0.0427\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8238e-05 - val_loss: 0.0427\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8313e-05 - val_loss: 0.0427\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.8417e-05 - val_loss: 0.0427\n",
      "Epoch 25/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7732e-05 - val_loss: 0.0427\n",
      "Epoch 26/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7773e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:18:03,573] Trial 46 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 444, 'embudo_1': 1.325350463699679, 'embudo_2': 2.2310981257521108, 'embudo_3': 2.053814679001564, 'dropout_threshold': 0.2098409807155821}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09090909090909091, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(10.931818181818182, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013636363636363636, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.038636363636363635, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06363636363636363, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.012727272727272728, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 400 neurons, 1.4329674060605295 embudo_1, 2.136326693929673 embudo_2, and 2.6998609590722107 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_149 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_150 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_151 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 3s 6ms/step - loss: 0.0045 - val_loss: 0.0430\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.1392e-04 - val_loss: 0.0429\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 8.5373e-05 - val_loss: 0.0429\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 5.6267e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 4.6047e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 3.9610e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 3.1369e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 3.1324e-05 - val_loss: 0.0428\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.8651e-05 - val_loss: 0.0428\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.6145e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.4975e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.2588e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.2741e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 2.0064e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.0919e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.8614e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:18:26,921] Trial 47 finished with value: 9.284959958354843 and parameters: {'n_layers': 3, 'n_neurons': 400, 'embudo_1': 1.4329674060605295, 'embudo_2': 2.136326693929673, 'embudo_3': 2.6998609590722107, 'dropout_threshold': 0.12788793736242765}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08888888888888889, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(10.688888888888888, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013333333333333332, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03777777777777778, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06222222222222222, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.012444444444444445, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 245 neurons, 1.380795542479611 embudo_1, 1.3840065336686487 embudo_2, and 1.813330674284091 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_152 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_153 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_154 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_155 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 4s 7ms/step - loss: 0.0074 - val_loss: 0.0428\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.0600e-04 - val_loss: 0.0428\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.2597e-04 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 8.3288e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 6.1806e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 4.5732e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 3.7395e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.8843e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 2.2568e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.9216e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.7417e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.7287e-05 - val_loss: 0.0427\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.6517e-05 - val_loss: 0.0427\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.6684e-05 - val_loss: 0.0427\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.6727e-05 - val_loss: 0.0427\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 1.6379e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n",
      "Predicting 201904 for product 1.714485974015015 (261/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:18:54,317] Trial 48 finished with value: 9.284959958354843 and parameters: {'n_layers': 4, 'n_neurons': 245, 'embudo_1': 1.380795542479611, 'embudo_2': 1.3840065336686487, 'embudo_3': 1.813330674284091, 'dropout_threshold': 0.2637575879209435}. Best is trial 0 with value: 9.284959958354843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08695652173913043, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(10.456521739130435, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013043478260869565, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03695652173913044, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0608695652173913, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.012173913043478262, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 75 neurons, 1.4726967252066978 embudo_1, 2.246969259000819 embudo_2, and 1.995097587848375 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_156 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_157 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_158 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "191/191 [==============================] - 3s 5ms/step - loss: 0.0126 - val_loss: 0.0430\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 3.9906e-04 - val_loss: 0.0427\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.3038e-04 - val_loss: 0.0427\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 3.8550e-05 - val_loss: 0.0427\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.8460e-05 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7587e-05 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 1s 5ms/step - loss: 1.6867e-05 - val_loss: 0.0427\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7074e-05 - val_loss: 0.0427\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.7118e-05 - val_loss: 0.0427\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6955e-05 - val_loss: 0.0427\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6782e-05 - val_loss: 0.0427\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 1.6899e-05 - val_loss: 0.0427\n",
      "Predicting 201904 for product -1.646914715022384 (1/265))\n",
      "Predicting 201904 for product -1.3936767566888766 (2/265))\n",
      "Predicting 201904 for product -1.268390608881773 (3/265))\n",
      "Predicting 201904 for product -1.1884207273027705 (4/265))\n",
      "Predicting 201904 for product -1.1724267509869701 (5/265))\n",
      "Predicting 201904 for product -1.1644297628290698 (6/265))\n",
      "Predicting 201904 for product -1.103119520285168 (7/265))\n",
      "Predicting 201904 for product -1.0471406031798662 (8/265))\n",
      "Predicting 201904 for product -1.036477952302666 (9/265))\n",
      "Predicting 201904 for product -0.9884960233552647 (10/265))\n",
      "Predicting 201904 for product -0.9698363843201641 (11/265))\n",
      "Predicting 201904 for product -0.7859056566884587 (12/265))\n",
      "Predicting 201904 for product -0.6286315562497541 (13/265))\n",
      "Predicting 201904 for product -0.5699869764251523 (14/265))\n",
      "Predicting 201904 for product -0.5673213137058523 (15/265))\n",
      "Predicting 201904 for product -0.559324325547952 (16/265))\n",
      "Predicting 201904 for product -0.5539930001093518 (17/265))\n",
      "Predicting 201904 for product -0.5113423966005506 (18/265))\n",
      "Predicting 201904 for product -0.4846857694075498 (19/265))\n",
      "Predicting 201904 for product -0.4633604676531492 (20/265))\n",
      "Predicting 201904 for product -0.4340381777408483 (21/265))\n",
      "Predicting 201904 for product -0.423375526863648 (22/265))\n",
      "Predicting 201904 for product -0.3727279351969465 (23/265))\n",
      "Predicting 201904 for product -0.35939962160044614 (24/265))\n",
      "Predicting 201904 for product -0.3434056452846457 (25/265))\n",
      "Predicting 201904 for product -0.33274299440744537 (26/265))\n",
      "Predicting 201904 for product -0.3300773316881453 (27/265))\n",
      "Predicting 201904 for product -0.31674901809164485 (28/265))\n",
      "Predicting 201904 for product -0.3007550417758444 (29/265))\n",
      "Predicting 201904 for product -0.27143275186354354 (30/265))\n",
      "Predicting 201904 for product -0.24744178738984285 (31/265))\n",
      "Predicting 201904 for product -0.23411347379334244 (32/265))\n",
      "Predicting 201904 for product -0.19945985844244143 (33/265))\n",
      "Predicting 201904 for product -0.19412853300384128 (34/265))\n",
      "Predicting 201904 for product -0.1541435922143401 (35/265))\n",
      "Predicting 201904 for product -0.15147792949504002 (36/265))\n",
      "Predicting 201904 for product -0.12482130230203924 (37/265))\n",
      "Predicting 201904 for product -0.0928333496704383 (38/265))\n",
      "Predicting 201904 for product -0.0795050360739379 (39/265))\n",
      "Predicting 201904 for product -0.07683937335463782 (40/265))\n",
      "Predicting 201904 for product -0.07150804791603767 (41/265))\n",
      "Predicting 201904 for product -0.06351105975813744 (42/265))\n",
      "Predicting 201904 for product -0.05018274616163704 (43/265))\n",
      "Predicting 201904 for product -0.04485142072303688 (44/265))\n",
      "Predicting 201904 for product -0.0022008172142356304 (45/265))\n",
      "Predicting 201904 for product 0.024455809978765154 (46/265))\n",
      "Predicting 201904 for product 0.032452798136665385 (47/265))\n",
      "Predicting 201904 for product 0.04844677445246586 (48/265))\n",
      "Predicting 201904 for product 0.05644376261036609 (49/265))\n",
      "Predicting 201904 for product 0.06177508804896625 (50/265))\n",
      "Predicting 201904 for product 0.06444075076826633 (51/265))\n",
      "Predicting 201904 for product 0.08310038980336688 (52/265))\n",
      "Predicting 201904 for product 0.09642870339986727 (53/265))\n",
      "Predicting 201904 for product 0.10176002883846742 (54/265))\n",
      "Predicting 201904 for product 0.10975701699636765 (55/265))\n",
      "Predicting 201904 for product 0.1177540051542679 (56/265))\n",
      "Predicting 201904 for product 0.12041966787356798 (57/265))\n",
      "Predicting 201904 for product 0.1417449696279686 (58/265))\n",
      "Predicting 201904 for product 0.16307027138236924 (59/265))\n",
      "Predicting 201904 for product 0.18439557313676985 (60/265))\n",
      "Predicting 201904 for product 0.18706123585606993 (61/265))\n",
      "Predicting 201904 for product 0.19505822401397016 (62/265))\n",
      "Predicting 201904 for product 0.20838653761047055 (63/265))\n",
      "Predicting 201904 for product 0.21904918848767088 (64/265))\n",
      "Predicting 201904 for product 0.22438051392627104 (65/265))\n",
      "Predicting 201904 for product 0.2403744902420715 (66/265))\n",
      "Predicting 201904 for product 0.256368466557872 (67/265))\n",
      "Predicting 201904 for product 0.2750281055929725 (68/265))\n",
      "Predicting 201904 for product 0.2776937683122726 (69/265))\n",
      "Predicting 201904 for product 0.28569075647017284 (70/265))\n",
      "Predicting 201904 for product 0.29368774462807307 (71/265))\n",
      "Predicting 201904 for product 0.2990190700666732 (72/265))\n",
      "Predicting 201904 for product 0.3043503955052734 (73/265))\n",
      "Predicting 201904 for product 0.32034437182107384 (74/265))\n",
      "Predicting 201904 for product 0.325675697259674 (75/265))\n",
      "Predicting 201904 for product 0.33100702269827414 (76/265))\n",
      "Predicting 201904 for product 0.34433533629477453 (77/265))\n",
      "Predicting 201904 for product 0.36032931261057505 (78/265))\n",
      "Predicting 201904 for product 0.3789889516456756 (79/265))\n",
      "Predicting 201904 for product 0.3869859398035758 (80/265))\n",
      "Predicting 201904 for product 0.3976485906807761 (81/265))\n",
      "Predicting 201904 for product 0.44029919418957736 (82/265))\n",
      "Predicting 201904 for product 0.4589588332246779 (83/265))\n",
      "Predicting 201904 for product 0.47228714682117834 (84/265))\n",
      "Predicting 201904 for product 0.4909467858562789 (85/265))\n",
      "Predicting 201904 for product 0.5122720876106794 (86/265))\n",
      "Predicting 201904 for product 0.5202690757685797 (87/265))\n",
      "Predicting 201904 for product 0.5442600402422805 (88/265))\n",
      "Predicting 201904 for product 0.5575883538387808 (89/265))\n",
      "Predicting 201904 for product 0.5789136555931814 (90/265))\n",
      "Predicting 201904 for product 0.5895763064703817 (91/265))\n",
      "Predicting 201904 for product 0.600238957347582 (92/265))\n",
      "Predicting 201904 for product 0.6242299218212828 (93/265))\n",
      "Predicting 201904 for product 0.6295612472598829 (94/265))\n",
      "Predicting 201904 for product 0.6348925726984831 (95/265))\n",
      "Predicting 201904 for product 0.6375582354177832 (96/265))\n",
      "Predicting 201904 for product 0.6535522117335837 (97/265))\n",
      "Predicting 201904 for product 0.6562178744528837 (98/265))\n",
      "Predicting 201904 for product 0.664214862610784 (99/265))\n",
      "Predicting 201904 for product 0.666880525330084 (100/265))\n",
      "Predicting 201904 for product 0.6775431762072843 (101/265))\n",
      "Predicting 201904 for product 0.6855401643651846 (102/265))\n",
      "Predicting 201904 for product 0.6935371525230848 (103/265))\n",
      "Predicting 201904 for product 0.6962028152423849 (104/265))\n",
      "Predicting 201904 for product 0.6988684779616849 (105/265))\n",
      "Predicting 201904 for product 0.7068654661195852 (106/265))\n",
      "Predicting 201904 for product 0.7095311288388852 (107/265))\n",
      "Predicting 201904 for product 0.7201937797160856 (108/265))\n",
      "Predicting 201904 for product 0.7228594424353857 (109/265))\n",
      "Predicting 201904 for product 0.7281907678739858 (110/265))\n",
      "Predicting 201904 for product 0.7308564305932859 (111/265))\n",
      "Predicting 201904 for product 0.736187756031886 (112/265))\n",
      "Predicting 201904 for product 0.7388534187511862 (113/265))\n",
      "Predicting 201904 for product 0.7415190814704862 (114/265))\n",
      "Predicting 201904 for product 0.7441847441897863 (115/265))\n",
      "Predicting 201904 for product 0.7521817323476866 (116/265))\n",
      "Predicting 201904 for product 0.7548473950669866 (117/265))\n",
      "Predicting 201904 for product 0.7575130577862867 (118/265))\n",
      "Predicting 201904 for product 0.7735070341020872 (119/265))\n",
      "Predicting 201904 for product 0.7788383595406874 (120/265))\n",
      "Predicting 201904 for product 0.7895010104178877 (121/265))\n",
      "Predicting 201904 for product 0.800163661295088 (122/265))\n",
      "Predicting 201904 for product 0.8081606494529882 (123/265))\n",
      "Predicting 201904 for product 0.8214889630494886 (124/265))\n",
      "Predicting 201904 for product 0.8321516139266889 (125/265))\n",
      "Predicting 201904 for product 0.8374829393652891 (126/265))\n",
      "Predicting 201904 for product 0.8428142648038892 (127/265))\n",
      "Predicting 201904 for product 0.8481455902424894 (128/265))\n",
      "Predicting 201904 for product 0.8588082411196897 (129/265))\n",
      "Predicting 201904 for product 0.86680522927759 (130/265))\n",
      "Predicting 201904 for product 0.8801335428740903 (131/265))\n",
      "Predicting 201904 for product 0.8987931819091909 (132/265))\n",
      "Predicting 201904 for product 0.9121214955056912 (133/265))\n",
      "Predicting 201904 for product 0.9147871582249913 (134/265))\n",
      "Predicting 201904 for product 0.9201184836635915 (135/265))\n",
      "Predicting 201904 for product 0.9334467972600918 (136/265))\n",
      "Predicting 201904 for product 0.9441094481372921 (137/265))\n",
      "Predicting 201904 for product 0.9494407735758923 (138/265))\n",
      "Predicting 201904 for product 0.9521064362951924 (139/265))\n",
      "Predicting 201904 for product 0.9654347498916928 (140/265))\n",
      "Predicting 201904 for product 0.9787630634881932 (141/265))\n",
      "Predicting 201904 for product 0.9814287262074933 (142/265))\n",
      "Predicting 201904 for product 1.0000883652425938 (143/265))\n",
      "Predicting 201904 for product 1.0027540279618938 (144/265))\n",
      "Predicting 201904 for product 1.0214136669969944 (145/265))\n",
      "Predicting 201904 for product 1.0267449924355947 (146/265))\n",
      "Predicting 201904 for product 1.037407643312795 (147/265))\n",
      "Predicting 201904 for product 1.045404631470695 (148/265))\n",
      "Predicting 201904 for product 1.0507359569092953 (149/265))\n",
      "Predicting 201904 for product 1.072061258663696 (150/265))\n",
      "Predicting 201904 for product 1.074726921382996 (151/265))\n",
      "Predicting 201904 for product 1.077392584102296 (152/265))\n",
      "Predicting 201904 for product 1.0880552349794965 (153/265))\n",
      "Predicting 201904 for product 1.0933865604180966 (154/265))\n",
      "Predicting 201904 for product 1.0960522231373966 (155/265))\n",
      "Predicting 201904 for product 1.1040492112952969 (156/265))\n",
      "Predicting 201904 for product 1.1253745130496975 (157/265))\n",
      "Predicting 201904 for product 1.1280401757689975 (158/265))\n",
      "Predicting 201904 for product 1.1307058384882978 (159/265))\n",
      "Predicting 201904 for product 1.1333715012075978 (160/265))\n",
      "Predicting 201904 for product 1.141368489365498 (161/265))\n",
      "Predicting 201904 for product 1.144034152084798 (162/265))\n",
      "Predicting 201904 for product 1.1626937911198987 (163/265))\n",
      "Predicting 201904 for product 1.1653594538391987 (164/265))\n",
      "Predicting 201904 for product 1.1706907792777987 (165/265))\n",
      "Predicting 201904 for product 1.178687767435699 (166/265))\n",
      "Predicting 201904 for product 1.1920160810321996 (167/265))\n",
      "Predicting 201904 for product 1.1973474064707996 (168/265))\n",
      "Predicting 201904 for product 1.2026787319094 (169/265))\n",
      "Predicting 201904 for product 1.208010057348 (170/265))\n",
      "Predicting 201904 for product 1.2106757200673 (171/265))\n",
      "Predicting 201904 for product 1.2160070455059002 (172/265))\n",
      "Predicting 201904 for product 1.2186727082252002 (173/265))\n",
      "Predicting 201904 for product 1.2213383709445003 (174/265))\n",
      "Predicting 201904 for product 1.2240040336638005 (175/265))\n",
      "Predicting 201904 for product 1.2266696963831005 (176/265))\n",
      "Predicting 201904 for product 1.2293353591024005 (177/265))\n",
      "Predicting 201904 for product 1.2320010218217006 (178/265))\n",
      "Predicting 201904 for product 1.2373323472603008 (179/265))\n",
      "Predicting 201904 for product 1.2399980099796009 (180/265))\n",
      "Predicting 201904 for product 1.2506606608568012 (181/265))\n",
      "Predicting 201904 for product 1.2559919862954014 (182/265))\n",
      "Predicting 201904 for product 1.2613233117340015 (183/265))\n",
      "Predicting 201904 for product 1.2719859626112018 (184/265))\n",
      "Predicting 201904 for product 1.2746516253305018 (185/265))\n",
      "Predicting 201904 for product 1.277317288049802 (186/265))\n",
      "Predicting 201904 for product 1.2853142762077021 (187/265))\n",
      "Predicting 201904 for product 1.2906456016463024 (188/265))\n",
      "Predicting 201904 for product 1.2933112643656024 (189/265))\n",
      "Predicting 201904 for product 1.2959769270849024 (190/265))\n",
      "Predicting 201904 for product 1.2986425898042027 (191/265))\n",
      "Predicting 201904 for product 1.3013082525235027 (192/265))\n",
      "Predicting 201904 for product 1.306639577962103 (193/265))\n",
      "Predicting 201904 for product 1.309305240681403 (194/265))\n",
      "Predicting 201904 for product 1.3173022288393033 (195/265))\n",
      "Predicting 201904 for product 1.3199678915586033 (196/265))\n",
      "Predicting 201904 for product 1.3226335542779033 (197/265))\n",
      "Predicting 201904 for product 1.3252992169972033 (198/265))\n",
      "Predicting 201904 for product 1.3332962051551036 (199/265))\n",
      "Predicting 201904 for product 1.341293193313004 (200/265))\n",
      "Predicting 201904 for product 1.343958856032304 (201/265))\n",
      "Predicting 201904 for product 1.3572871696288042 (202/265))\n",
      "Predicting 201904 for product 1.3679498205060046 (203/265))\n",
      "Predicting 201904 for product 1.3759468086639048 (204/265))\n",
      "Predicting 201904 for product 1.381278134102505 (205/265))\n",
      "Predicting 201904 for product 1.3839437968218051 (206/265))\n",
      "Predicting 201904 for product 1.3892751222604052 (207/265))\n",
      "Predicting 201904 for product 1.3946064476990054 (208/265))\n",
      "Predicting 201904 for product 1.3972721104183055 (209/265))\n",
      "Predicting 201904 for product 1.3999377731376055 (210/265))\n",
      "Predicting 201904 for product 1.4052690985762057 (211/265))\n",
      "Predicting 201904 for product 1.4106004240148058 (212/265))\n",
      "Predicting 201904 for product 1.4212630748920063 (213/265))\n",
      "Predicting 201904 for product 1.4292600630499064 (214/265))\n",
      "Predicting 201904 for product 1.4319257257692066 (215/265))\n",
      "Predicting 201904 for product 1.4559166902429073 (216/265))\n",
      "Predicting 201904 for product 1.4585823529622073 (217/265))\n",
      "Predicting 201904 for product 1.4772419919973079 (218/265))\n",
      "Predicting 201904 for product 1.479907654716608 (219/265))\n",
      "Predicting 201904 for product 1.4959016310324085 (220/265))\n",
      "Predicting 201904 for product 1.5038986191903085 (221/265))\n",
      "Predicting 201904 for product 1.5092299446289088 (222/265))\n",
      "Predicting 201904 for product 1.517226932786809 (223/265))\n",
      "Predicting 201904 for product 1.5198925955061091 (224/265))\n",
      "Predicting 201904 for product 1.5225582582254091 (225/265))\n",
      "Predicting 201904 for product 1.5385522345412097 (226/265))\n",
      "Predicting 201904 for product 1.5412178972605097 (227/265))\n",
      "Predicting 201904 for product 1.54921488541841 (228/265))\n",
      "Predicting 201904 for product 1.5572118735763103 (229/265))\n",
      "Predicting 201904 for product 1.5598775362956103 (230/265))\n",
      "Predicting 201904 for product 1.5625431990149103 (231/265))\n",
      "Predicting 201904 for product 1.5652088617342104 (232/265))\n",
      "Predicting 201904 for product 1.5705401871728106 (233/265))\n",
      "Predicting 201904 for product 1.578537175330711 (234/265))\n",
      "Predicting 201904 for product 1.581202838050011 (235/265))\n",
      "Predicting 201904 for product 1.5918654889272112 (236/265))\n",
      "Predicting 201904 for product 1.5998624770851115 (237/265))\n",
      "Predicting 201904 for product 1.6025281398044116 (238/265))\n",
      "Predicting 201904 for product 1.6051938025237116 (239/265))\n",
      "Predicting 201904 for product 1.6105251279623118 (240/265))\n",
      "Predicting 201904 for product 1.6211877788395122 (241/265))\n",
      "Predicting 201904 for product 1.6371817551553125 (242/265))\n",
      "Predicting 201904 for product 1.6398474178746127 (243/265))\n",
      "Predicting 201904 for product 1.6425130805939128 (244/265))\n",
      "Predicting 201904 for product 1.6451787433132128 (245/265))\n",
      "Predicting 201904 for product 1.650510068751813 (246/265))\n",
      "Predicting 201904 for product 1.653175731471113 (247/265))\n",
      "Predicting 201904 for product 1.655841394190413 (248/265))\n",
      "Predicting 201904 for product 1.6691697077869134 (249/265))\n",
      "Predicting 201904 for product 1.6745010332255137 (250/265))\n",
      "Predicting 201904 for product 1.6771666959448137 (251/265))\n",
      "Predicting 201904 for product 1.6798323586641137 (252/265))\n",
      "Predicting 201904 for product 1.682498021383414 (253/265))\n",
      "Predicting 201904 for product 1.687829346822014 (254/265))\n",
      "Predicting 201904 for product 1.690495009541314 (255/265))\n",
      "Predicting 201904 for product 1.7011576604185144 (256/265))\n",
      "Predicting 201904 for product 1.7038233231378146 (257/265))\n",
      "Predicting 201904 for product 1.7064889858571146 (258/265))\n",
      "Predicting 201904 for product 1.7091546485764146 (259/265))\n",
      "Predicting 201904 for product 1.7118203112957147 (260/265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:19:13,676] Trial 49 finished with value: 9.284959958354843 and parameters: {'n_layers': 3, 'n_neurons': 75, 'embudo_1': 1.4726967252066978, 'embudo_2': 2.246969259000819, 'embudo_3': 1.995097587848375, 'dropout_threshold': 0.4129138283492553}. Best is trial 0 with value: 9.284959958354843.\n",
      "[I 2023-11-25 22:19:13,677] A new study created in memory with name: lstm_10_BO_0.7677485281160644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.714485974015015 (261/265))\n",
      "Predicting 201904 for product 1.717151636734315 (262/265))\n",
      "Predicting 201904 for product 1.722482962172915 (263/265))\n",
      "Predicting 201904 for product 1.7278142876115152 (264/265))\n",
      "Predicting 201904 for product 1.7304799503308153 (265/265))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "Best trial for cluster-0.020059937789255886:\n",
      "n_layers: 3\n",
      "n_neurons: 434\n",
      "embudo_1: 1.4262970651291207\n",
      "embudo_2: 2.4372122423922833\n",
      "embudo_3: 1.226984633715104\n",
      "dropout_threshold: 0.4535291792003888\n",
      "Finished training for cluster -0.020059937789255886\n",
      "Training for cluster 0.7677485281160644\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 181 neurons, 0.9302597465269228 embudo_1, 1.695237808348136 embudo_2, and 3.095988859186442 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_159 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 1s 3ms/step - loss: 0.0469 - val_loss: 0.0691\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0310 - val_loss: 0.0581\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0563\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0645\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0550\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0541\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0639\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0577\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0535\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0527\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0512\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0603\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0588\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0527\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0536\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0566\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0536\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0589\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0626\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0549\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0512\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0526\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0541\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0539\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0565\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0535\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0557\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0531\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0594\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0554\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0554\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:19:26,089] Trial 0 finished with value: 1.36310824363451 and parameters: {'n_layers': 1, 'n_neurons': 181, 'embudo_1': 0.9302597465269228, 'embudo_2': 1.695237808348136, 'embudo_3': 3.095988859186442, 'dropout_threshold': 0.5332946472143261}. Best is trial 0 with value: 1.36310824363451.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 186 neurons, 0.9284312967107964 embudo_1, 2.0666109829230384 embudo_2, and 3.180396754970398 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_160 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_161 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_162 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 3s 6ms/step - loss: 0.0751 - val_loss: 0.1092\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0399 - val_loss: 0.0842\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0325 - val_loss: 0.0650\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0334 - val_loss: 0.0628\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0305 - val_loss: 0.0585\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0318 - val_loss: 0.0607\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0322 - val_loss: 0.0523\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0311 - val_loss: 0.0597\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0299 - val_loss: 0.0477\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0443\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0281 - val_loss: 0.0482\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0447\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0309 - val_loss: 0.0510\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0269 - val_loss: 0.0432\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0439\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0270 - val_loss: 0.0482\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0489\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0262 - val_loss: 0.0455\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0256 - val_loss: 0.0454\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0264 - val_loss: 0.0451\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0260 - val_loss: 0.0433\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0258 - val_loss: 0.0429\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0263 - val_loss: 0.0426\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0249 - val_loss: 0.0476\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0253 - val_loss: 0.0450\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0404\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0246 - val_loss: 0.0629\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0263 - val_loss: 0.0459\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0251 - val_loss: 0.0471\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0241 - val_loss: 0.0438\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0238 - val_loss: 0.0471\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0269 - val_loss: 0.0457\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0243 - val_loss: 0.0459\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0232 - val_loss: 0.0464\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0237 - val_loss: 0.0513\n",
      "Epoch 36/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0234 - val_loss: 0.0500\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:19:49,461] Trial 1 finished with value: 1.3614480180967585 and parameters: {'n_layers': 3, 'n_neurons': 186, 'embudo_1': 0.9284312967107964, 'embudo_2': 2.0666109829230384, 'embudo_3': 3.180396754970398, 'dropout_threshold': 0.10570052181456355}. Best is trial 1 with value: 1.3614480180967585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 477 neurons, 1.4208492433386661 embudo_1, 1.3357478854141753 embudo_2, and 3.2505835931107887 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_163 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_164 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_165 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 3s 7ms/step - loss: 0.0695 - val_loss: 0.0926\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0377 - val_loss: 0.0630\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0348 - val_loss: 0.0552\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0323 - val_loss: 0.0635\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0331 - val_loss: 0.0536\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0292 - val_loss: 0.0556\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0308 - val_loss: 0.0455\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0284 - val_loss: 0.0516\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0289 - val_loss: 0.0415\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0295 - val_loss: 0.0411\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0267 - val_loss: 0.0481\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0284 - val_loss: 0.0468\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0275 - val_loss: 0.0435\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0285 - val_loss: 0.0425\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0267 - val_loss: 0.0468\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0274 - val_loss: 0.0511\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0253 - val_loss: 0.0428\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0255 - val_loss: 0.0446\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0500\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0260 - val_loss: 0.0607\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:20:06,552] Trial 2 finished with value: 1.3717086367777604 and parameters: {'n_layers': 3, 'n_neurons': 477, 'embudo_1': 1.4208492433386661, 'embudo_2': 1.3357478854141753, 'embudo_3': 3.2505835931107887, 'dropout_threshold': 0.2679879438354674}. Best is trial 1 with value: 1.3614480180967585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 373 neurons, 1.0628910994050693 embudo_1, 1.4642778825378038 embudo_2, and 3.0248730139257427 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_166 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_167 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_168 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 3s 6ms/step - loss: 0.0618 - val_loss: 0.0736\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0353 - val_loss: 0.0555\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0348 - val_loss: 0.0558\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0321 - val_loss: 0.0571\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0321 - val_loss: 0.0505\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0313 - val_loss: 0.0506\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0638\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0310 - val_loss: 0.0454\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0323 - val_loss: 0.0460\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0310 - val_loss: 0.0472\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0293 - val_loss: 0.0518\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0295 - val_loss: 0.0464\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0305 - val_loss: 0.0463\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0303 - val_loss: 0.0561\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0285 - val_loss: 0.0452\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0451\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0274 - val_loss: 0.0472\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0272 - val_loss: 0.0468\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0264 - val_loss: 0.0476\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0269 - val_loss: 0.0477\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0262 - val_loss: 0.0494\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0458\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0263 - val_loss: 0.0572\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0285 - val_loss: 0.0472\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0264 - val_loss: 0.0526\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0272 - val_loss: 0.0490\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:20:26,135] Trial 3 finished with value: 1.3707643525228008 and parameters: {'n_layers': 3, 'n_neurons': 373, 'embudo_1': 1.0628910994050693, 'embudo_2': 1.4642778825378038, 'embudo_3': 3.0248730139257427, 'dropout_threshold': 0.3202300751049366}. Best is trial 1 with value: 1.3614480180967585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 338 neurons, 1.1216395437834237 embudo_1, 2.146462936762466 embudo_2, and 3.2385158089685664 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_169 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_170 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_171 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_172 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 3s 8ms/step - loss: 0.0820 - val_loss: 0.1325\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0622 - val_loss: 0.1033\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0507 - val_loss: 0.0662\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0422 - val_loss: 0.0575\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0356 - val_loss: 0.0559\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0398 - val_loss: 0.0469\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0420 - val_loss: 0.0516\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0360 - val_loss: 0.0528\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0363 - val_loss: 0.0514\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0348 - val_loss: 0.0593\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0361 - val_loss: 0.0500\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0328 - val_loss: 0.0459\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0333 - val_loss: 0.0475\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0330 - val_loss: 0.0492\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0322 - val_loss: 0.0476\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0325 - val_loss: 0.0479\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0309 - val_loss: 0.0504\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0344 - val_loss: 0.0453\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0325 - val_loss: 0.0463\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0344 - val_loss: 0.0465\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0318 - val_loss: 0.0503\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0330 - val_loss: 0.0449\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0342 - val_loss: 0.0514\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0321 - val_loss: 0.0554\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0316 - val_loss: 0.0466\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0312 - val_loss: 0.0505\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0318 - val_loss: 0.0449\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0302 - val_loss: 0.0454\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0297 - val_loss: 0.0481\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0292 - val_loss: 0.0458\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0321 - val_loss: 0.0516\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0314 - val_loss: 0.0493\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:20:52,871] Trial 4 finished with value: 1.3788169601067892 and parameters: {'n_layers': 4, 'n_neurons': 338, 'embudo_1': 1.1216395437834237, 'embudo_2': 2.146462936762466, 'embudo_3': 3.2385158089685664, 'dropout_threshold': 0.6268679639058569}. Best is trial 1 with value: 1.3614480180967585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 185 neurons, 1.3720413304076164 embudo_1, 1.2584844023243082 embudo_2, and 1.8942797216640348 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_173 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_174 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_175 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 3s 7ms/step - loss: 0.0619 - val_loss: 0.0800\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0363 - val_loss: 0.0803\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0333 - val_loss: 0.0567\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0351 - val_loss: 0.0572\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0303 - val_loss: 0.0508\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0286 - val_loss: 0.0492\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0307 - val_loss: 0.0531\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0461\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0301 - val_loss: 0.0444\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0277 - val_loss: 0.0465\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0279 - val_loss: 0.0445\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0270 - val_loss: 0.0449\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0507\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0468\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0451\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0547\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0270 - val_loss: 0.0470\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0269 - val_loss: 0.0462\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0256 - val_loss: 0.0492\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:21:08,925] Trial 5 finished with value: 1.3729755053686594 and parameters: {'n_layers': 3, 'n_neurons': 185, 'embudo_1': 1.3720413304076164, 'embudo_2': 1.2584844023243082, 'embudo_3': 1.8942797216640348, 'dropout_threshold': 0.16640242261604446}. Best is trial 1 with value: 1.3614480180967585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 84 neurons, 1.151162810284589 embudo_1, 2.3307326654784832 embudo_2, and 2.587481932288342 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_176 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 1s 3ms/step - loss: 0.0493 - val_loss: 0.0747\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 0.0672\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0588\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0637\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0558\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0587\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0549\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0559\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0543\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0570\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0574\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0581\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0545\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0565\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0562\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0578\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0523\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0558\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0529\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0252 - val_loss: 0.0539\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0527\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0505\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0525\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0528\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0533\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0508\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0513\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0515\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0550\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0538\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0564\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0538\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:21:21,195] Trial 6 finished with value: 1.3639367716519808 and parameters: {'n_layers': 1, 'n_neurons': 84, 'embudo_1': 1.151162810284589, 'embudo_2': 2.3307326654784832, 'embudo_3': 2.587481932288342, 'dropout_threshold': 0.6037903227678059}. Best is trial 1 with value: 1.3614480180967585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 187 neurons, 1.1077856792970437 embudo_1, 1.8752256569433718 embudo_2, and 2.2935439897501455 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_177 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_178 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 5ms/step - loss: 0.0669 - val_loss: 0.0756\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.0754\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.0644\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0619\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0351 - val_loss: 0.0645\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0302 - val_loss: 0.0534\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0315 - val_loss: 0.0575\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0541\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 0.0513\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0309 - val_loss: 0.0543\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0505\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0519\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0539\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0553\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0507\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0528\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0310 - val_loss: 0.0537\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0481\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0503\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0281 - val_loss: 0.0507\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0505\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0267 - val_loss: 0.0489\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0463\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0473\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0478\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0482\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.0485\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0548\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0450\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.0495\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0521\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0515\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0520\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0473\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0538\n",
      "Epoch 36/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0463\n",
      "Epoch 37/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0492\n",
      "Epoch 38/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0521\n",
      "Epoch 39/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0564\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:21:41,247] Trial 7 finished with value: 1.3652944872967239 and parameters: {'n_layers': 2, 'n_neurons': 187, 'embudo_1': 1.1077856792970437, 'embudo_2': 1.8752256569433718, 'embudo_3': 2.2935439897501455, 'dropout_threshold': 0.4846816369102678}. Best is trial 1 with value: 1.3614480180967585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 247 neurons, 1.4645429805563035 embudo_1, 0.8775165816896016 embudo_2, and 3.359355914746616 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_179 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_180 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_181 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_182 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 4s 8ms/step - loss: 0.0757 - val_loss: 0.1022\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0411 - val_loss: 0.0660\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0361 - val_loss: 0.0574\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0330 - val_loss: 0.0597\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0317 - val_loss: 0.0522\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0322 - val_loss: 0.0567\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0312 - val_loss: 0.0457\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0278 - val_loss: 0.0519\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0307 - val_loss: 0.0476\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0278 - val_loss: 0.0588\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0297 - val_loss: 0.0403\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0451\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0278 - val_loss: 0.0472\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0274 - val_loss: 0.0403\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0257 - val_loss: 0.0442\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0300 - val_loss: 0.0439\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0266 - val_loss: 0.0521\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0280 - val_loss: 0.0443\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0271 - val_loss: 0.0399\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0254 - val_loss: 0.0447\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0258 - val_loss: 0.0524\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0253 - val_loss: 0.0501\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0260 - val_loss: 0.0441\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0260 - val_loss: 0.0430\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0278 - val_loss: 0.0453\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0244 - val_loss: 0.0449\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0570\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0253 - val_loss: 0.0467\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0237 - val_loss: 0.0528\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:22:06,675] Trial 8 finished with value: 1.3684206078842451 and parameters: {'n_layers': 4, 'n_neurons': 247, 'embudo_1': 1.4645429805563035, 'embudo_2': 0.8775165816896016, 'embudo_3': 3.359355914746616, 'dropout_threshold': 0.12270000410288634}. Best is trial 1 with value: 1.3614480180967585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 24 neurons, 1.0965242549273062 embudo_1, 1.5596114467948916 embudo_2, and 3.337523963689206 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_183 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_184 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_185 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 3s 7ms/step - loss: 0.0940 - val_loss: 0.1531\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0619 - val_loss: 0.1075\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0492 - val_loss: 0.0865\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0500 - val_loss: 0.0754\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0472 - val_loss: 0.0766\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0441 - val_loss: 0.0764\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0436 - val_loss: 0.0771\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0411 - val_loss: 0.0720\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0430 - val_loss: 0.0716\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0396 - val_loss: 0.0715\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0406 - val_loss: 0.0708\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0359 - val_loss: 0.0658\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0365 - val_loss: 0.0677\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0377 - val_loss: 0.0633\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0638\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0370 - val_loss: 0.0671\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0370 - val_loss: 0.0684\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0354 - val_loss: 0.0626\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0318 - val_loss: 0.0643\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0315 - val_loss: 0.0569\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0330 - val_loss: 0.0626\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0325 - val_loss: 0.0684\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0331 - val_loss: 0.0598\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0358 - val_loss: 0.0622\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0311 - val_loss: 0.0681\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0327 - val_loss: 0.0642\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0364 - val_loss: 0.0594\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0348 - val_loss: 0.0652\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0351 - val_loss: 0.0611\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0359 - val_loss: 0.0620\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:22:27,972] Trial 9 finished with value: 1.387462956879546 and parameters: {'n_layers': 3, 'n_neurons': 24, 'embudo_1': 1.0965242549273062, 'embudo_2': 1.5596114467948916, 'embudo_3': 3.337523963689206, 'dropout_threshold': 0.2645107535333471}. Best is trial 1 with value: 1.3614480180967585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.3333333333333333, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.36363636363636365, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(160.33333333333334, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(43.72727272727273, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.19999999999999998, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05454545454545454, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5666666666666667, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15454545454545454, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.9333333333333332, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2545454545454545, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.18666666666666668, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05090909090909091, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 91 neurons, 0.9116874033938135 embudo_1, 2.4775911599882594 embudo_2, and 0.9991123451068593 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_186 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_187 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 5ms/step - loss: 0.0662 - val_loss: 0.0764\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.0784\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 0.0688\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0642\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0304 - val_loss: 0.0648\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0774\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0586\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0307 - val_loss: 0.0654\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0541\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0305 - val_loss: 0.0564\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0574\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0328 - val_loss: 0.0547\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0303 - val_loss: 0.0524\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0530\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0498\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0515\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0539\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0540\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0541\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0471\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0479\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0559\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0529\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0488\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0470\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0467\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0543\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0480\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0435\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0464\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0477\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0276 - val_loss: 0.0453\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0432\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0471\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0476\n",
      "Epoch 36/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0490\n",
      "Epoch 37/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0492\n",
      "Epoch 38/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0462\n",
      "Epoch 39/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0487\n",
      "Epoch 40/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0477\n",
      "Epoch 41/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0482\n",
      "Epoch 42/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0481\n",
      "Epoch 43/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0512\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:22:48,773] Trial 10 finished with value: 1.3550314984193343 and parameters: {'n_layers': 2, 'n_neurons': 91, 'embudo_1': 0.9116874033938135, 'embudo_2': 2.4775911599882594, 'embudo_3': 0.9991123451068593, 'dropout_threshold': 0.4067926596254133}. Best is trial 10 with value: 1.3550314984193343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.36363636363636365, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(43.72727272727273, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05454545454545454, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15454545454545454, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2545454545454545, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05090909090909091, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 99 neurons, 0.9100333513579576 embudo_1, 2.468270428242658 embudo_2, and 1.122536291670659 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_188 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_189 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 5ms/step - loss: 0.0692 - val_loss: 0.0771\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.0753\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0714\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 0.0643\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0317 - val_loss: 0.0671\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0336 - val_loss: 0.0597\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 0.0560\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0311 - val_loss: 0.0579\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0519\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 0.0550\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0571\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0522\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0497\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0497\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0513\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0665\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0506\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0502\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0481\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0587\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0516\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0497\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0475\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0490\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0432\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0260 - val_loss: 0.0512\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0281 - val_loss: 0.0459\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0479\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0464\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0505\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0470\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0561\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0490\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0458\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0487\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:23:07,640] Trial 11 finished with value: 1.3743394739817316 and parameters: {'n_layers': 2, 'n_neurons': 99, 'embudo_1': 0.9100333513579576, 'embudo_2': 2.468270428242658, 'embudo_3': 1.122536291670659, 'dropout_threshold': 0.4056167100354792}. Best is trial 10 with value: 1.3550314984193343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.3333333333333333, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(40.083333333333336, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.049999999999999996, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14166666666666666, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2333333333333333, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04666666666666667, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 106 neurons, 0.9867789838166751 embudo_1, 2.101978382616194 embudo_2, and 0.8320688711446743 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_190 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_191 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 4ms/step - loss: 0.0636 - val_loss: 0.0725\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0366 - val_loss: 0.0681\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0617\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 0.0593\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0319 - val_loss: 0.0576\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0524\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0559\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0515\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0485\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0566\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0493\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0532\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0495\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0500\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0469\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0278 - val_loss: 0.0526\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0536\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0453\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0485\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0492\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0277 - val_loss: 0.0468\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0470\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0505\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0462\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0511\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0518\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0541\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0540\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:23:23,270] Trial 12 finished with value: 1.3701920324584222 and parameters: {'n_layers': 2, 'n_neurons': 106, 'embudo_1': 0.9867789838166751, 'embudo_2': 2.101978382616194, 'embudo_3': 0.8320688711446743, 'dropout_threshold': 0.39773699136477614}. Best is trial 10 with value: 1.3550314984193343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.3076923076923077, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(37.0, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04615384615384615, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13076923076923078, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.21538461538461537, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04307692307692308, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 277 neurons, 0.9585043990169838 embudo_1, 2.4909080789125166 embudo_2, and 1.6740191061498162 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_192 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_193 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 5ms/step - loss: 0.0517 - val_loss: 0.0748\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0326 - val_loss: 0.0605\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0322 - val_loss: 0.0617\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0291 - val_loss: 0.0587\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0280 - val_loss: 0.0548\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0542\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0553\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0478\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0534\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0463\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0278 - val_loss: 0.0526\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0480\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0433\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0500\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0451\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0256 - val_loss: 0.0469\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0637\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0457\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0246 - val_loss: 0.0525\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0488\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0468\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0243 - val_loss: 0.0482\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0519\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:23:37,688] Trial 13 finished with value: 1.3648842420621299 and parameters: {'n_layers': 2, 'n_neurons': 277, 'embudo_1': 0.9585043990169838, 'embudo_2': 2.4909080789125166, 'embudo_3': 1.6740191061498162, 'dropout_threshold': 0.1068514432321628}. Best is trial 10 with value: 1.3550314984193343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2857142857142857, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(34.357142857142854, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04285714285714286, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12142857142857143, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.19999999999999998, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 53 neurons, 1.0140796671050736 embudo_1, 2.1666601469532893 embudo_2, and 1.4656849159502183 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_194 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_195 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_196 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_197 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 3s 8ms/step - loss: 0.0845 - val_loss: 0.1251\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0494 - val_loss: 0.0860\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0466 - val_loss: 0.0858\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0408 - val_loss: 0.0743\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0405 - val_loss: 0.0741\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0391 - val_loss: 0.0690\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0369 - val_loss: 0.0601\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0332 - val_loss: 0.0606\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0318 - val_loss: 0.0520\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0336 - val_loss: 0.0574\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0324 - val_loss: 0.0504\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0321 - val_loss: 0.0510\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0306 - val_loss: 0.0522\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0285 - val_loss: 0.0465\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0311 - val_loss: 0.0496\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0294 - val_loss: 0.0521\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0324 - val_loss: 0.0488\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0308 - val_loss: 0.0424\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0292 - val_loss: 0.0451\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0300 - val_loss: 0.0462\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0283 - val_loss: 0.0476\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0293 - val_loss: 0.0493\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0291 - val_loss: 0.0465\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0505\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0298 - val_loss: 0.0522\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0295 - val_loss: 0.0385\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0292 - val_loss: 0.0442\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0273 - val_loss: 0.0468\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0432\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0266 - val_loss: 0.0445\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0270 - val_loss: 0.0395\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0307 - val_loss: 0.0475\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0278 - val_loss: 0.0431\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0291 - val_loss: 0.0435\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0426\n",
      "Epoch 36/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0271 - val_loss: 0.0478\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:24:06,155] Trial 14 finished with value: 1.3759443220329908 and parameters: {'n_layers': 4, 'n_neurons': 53, 'embudo_1': 1.0140796671050736, 'embudo_2': 2.1666601469532893, 'embudo_3': 1.4656849159502183, 'dropout_threshold': 0.20665888266617485}. Best is trial 10 with value: 1.3550314984193343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.26666666666666666, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(32.06666666666667, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11333333333333333, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.18666666666666665, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.037333333333333336, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 145 neurons, 1.2394068347555454 embudo_1, 1.8564514755940988 embudo_2, and 2.2175048979838703 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_198 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 1s 4ms/step - loss: 0.0473 - val_loss: 0.0650\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.0663\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0577\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0564\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0599\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0597\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0590\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0615\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0573\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0598\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0595\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0596\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0598\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0604\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:24:14,816] Trial 15 finished with value: 1.3747607625190246 and parameters: {'n_layers': 1, 'n_neurons': 145, 'embudo_1': 1.2394068347555454, 'embudo_2': 1.8564514755940988, 'embudo_3': 2.2175048979838703, 'dropout_threshold': 0.3502489891715073}. Best is trial 10 with value: 1.3550314984193343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.25, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(30.0625, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0375, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10625, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.175, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.035, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 259 neurons, 0.9124295982253288 embudo_1, 2.3032925272179297 embudo_2, and 2.7122469170108725 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_199 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_200 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 5ms/step - loss: 0.0636 - val_loss: 0.0664\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.0701\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0355 - val_loss: 0.0775\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0627\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0569\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0308 - val_loss: 0.0568\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0307 - val_loss: 0.0585\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0307 - val_loss: 0.0478\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0327 - val_loss: 0.0505\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0654\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0514\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 0.0549\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0564\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0298 - val_loss: 0.0487\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0294 - val_loss: 0.0568\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0596\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0276 - val_loss: 0.0490\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0278 - val_loss: 0.0462\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0504\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0579\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.0452\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0277 - val_loss: 0.0482\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0277 - val_loss: 0.0496\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.0493\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0512\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0444\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0473\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0456\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0520\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0503\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0480\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0650\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0530\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0531\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0471\n",
      "Epoch 36/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0263 - val_loss: 0.0474\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:24:34,073] Trial 16 finished with value: 1.3655041407488175 and parameters: {'n_layers': 2, 'n_neurons': 259, 'embudo_1': 0.9124295982253288, 'embudo_2': 2.3032925272179297, 'embudo_3': 2.7122469170108725, 'dropout_threshold': 0.44572130891442086}. Best is trial 10 with value: 1.3550314984193343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.23529411764705882, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(28.294117647058822, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03529411764705882, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.16470588235294117, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03294117647058824, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 128 neurons, 1.0292396770585115 embudo_1, 2.0339201435953416 embudo_2, and 0.7257282001360152 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_201 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_202 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_203 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 3s 6ms/step - loss: 0.0716 - val_loss: 0.0736\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0366 - val_loss: 0.0752\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0703\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0337 - val_loss: 0.0602\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0324 - val_loss: 0.0537\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0316 - val_loss: 0.0534\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0324 - val_loss: 0.0561\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0309 - val_loss: 0.0449\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0307 - val_loss: 0.0531\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0484\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0301 - val_loss: 0.0529\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0306 - val_loss: 0.0536\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0276 - val_loss: 0.0478\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0293 - val_loss: 0.0474\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0296 - val_loss: 0.0452\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0290 - val_loss: 0.0421\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0275 - val_loss: 0.0440\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0303 - val_loss: 0.0471\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0278 - val_loss: 0.0419\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0269 - val_loss: 0.0486\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0284 - val_loss: 0.0449\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0278 - val_loss: 0.0451\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0281 - val_loss: 0.0442\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0267 - val_loss: 0.0419\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0264 - val_loss: 0.0420\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0442\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0436\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0263 - val_loss: 0.0451\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0264 - val_loss: 0.0455\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:24:54,647] Trial 17 finished with value: 1.3749268588983468 and parameters: {'n_layers': 3, 'n_neurons': 128, 'embudo_1': 1.0292396770585115, 'embudo_2': 2.0339201435953416, 'embudo_3': 0.7257282001360152, 'dropout_threshold': 0.30953721547224794}. Best is trial 10 with value: 1.3550314984193343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2222222222222222, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(26.72222222222222, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03333333333333333, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09444444444444444, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15555555555555556, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.031111111111111114, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 223 neurons, 0.9900123294622032 embudo_1, 2.3208307314282397 embudo_2, and 1.3216281776717493 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_204 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_205 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_206 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 3s 6ms/step - loss: 0.0691 - val_loss: 0.0649\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0376 - val_loss: 0.0734\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0328 - val_loss: 0.0576\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0304 - val_loss: 0.0708\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0303 - val_loss: 0.0608\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0303 - val_loss: 0.0560\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0292 - val_loss: 0.0604\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0297 - val_loss: 0.0460\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0300 - val_loss: 0.0553\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0292 - val_loss: 0.0538\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0291 - val_loss: 0.0454\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0487\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0269 - val_loss: 0.0530\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0272 - val_loss: 0.0508\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0265 - val_loss: 0.0456\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0268 - val_loss: 0.0422\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0280 - val_loss: 0.0405\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0269 - val_loss: 0.0423\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0270 - val_loss: 0.0431\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0265 - val_loss: 0.0478\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0263 - val_loss: 0.0446\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0266 - val_loss: 0.0414\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0446\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0254 - val_loss: 0.0452\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0250 - val_loss: 0.0424\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.0423\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0257 - val_loss: 0.0412\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:25:14,765] Trial 18 finished with value: 1.3726342398388356 and parameters: {'n_layers': 3, 'n_neurons': 223, 'embudo_1': 0.9900123294622032, 'embudo_2': 2.3208307314282397, 'embudo_3': 1.3216281776717493, 'dropout_threshold': 0.21106776434591035}. Best is trial 10 with value: 1.3550314984193343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.21052631578947367, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(25.31578947368421, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.031578947368421054, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08947368421052632, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14736842105263157, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02947368421052632, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 327 neurons, 0.9043218762847649 embudo_1, 1.9404980919506725 embudo_2, and 1.8639902851229038 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_207 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_208 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 5ms/step - loss: 0.0573 - val_loss: 0.0669\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0338 - val_loss: 0.0648\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0313 - val_loss: 0.0630\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 0.0632\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0316 - val_loss: 0.0618\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0318 - val_loss: 0.0616\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0310 - val_loss: 0.0603\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0285 - val_loss: 0.0565\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0524\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0527\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0292 - val_loss: 0.0695\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0578\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0515\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0472\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0563\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0531\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0493\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0290 - val_loss: 0.0489\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0514\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0481\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0447\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0283 - val_loss: 0.0561\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0489\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0531\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0559\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0531\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0258 - val_loss: 0.0442\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0500\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0470\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0485\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0508\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0531\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0483\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0593\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0238 - val_loss: 0.0490\n",
      "Epoch 36/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0555\n",
      "Epoch 37/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0540\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:25:34,479] Trial 19 finished with value: 1.3476995419660092 and parameters: {'n_layers': 2, 'n_neurons': 327, 'embudo_1': 0.9043218762847649, 'embudo_2': 1.9404980919506725, 'embudo_3': 1.8639902851229038, 'dropout_threshold': 0.3625466315744853}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(24.05, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08499999999999999, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13999999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.028000000000000004, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 420 neurons, 1.0428328126909536 embudo_1, 1.769355671203943 embudo_2, and 1.8697289143519342 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_209 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 1s 3ms/step - loss: 0.0451 - val_loss: 0.0621\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.0603\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0624\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0561\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0563\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0621\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0535\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0607\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0554\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0608\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0554\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0517\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0594\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0512\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0561\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0581\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0560\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0539\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0592\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0608\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0538\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0536\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0535\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0612\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:25:45,425] Trial 20 finished with value: 1.3676172471622945 and parameters: {'n_layers': 1, 'n_neurons': 420, 'embudo_1': 1.0428328126909536, 'embudo_2': 1.769355671203943, 'embudo_3': 1.8697289143519342, 'dropout_threshold': 0.36805228910744575}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(24.05, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08499999999999999, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13999999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.028000000000000004, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 317 neurons, 0.9040446281947544 embudo_1, 1.9468848463465498 embudo_2, and 1.0310304055075155 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_210 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_211 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 5ms/step - loss: 0.0533 - val_loss: 0.0667\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 0.0711\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0731\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 0.0524\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0305 - val_loss: 0.0547\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0314 - val_loss: 0.0587\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0640\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0570\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0526\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0496\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 0.0519\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0314 - val_loss: 0.0565\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0578\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0507\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0480\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0539\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0485\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0635\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0503\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0477\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0564\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0475\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0497\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0455\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0467\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0264 - val_loss: 0.0475\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0484\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0258 - val_loss: 0.0523\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0495\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0523\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0523\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0498\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0492\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0492\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:26:03,537] Trial 21 finished with value: 1.3932788377578742 and parameters: {'n_layers': 2, 'n_neurons': 317, 'embudo_1': 0.9040446281947544, 'embudo_2': 1.9468848463465498, 'embudo_3': 1.0310304055075155, 'dropout_threshold': 0.4521525590574631}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.19047619047619047, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(22.904761904761905, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02857142857142857, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08095238095238096, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13333333333333333, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02666666666666667, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 309 neurons, 0.9521686587763674 embudo_1, 1.9836500371422154 embudo_2, and 1.6558217643947357 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_212 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_213 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 8ms/step - loss: 0.0532 - val_loss: 0.0730\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0338 - val_loss: 0.0632\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0314 - val_loss: 0.0580\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0539\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0312 - val_loss: 0.0559\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0510\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0555\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0291 - val_loss: 0.0494\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0285 - val_loss: 0.0516\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0511\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 0.0495\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0490\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0541\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0463\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0588\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0478\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0484\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0267 - val_loss: 0.0492\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0578\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0516\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0523\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0263 - val_loss: 0.0456\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0505\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0263 - val_loss: 0.0448\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0454\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0297 - val_loss: 0.0449\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0249 - val_loss: 0.0440\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0460\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0463\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0567\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.0574\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0451\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0505\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0531\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0245 - val_loss: 0.0520\n",
      "Epoch 36/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0538\n",
      "Epoch 37/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0251 - val_loss: 0.0543\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:26:23,416] Trial 22 finished with value: 1.3520600867592696 and parameters: {'n_layers': 2, 'n_neurons': 309, 'embudo_1': 0.9521686587763674, 'embudo_2': 1.9836500371422154, 'embudo_3': 1.6558217643947357, 'dropout_threshold': 0.285617738597883}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.18181818181818182, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(21.863636363636363, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02727272727272727, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07727272727272727, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12727272727272726, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.025454545454545455, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 388 neurons, 0.9800269587633104 embudo_1, 1.9589319793326139 embudo_2, and 1.5064103046321435 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_214 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_215 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 5ms/step - loss: 0.0501 - val_loss: 0.0641\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0341 - val_loss: 0.0642\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0313 - val_loss: 0.0633\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0326 - val_loss: 0.0717\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0582\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0578\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0304 - val_loss: 0.0495\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0284 - val_loss: 0.0555\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0583\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0285 - val_loss: 0.0657\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0300 - val_loss: 0.0543\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0284 - val_loss: 0.0485\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0471\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0525\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0506\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0490\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0436\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0488\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0454\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0444\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0539\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0531\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0479\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0543\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0526\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0495\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0276 - val_loss: 0.0534\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:26:39,221] Trial 23 finished with value: 1.3678812428779836 and parameters: {'n_layers': 2, 'n_neurons': 388, 'embudo_1': 0.9800269587633104, 'embudo_2': 1.9589319793326139, 'embudo_3': 1.5064103046321435, 'dropout_threshold': 0.34248205006720983}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.17391304347826086, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20.91304347826087, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02608695652173913, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07391304347826087, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.1217391304347826, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.024347826086956525, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 303 neurons, 0.9673085125762411 embudo_1, 1.8013947854485655 embudo_2, and 1.1876396143663202 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_216 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_217 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 5ms/step - loss: 0.0566 - val_loss: 0.0748\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 0.0605\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.0597\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0314 - val_loss: 0.0543\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0317 - val_loss: 0.0550\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0570\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0300 - val_loss: 0.0583\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0307 - val_loss: 0.0597\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0538\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0518\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0274 - val_loss: 0.0498\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0479\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0487\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0301 - val_loss: 0.0507\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0634\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0447\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0277 - val_loss: 0.0471\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0449\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0482\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0484\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0472\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0509\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0509\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0476\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0461\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0256 - val_loss: 0.0508\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:26:54,896] Trial 24 finished with value: 1.369856070711322 and parameters: {'n_layers': 2, 'n_neurons': 303, 'embudo_1': 0.9673085125762411, 'embudo_2': 1.8013947854485655, 'embudo_3': 1.1876396143663202, 'dropout_threshold': 0.3959372903043946}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.16666666666666666, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20.041666666666668, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.024999999999999998, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07083333333333333, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11666666666666665, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.023333333333333334, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 362 neurons, 1.0183473621558703 embudo_1, 2.2252428903173325 embudo_2, and 1.7145349117098172 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_218 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 1s 3ms/step - loss: 0.0408 - val_loss: 0.0634\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0304 - val_loss: 0.0595\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0613\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0583\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0624\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0535\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0558\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0579\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0585\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0528\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0594\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0578\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0667\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0533\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0603\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0569\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0555\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0555\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0607\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0504\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0509\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0533\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0562\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0503\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0529\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0611\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0535\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0546\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0562\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0608\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0530\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0539\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0614\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0567\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:27:08,263] Trial 25 finished with value: 1.3548037188604283 and parameters: {'n_layers': 1, 'n_neurons': 362, 'embudo_1': 1.0183473621558703, 'embudo_2': 2.2252428903173325, 'embudo_3': 1.7145349117098172, 'dropout_threshold': 0.29568176060293233}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.16, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(19.24, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.024, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.068, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11199999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.022400000000000003, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 445 neurons, 1.036670753041278 embudo_1, 2.1960093171112276 embudo_2, and 1.952918321722216 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_219 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 1s 3ms/step - loss: 0.0429 - val_loss: 0.0654\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0651\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0566\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0557\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0591\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0558\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0615\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0546\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0568\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0548\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0625\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0589\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0576\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0521\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0527\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0584\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0590\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0617\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0302 - val_loss: 0.0509\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0521\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0648\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0574\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0508\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0545\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0600\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0503\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0560\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0517\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0539\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0598\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0607\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0616\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0529\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0582\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0618\n",
      "Epoch 36/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0553\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:27:21,938] Trial 26 finished with value: 1.359235064018567 and parameters: {'n_layers': 1, 'n_neurons': 445, 'embudo_1': 1.036670753041278, 'embudo_2': 2.1960093171112276, 'embudo_3': 1.952918321722216, 'dropout_threshold': 0.2812908266158065}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15384615384615385, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(18.5, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.023076923076923075, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06538461538461539, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10769230769230768, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02153846153846154, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 358 neurons, 0.9611191430038234 embudo_1, 1.9458189266758474 embudo_2, and 1.7083776048573127 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_220 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 1s 3ms/step - loss: 0.0424 - val_loss: 0.0630\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.0591\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0582\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0521\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0557\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0602\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0643\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0555\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0580\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0544\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0542\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0566\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0641\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0580\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:27:30,434] Trial 27 finished with value: 1.3734670003359786 and parameters: {'n_layers': 1, 'n_neurons': 358, 'embudo_1': 0.9611191430038234, 'embudo_2': 1.9458189266758474, 'embudo_3': 1.7083776048573127, 'dropout_threshold': 0.30807059415905313}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14814814814814814, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(17.814814814814813, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.022222222222222223, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06296296296296296, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.1037037037037037, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.020740740740740744, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 409 neurons, 0.9968559427329065 embudo_1, 1.684719090653476 embudo_2, and 2.158969512402492 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_221 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 1s 3ms/step - loss: 0.0416 - val_loss: 0.0645\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0306 - val_loss: 0.0635\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.0641\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0582\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0624\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0581\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0536\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0610\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0561\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0539\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0522\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0556\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0548\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0509\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0554\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0531\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0535\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0518\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0571\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0546\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0518\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.0535\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0513\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0542\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:27:41,206] Trial 28 finished with value: 1.3671214170256325 and parameters: {'n_layers': 1, 'n_neurons': 409, 'embudo_1': 0.9968559427329065, 'embudo_2': 1.684719090653476, 'embudo_3': 2.158969512402492, 'dropout_threshold': 0.22364792755368246}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14285714285714285, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(17.178571428571427, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02142857142857143, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.060714285714285714, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 307 neurons, 0.943457847218039 embudo_1, 1.729759383677287 embudo_2, and 1.4985979584670979 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_222 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 1s 3ms/step - loss: 0.0422 - val_loss: 0.0649\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.0616\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0694\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.0576\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0600\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0581\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0573\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0558\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0585\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0557\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0539\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0556\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0552\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0586\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0582\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0567\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0549\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0612\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0524\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0558\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0557\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0522\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0572\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0523\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0499\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0579\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0496\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0571\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0680\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0611\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0582\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0613\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0630\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0552\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0559\n",
      "Epoch 36/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0649\n",
      "Epoch 37/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0600\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:27:55,005] Trial 29 finished with value: 1.348922104134851 and parameters: {'n_layers': 1, 'n_neurons': 307, 'embudo_1': 0.943457847218039, 'embudo_2': 1.729759383677287, 'embudo_3': 1.4985979584670979, 'dropout_threshold': 0.35711544646901416}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13793103448275862, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(16.586206896551722, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.020689655172413793, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05862068965517241, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09655172413793103, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.019310344827586208, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 294 neurons, 0.9477982362597447 embudo_1, 1.7250382584865793 embudo_2, and 1.4097451293874554 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_223 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 1s 3ms/step - loss: 0.0426 - val_loss: 0.0674\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0304 - val_loss: 0.0697\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0606\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0585\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0567\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0627\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0641\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0583\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0593\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0596\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0594\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0304 - val_loss: 0.0647\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0580\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0600\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0576\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:28:03,381] Trial 30 finished with value: 1.3732961691302163 and parameters: {'n_layers': 1, 'n_neurons': 294, 'embudo_1': 0.9477982362597447, 'embudo_2': 1.7250382584865793, 'embudo_3': 1.4097451293874554, 'dropout_threshold': 0.3507570759938144}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13793103448275862, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(16.586206896551722, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.020689655172413793, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05862068965517241, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09655172413793103, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.019310344827586208, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 337 neurons, 0.9402398818401376 embudo_1, 2.006617163282146 embudo_2, and 1.6986160746577834 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_224 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 1s 3ms/step - loss: 0.0412 - val_loss: 0.0630\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0313 - val_loss: 0.0602\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0603\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0686\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0552\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0566\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0574\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0522\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0532\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0568\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0502\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0548\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0575\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0547\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0582\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0535\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0559\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0607\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0535\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0520\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0561\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:28:13,858] Trial 31 finished with value: 1.3716226771971642 and parameters: {'n_layers': 1, 'n_neurons': 337, 'embudo_1': 0.9402398818401376, 'embudo_2': 2.006617163282146, 'embudo_3': 1.6986160746577834, 'dropout_threshold': 0.27022032645018984}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13333333333333333, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(16.033333333333335, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.056666666666666664, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333332, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.018666666666666668, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 232 neurons, 1.010914787117637 embudo_1, 1.8339358553622385 embudo_2, and 1.543874158132279 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_225 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 1s 3ms/step - loss: 0.0450 - val_loss: 0.0684\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.0649\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0567\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0557\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0582\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0572\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0510\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0585\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0570\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0548\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0532\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0532\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0589\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0504\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0520\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0511\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0496\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0574\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0518\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0539\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0590\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0588\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0509\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0536\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0513\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0544\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0520\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:28:25,152] Trial 32 finished with value: 1.3514155231238294 and parameters: {'n_layers': 1, 'n_neurons': 232, 'embudo_1': 1.010914787117637, 'embudo_2': 1.8339358553622385, 'embudo_3': 1.543874158132279, 'dropout_threshold': 0.3179659056842327}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12903225806451613, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(15.516129032258064, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01935483870967742, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.054838709677419356, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09032258064516129, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01806451612903226, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 215 neurons, 0.940787125283955 embudo_1, 1.635510079451591 embudo_2, and 1.5493863989468704 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_226 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 1s 3ms/step - loss: 0.0430 - val_loss: 0.0621\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.0640\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0653\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0551\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0566\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0565\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0606\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0511\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0526\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0578\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0557\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0578\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0523\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0499\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0604\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0576\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0557\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0543\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0598\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0543\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0575\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0541\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0568\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0538\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:28:35,755] Trial 33 finished with value: 1.3657002696937357 and parameters: {'n_layers': 1, 'n_neurons': 215, 'embudo_1': 0.940787125283955, 'embudo_2': 1.635510079451591, 'embudo_3': 1.5493863989468704, 'dropout_threshold': 0.3336937388969885}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.125, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(15.03125, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01875, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.053125, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0875, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0175, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 230 neurons, 0.904224872798381 embudo_1, 1.8387299579442677 embudo_2, and 2.0299752715209145 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_227 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_228 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 5ms/step - loss: 0.0666 - val_loss: 0.0859\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.0827\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0636\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 0.0560\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0320 - val_loss: 0.0524\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0310 - val_loss: 0.0553\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0302 - val_loss: 0.0534\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0305 - val_loss: 0.0501\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0533\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0577\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0548\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0281 - val_loss: 0.0490\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0497\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0504\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0519\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0467\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0270 - val_loss: 0.0530\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0485\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0282 - val_loss: 0.0513\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0545\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0502\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0263 - val_loss: 0.0528\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0277 - val_loss: 0.0452\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0455\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0467\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0270 - val_loss: 0.0522\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0437\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0416\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0264 - val_loss: 0.0435\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0487\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0260 - val_loss: 0.0534\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0239 - val_loss: 0.0487\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0251 - val_loss: 0.0522\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0256 - val_loss: 0.0437\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0262 - val_loss: 0.0648\n",
      "Epoch 36/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0538\n",
      "Epoch 37/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0496\n",
      "Epoch 38/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0244 - val_loss: 0.0520\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:28:56,255] Trial 34 finished with value: 1.3548984126060593 and parameters: {'n_layers': 2, 'n_neurons': 230, 'embudo_1': 0.904224872798381, 'embudo_2': 1.8387299579442677, 'embudo_3': 2.0299752715209145, 'dropout_threshold': 0.36881816981981985}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12121212121212122, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(14.575757575757576, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01818181818181818, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.051515151515151514, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08484848484848484, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01696969696969697, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 277 neurons, 1.0663255711855417 embudo_1, 2.0534635693917873 embudo_2, and 1.288781262958231 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_229 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 1s 3ms/step - loss: 0.0429 - val_loss: 0.0624\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 0.0620\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0587\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0615\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0660\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0594\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0564\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0539\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0651\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0557\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0559\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0539\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0556\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0587\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0549\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0508\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0532\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0672\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0514\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0601\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0585\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0543\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0624\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0546\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0546\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0574\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:29:07,326] Trial 35 finished with value: 1.3670378886923757 and parameters: {'n_layers': 1, 'n_neurons': 277, 'embudo_1': 1.0663255711855417, 'embudo_2': 2.0534635693917873, 'embudo_3': 1.288781262958231, 'dropout_threshold': 0.2456545860258827}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11764705882352941, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(14.147058823529411, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01764705882352941, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.049999999999999996, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08235294117647059, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01647058823529412, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 494 neurons, 0.9460874749748872 embudo_1, 1.903080536097637 embudo_2, and 1.5705237633229168 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_230 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_231 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 8ms/step - loss: 0.0539 - val_loss: 0.0629\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0340 - val_loss: 0.0574\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0308 - val_loss: 0.0688\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0318 - val_loss: 0.0626\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0318 - val_loss: 0.0679\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0522\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0315 - val_loss: 0.0623\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0296 - val_loss: 0.0611\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0517\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0290 - val_loss: 0.0523\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0484\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0501\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0284 - val_loss: 0.0515\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0481\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0494\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0527\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0528\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0274 - val_loss: 0.0481\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.0497\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0456\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0492\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0269 - val_loss: 0.0520\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0269 - val_loss: 0.0485\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0252 - val_loss: 0.0476\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0522\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0598\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0263 - val_loss: 0.0458\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0510\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0256 - val_loss: 0.0509\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0618\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:29:25,254] Trial 36 finished with value: 1.3620243957908293 and parameters: {'n_layers': 2, 'n_neurons': 494, 'embudo_1': 0.9460874749748872, 'embudo_2': 1.903080536097637, 'embudo_3': 1.5705237633229168, 'dropout_threshold': 0.3214250832028837}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11428571428571428, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(13.742857142857142, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.017142857142857144, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04857142857142857, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.016, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 333 neurons, 1.0014767654802172 embudo_1, 1.7537190392714597 embudo_2, and 1.8101781622456568 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_232 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 1s 3ms/step - loss: 0.0440 - val_loss: 0.0604\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0306 - val_loss: 0.0573\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.0622\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0667\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0597\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0537\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0592\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0637\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0552\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0514\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0569\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0623\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0590\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0587\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0547\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0577\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0522\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0558\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0627\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0773\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:29:35,085] Trial 37 finished with value: 1.369735601850565 and parameters: {'n_layers': 1, 'n_neurons': 333, 'embudo_1': 1.0014767654802172, 'embudo_2': 1.7537190392714597, 'embudo_3': 1.8101781622456568, 'dropout_threshold': 0.29909601068630104}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.1111111111111111, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(13.36111111111111, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.016666666666666666, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04722222222222222, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07777777777777778, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015555555555555557, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 163 neurons, 1.0620679489472762 embudo_1, 1.597853850312989 embudo_2, and 2.0004505724782695 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_233 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_234 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_235 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 3s 7ms/step - loss: 0.0745 - val_loss: 0.1092\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0421 - val_loss: 0.0667\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 0.0754\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0330 - val_loss: 0.0578\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0318 - val_loss: 0.0542\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0505\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0304 - val_loss: 0.0487\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0288 - val_loss: 0.0570\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0293 - val_loss: 0.0493\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0280 - val_loss: 0.0468\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0317 - val_loss: 0.0485\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0304 - val_loss: 0.0499\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0285 - val_loss: 0.0538\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0288 - val_loss: 0.0469\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0264 - val_loss: 0.0468\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0459\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0447\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0265 - val_loss: 0.0489\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0270 - val_loss: 0.0459\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0494\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0270 - val_loss: 0.0505\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0442\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0261 - val_loss: 0.0455\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0260 - val_loss: 0.0419\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0454\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0251 - val_loss: 0.0508\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0253 - val_loss: 0.0525\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0258 - val_loss: 0.0469\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0259 - val_loss: 0.0465\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0553\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0256 - val_loss: 0.0452\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0277 - val_loss: 0.0455\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0238 - val_loss: 0.0472\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0251 - val_loss: 0.0513\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:29:58,683] Trial 38 finished with value: 1.3642270819836857 and parameters: {'n_layers': 3, 'n_neurons': 163, 'embudo_1': 1.0620679489472762, 'embudo_2': 1.597853850312989, 'embudo_3': 2.0004505724782695, 'dropout_threshold': 0.2544871659288479}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10810810810810811, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(13.0, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.016216216216216217, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04594594594594594, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07567567567567567, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015135135135135137, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 287 neurons, 0.9341864788169897 embudo_1, 1.8318492519016294 embudo_2, and 1.8202961010316754 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_236 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_237 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 5ms/step - loss: 0.0610 - val_loss: 0.0684\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 0.0618\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0330 - val_loss: 0.0571\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0319 - val_loss: 0.0567\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0313 - val_loss: 0.0585\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0292 - val_loss: 0.0640\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0310 - val_loss: 0.0540\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0616\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0302 - val_loss: 0.0504\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0546\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0294 - val_loss: 0.0511\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0280 - val_loss: 0.0496\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0456\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0521\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0278 - val_loss: 0.0542\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0565\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0525\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0567\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0290 - val_loss: 0.0494\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0269 - val_loss: 0.0488\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0292 - val_loss: 0.0467\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0274 - val_loss: 0.0479\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0592\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:30:13,067] Trial 39 finished with value: 1.372263605063181 and parameters: {'n_layers': 2, 'n_neurons': 287, 'embudo_1': 0.9341864788169897, 'embudo_2': 1.8318492519016294, 'embudo_3': 1.8202961010316754, 'dropout_threshold': 0.3331142383653665}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10526315789473684, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.657894736842104, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015789473684210527, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04473684210526316, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07368421052631578, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01473684210526316, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 201 neurons, 0.9668046745659994 embudo_1, 1.491517412801536 embudo_2, and 1.3660502194511226 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_238 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 1s 3ms/step - loss: 0.0404 - val_loss: 0.0709\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.0589\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0302 - val_loss: 0.0636\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0615\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0602\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0597\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0564\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0551\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0617\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0554\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0545\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0562\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0525\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0514\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0542\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0556\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0569\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0562\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0518\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0550\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0530\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0639\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0549\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0497\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0553\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0539\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0495\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0521\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0552\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0577\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0556\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0546\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0546\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0643\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0593\n",
      "Epoch 36/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0507\n",
      "Epoch 37/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0533\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:30:26,758] Trial 40 finished with value: 1.3667251220233894 and parameters: {'n_layers': 1, 'n_neurons': 201, 'embudo_1': 0.9668046745659994, 'embudo_2': 1.491517412801536, 'embudo_3': 1.3660502194511226, 'dropout_threshold': 0.3720444880566531}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10526315789473684, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.657894736842104, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015789473684210527, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04473684210526316, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07368421052631578, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01473684210526316, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 360 neurons, 1.021604336175339 embudo_1, 2.0896133492379336 embudo_2, and 1.6598264246857293 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_239 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0415 - val_loss: 0.0647\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.0606\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0596\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0576\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0585\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0563\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0567\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0549\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0532\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0563\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0572\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0547\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0506\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0589\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0533\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0567\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0543\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0586\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0508\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0553\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0543\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0560\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0574\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:30:37,627] Trial 41 finished with value: 1.3681606753490414 and parameters: {'n_layers': 1, 'n_neurons': 360, 'embudo_1': 1.021604336175339, 'embudo_2': 2.0896133492379336, 'embudo_3': 1.6598264246857293, 'dropout_threshold': 0.28882932203106915}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10256410256410256, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.333333333333334, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015384615384615384, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04358974358974359, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07179487179487179, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01435897435897436, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 319 neurons, 1.0070149482669348 embudo_1, 1.9738565658392941 embudo_2, and 1.5855782860595626 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_240 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 1s 3ms/step - loss: 0.0449 - val_loss: 0.0612\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0314 - val_loss: 0.0560\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0598\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0604\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0591\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0568\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0564\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0558\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0528\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0562\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0518\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0572\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0555\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0507\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0599\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0613\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0544\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0648\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0252 - val_loss: 0.0667\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0602\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0597\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0676\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0542\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0568\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:30:48,573] Trial 42 finished with value: 1.3649926740282385 and parameters: {'n_layers': 1, 'n_neurons': 319, 'embudo_1': 1.0070149482669348, 'embudo_2': 1.9738565658392941, 'embudo_3': 1.5855782860595626, 'dropout_threshold': 0.2870361294205177}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.1, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.025, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.042499999999999996, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06999999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.014000000000000002, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 249 neurons, 0.9273957872946749 embudo_1, 1.6988829202822577 embudo_2, and 1.7826295035267075 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_241 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 1s 3ms/step - loss: 0.0463 - val_loss: 0.0680\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0539\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0590\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0682\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0574\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0514\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0551\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0565\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0581\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0586\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0625\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0539\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0600\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0547\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0565\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0600\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:30:57,242] Trial 43 finished with value: 1.3727470291774593 and parameters: {'n_layers': 1, 'n_neurons': 249, 'embudo_1': 0.9273957872946749, 'embudo_2': 1.6988829202822577, 'embudo_3': 1.7826295035267075, 'dropout_threshold': 0.31825714131925764}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0975609756097561, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(11.731707317073171, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.014634146341463414, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.041463414634146344, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06829268292682926, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013658536585365855, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 388 neurons, 0.9833867288581949 embudo_1, 1.8886386142529572 embudo_2, and 1.4526746587885235 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_242 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_243 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 5ms/step - loss: 0.0492 - val_loss: 0.0737\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0354 - val_loss: 0.0665\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 0.0543\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0320 - val_loss: 0.0511\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0290 - val_loss: 0.0529\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0302 - val_loss: 0.0526\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0285 - val_loss: 0.0548\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0295 - val_loss: 0.0516\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0503\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0484\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0282 - val_loss: 0.0444\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0542\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0258 - val_loss: 0.0456\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0522\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0505\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0268 - val_loss: 0.0529\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0274 - val_loss: 0.0434\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0454\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0498\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0558\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0453\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0267 - val_loss: 0.0498\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0262 - val_loss: 0.0473\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0245 - val_loss: 0.0461\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0274 - val_loss: 0.0471\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0252 - val_loss: 0.0469\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0252 - val_loss: 0.0439\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:31:13,872] Trial 44 finished with value: 1.3536800163151446 and parameters: {'n_layers': 2, 'n_neurons': 388, 'embudo_1': 0.9833867288581949, 'embudo_2': 1.8886386142529572, 'embudo_3': 1.4526746587885235, 'dropout_threshold': 0.24608776610793293}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09523809523809523, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(11.452380952380953, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.014285714285714285, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04047619047619048, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06666666666666667, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013333333333333334, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 395 neurons, 0.9690143193505726 embudo_1, 1.9002379512093168 embudo_2, and 1.4329457275772994 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_244 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_245 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 5ms/step - loss: 0.0513 - val_loss: 0.0738\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0325 - val_loss: 0.0671\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0327 - val_loss: 0.0680\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0333 - val_loss: 0.0533\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0295 - val_loss: 0.0612\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0284 - val_loss: 0.0588\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0291 - val_loss: 0.0484\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0497\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0317 - val_loss: 0.0608\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0472\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0586\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0515\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0269 - val_loss: 0.0562\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0490\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0510\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0453\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0502\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0482\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0269 - val_loss: 0.0515\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0460\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0470\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0463\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0249 - val_loss: 0.0504\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0268 - val_loss: 0.0434\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0480\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0449\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0544\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0470\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0607\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0533\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0251 - val_loss: 0.0446\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0545\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0249 - val_loss: 0.0523\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0242 - val_loss: 0.0500\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:31:32,463] Trial 45 finished with value: 1.3640011997064805 and parameters: {'n_layers': 2, 'n_neurons': 395, 'embudo_1': 0.9690143193505726, 'embudo_2': 1.9002379512093168, 'embudo_3': 1.4329457275772994, 'dropout_threshold': 0.24168081893853396}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09302325581395349, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(11.186046511627907, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013953488372093023, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03953488372093023, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06511627906976744, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013023255813953489, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 448 neurons, 0.9909525519801726 embudo_1, 1.7964518212188998 embudo_2, and 1.2236341072800974 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_246 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_247 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 5ms/step - loss: 0.0513 - val_loss: 0.0681\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0328 - val_loss: 0.0652\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0313 - val_loss: 0.0625\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0297 - val_loss: 0.0520\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0295 - val_loss: 0.0512\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0298 - val_loss: 0.0533\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0464\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0292 - val_loss: 0.0494\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.0503\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0268 - val_loss: 0.0495\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0487\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0467\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0593\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0611\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0453\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0277 - val_loss: 0.0513\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0486\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0247 - val_loss: 0.0450\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0470\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0448\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0458\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0491\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0476\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0463\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0283 - val_loss: 0.0476\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0465\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0465\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0484\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0249 - val_loss: 0.0484\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0250 - val_loss: 0.0479\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:31:50,079] Trial 46 finished with value: 1.3586480204757223 and parameters: {'n_layers': 2, 'n_neurons': 448, 'embudo_1': 0.9909525519801726, 'embudo_2': 1.7964518212188998, 'embudo_3': 1.2236341072800974, 'dropout_threshold': 0.19932833635930777}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09090909090909091, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(10.931818181818182, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013636363636363636, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.038636363636363635, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06363636363636363, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.012727272727272728, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 315 neurons, 0.9282513568285162 embudo_1, 2.0365851191873796 embudo_2, and 1.90930393289353 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_248 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_249 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_250 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 3s 7ms/step - loss: 0.0585 - val_loss: 0.0624\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0335 - val_loss: 0.0589\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0312 - val_loss: 0.0590\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0299 - val_loss: 0.0600\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0338 - val_loss: 0.0529\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0301 - val_loss: 0.0471\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0302 - val_loss: 0.0595\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0298 - val_loss: 0.0519\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0306 - val_loss: 0.0506\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0274 - val_loss: 0.0536\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0280 - val_loss: 0.0488\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0460\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0472\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0269 - val_loss: 0.0405\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0277 - val_loss: 0.0503\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0269 - val_loss: 0.0447\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0253 - val_loss: 0.0430\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0264 - val_loss: 0.0482\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0440\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0265 - val_loss: 0.0442\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0255 - val_loss: 0.0473\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0262 - val_loss: 0.0499\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0255 - val_loss: 0.0451\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0262 - val_loss: 0.0472\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:32:08,139] Trial 47 finished with value: 1.3681402817355124 and parameters: {'n_layers': 3, 'n_neurons': 315, 'embudo_1': 0.9282513568285162, 'embudo_2': 2.0365851191873796, 'embudo_3': 1.90930393289353, 'dropout_threshold': 0.16487873497495015}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08888888888888889, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(10.688888888888888, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013333333333333332, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03777777777777778, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06222222222222222, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.012444444444444445, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 266 neurons, 1.0917533591490218 embudo_1, 1.6432194034389294 embudo_2, and 1.4458497919476638 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_251 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_252 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 5ms/step - loss: 0.0593 - val_loss: 0.0667\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 0.0691\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0313 - val_loss: 0.0626\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0307 - val_loss: 0.0570\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0302 - val_loss: 0.0662\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0594\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0516\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0304 - val_loss: 0.0538\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0291 - val_loss: 0.0478\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0454\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0570\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0494\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0277 - val_loss: 0.0465\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0539\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0599\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0270 - val_loss: 0.0513\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0513\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0276 - val_loss: 0.0473\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0537\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0494\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:32:21,507] Trial 48 finished with value: 1.3719769510104136 and parameters: {'n_layers': 2, 'n_neurons': 266, 'embudo_1': 1.0917533591490218, 'embudo_2': 1.6432194034389294, 'embudo_3': 1.4458497919476638, 'dropout_threshold': 0.2625912750860588}. Best is trial 19 with value: 1.3476995419660092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08695652173913043, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(10.456521739130435, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013043478260869565, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03695652173913044, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0608695652173913, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.012173913043478262, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 346 neurons, 0.9666514992740858 embudo_1, 1.889025121971658 embudo_2, and 1.6195431920626777 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_253 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_254 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 5ms/step - loss: 0.0574 - val_loss: 0.0730\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0348 - val_loss: 0.0621\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 0.0587\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0310 - val_loss: 0.0710\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0307 - val_loss: 0.0580\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0610\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0318 - val_loss: 0.0657\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0642\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0488\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0530\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0473\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0285 - val_loss: 0.0449\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0302 - val_loss: 0.0486\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0278 - val_loss: 0.0441\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0514\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0485\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0498\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0456\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0502\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0569\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0468\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.0270 - val_loss: 0.0494\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0584\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0443\n",
      "Predicting 201904 for product -1.6015984487942827 (1/149))\n",
      "Predicting 201904 for product -1.4336616974783778 (2/149))\n",
      "Predicting 201904 for product -1.3963424194081766 (3/149))\n",
      "Predicting 201904 for product -1.3910110939695766 (4/149))\n",
      "Predicting 201904 for product -1.3856797685309763 (5/149))\n",
      "Predicting 201904 for product -1.372351454934476 (6/149))\n",
      "Predicting 201904 for product -1.3510261531800754 (7/149))\n",
      "Predicting 201904 for product -1.3350321768642748 (8/149))\n",
      "Predicting 201904 for product -1.3323665141449748 (9/149))\n",
      "Predicting 201904 for product -1.3083755496712741 (10/149))\n",
      "Predicting 201904 for product -1.2950472360747736 (11/149))\n",
      "Predicting 201904 for product -1.2790532597589732 (12/149))\n",
      "Predicting 201904 for product -1.273721934320373 (13/149))\n",
      "Predicting 201904 for product -1.236402656250172 (14/149))\n",
      "Predicting 201904 for product -1.2230743426536714 (15/149))\n",
      "Predicting 201904 for product -1.1937520527413708 (16/149))\n",
      "Predicting 201904 for product -1.1910863900220705 (17/149))\n",
      "Predicting 201904 for product -1.1511014492325693 (18/149))\n",
      "Predicting 201904 for product -1.1217791593202686 (19/149))\n",
      "Predicting 201904 for product -1.100453857565868 (20/149))\n",
      "Predicting 201904 for product -1.0391436150219662 (21/149))\n",
      "Predicting 201904 for product -1.033812289583366 (22/149))\n",
      "Predicting 201904 for product -1.0231496387061656 (23/149))\n",
      "Predicting 201904 for product -0.9405140944078632 (24/149))\n",
      "Predicting 201904 for product -0.9378484316885631 (25/149))\n",
      "Predicting 201904 for product -0.9165231299341625 (26/149))\n",
      "Predicting 201904 for product -0.8978634908990619 (27/149))\n",
      "Predicting 201904 for product -0.8818695145832615 (28/149))\n",
      "Predicting 201904 for product -0.8712068637060612 (29/149))\n",
      "Predicting 201904 for product -0.8418845737937604 (30/149))\n",
      "Predicting 201904 for product -0.8338875856358601 (31/149))\n",
      "Predicting 201904 for product -0.8258905974779598 (32/149))\n",
      "Predicting 201904 for product -0.8178936093200596 (33/149))\n",
      "Predicting 201904 for product -0.8152279466007595 (34/149))\n",
      "Predicting 201904 for product -0.8125622838814595 (35/149))\n",
      "Predicting 201904 for product -0.8045652957235592 (36/149))\n",
      "Predicting 201904 for product -0.796568307565659 (37/149))\n",
      "Predicting 201904 for product -0.7619146922147579 (38/149))\n",
      "Predicting 201904 for product -0.7512520413375576 (39/149))\n",
      "Predicting 201904 for product -0.7459207158989575 (40/149))\n",
      "Predicting 201904 for product -0.727261076863857 (41/149))\n",
      "Predicting 201904 for product -0.7245954141445569 (42/149))\n",
      "Predicting 201904 for product -0.7006044496708561 (43/149))\n",
      "Predicting 201904 for product -0.695273124232256 (44/149))\n",
      "Predicting 201904 for product -0.6926074615129559 (45/149))\n",
      "Predicting 201904 for product -0.6846104733550556 (46/149))\n",
      "Predicting 201904 for product -0.6792791479164555 (47/149))\n",
      "Predicting 201904 for product -0.660619508881355 (48/149))\n",
      "Predicting 201904 for product -0.6579538461620549 (49/149))\n",
      "Predicting 201904 for product -0.625965893530454 (50/149))\n",
      "Predicting 201904 for product -0.6206345680918538 (51/149))\n",
      "Predicting 201904 for product -0.5459960119514516 (52/149))\n",
      "Predicting 201904 for product -0.5406646865128515 (53/149))\n",
      "Predicting 201904 for product -0.5300020356356512 (54/149))\n",
      "Predicting 201904 for product -0.524670710197051 (55/149))\n",
      "Predicting 201904 for product -0.5086767338812506 (56/149))\n",
      "Predicting 201904 for product -0.45802914221454905 (57/149))\n",
      "Predicting 201904 for product -0.45536347949524897 (58/149))\n",
      "Predicting 201904 for product -0.4500321540566488 (59/149))\n",
      "Predicting 201904 for product -0.4313725150215482 (60/149))\n",
      "Predicting 201904 for product -0.4100472132671476 (61/149))\n",
      "Predicting 201904 for product -0.3780592606355467 (62/149))\n",
      "Predicting 201904 for product -0.3647309470390463 (63/149))\n",
      "Predicting 201904 for product -0.3407399825653456 (64/149))\n",
      "Predicting 201904 for product -0.2900923908986441 (65/149))\n",
      "Predicting 201904 for product -0.287426728179344 (66/149))\n",
      "Predicting 201904 for product -0.28209540274074385 (67/149))\n",
      "Predicting 201904 for product -0.2794297400214438 (68/149))\n",
      "Predicting 201904 for product -0.2767640773021437 (69/149))\n",
      "Predicting 201904 for product -0.26077010098634323 (70/149))\n",
      "Predicting 201904 for product -0.22878214835474228 (71/149))\n",
      "Predicting 201904 for product -0.22078516019684205 (72/149))\n",
      "Predicting 201904 for product -0.21012250931964174 (73/149))\n",
      "Predicting 201904 for product -0.2047911838810416 (74/149))\n",
      "Predicting 201904 for product -0.2021255211617415 (75/149))\n",
      "Predicting 201904 for product -0.19679419572314136 (76/149))\n",
      "Predicting 201904 for product -0.1754688939687407 (77/149))\n",
      "Predicting 201904 for product -0.16214058037224033 (78/149))\n",
      "Predicting 201904 for product -0.14614660405643987 (79/149))\n",
      "Predicting 201904 for product -0.13814961589853963 (80/149))\n",
      "Predicting 201904 for product -0.12215563958273916 (81/149))\n",
      "Predicting 201904 for product -0.11149298870553884 (82/149))\n",
      "Predicting 201904 for product -0.10616166326693868 (83/149))\n",
      "Predicting 201904 for product -0.06884238519673759 (84/149))\n",
      "Predicting 201904 for product -0.03952009528443673 (85/149))\n",
      "Predicting 201904 for product -0.010197805372135866 (86/149))\n",
      "Predicting 201904 for product 0.0004648455050644479 (87/149))\n",
      "Predicting 201904 for product 0.019124484540164997 (88/149))\n",
      "Predicting 201904 for product 0.04578111173316578 (89/149))\n",
      "Predicting 201904 for product 0.053778099891066015 (90/149))\n",
      "Predicting 201904 for product 0.20572087489117047 (91/149))\n",
      "Predicting 201904 for product 0.21105220032977065 (92/149))\n",
      "Predicting 201904 for product 0.2163835257683708 (93/149))\n",
      "Predicting 201904 for product 0.24304015296137157 (94/149))\n",
      "Predicting 201904 for product 0.24837147839997173 (95/149))\n",
      "Predicting 201904 for product 0.25903412927717206 (96/149))\n",
      "Predicting 201904 for product 0.26169979199647214 (97/149))\n",
      "Predicting 201904 for product 0.2670311174350723 (98/149))\n",
      "Predicting 201904 for product 0.2883564191894729 (99/149))\n",
      "Predicting 201904 for product 0.30968172094387353 (100/149))\n",
      "Predicting 201904 for product 0.3123473836631736 (101/149))\n",
      "Predicting 201904 for product 0.3230100345403739 (102/149))\n",
      "Predicting 201904 for product 0.3336726854175742 (103/149))\n",
      "Predicting 201904 for product 0.3390040108561744 (104/149))\n",
      "Predicting 201904 for product 0.3523323244526748 (105/149))\n",
      "Predicting 201904 for product 0.37365762620707543 (106/149))\n",
      "Predicting 201904 for product 0.3763232889263755 (107/149))\n",
      "Predicting 201904 for product 0.38432027708427574 (108/149))\n",
      "Predicting 201904 for product 0.4136425669965766 (109/149))\n",
      "Predicting 201904 for product 0.4243052178737769 (110/149))\n",
      "Predicting 201904 for product 0.42963654331237705 (111/149))\n",
      "Predicting 201904 for product 0.4482961823474776 (112/149))\n",
      "Predicting 201904 for product 0.4856154604176787 (113/149))\n",
      "Predicting 201904 for product 0.5096064248913794 (114/149))\n",
      "Predicting 201904 for product 0.5469257029615805 (115/149))\n",
      "Predicting 201904 for product 0.5522570284001806 (116/149))\n",
      "Predicting 201904 for product 0.597573294628282 (117/149))\n",
      "Predicting 201904 for product 0.6055702827861822 (118/149))\n",
      "Predicting 201904 for product 0.6268955845405828 (119/149))\n",
      "Predicting 201904 for product 0.6615491998914839 (120/149))\n",
      "Predicting 201904 for product 0.7041998034002851 (121/149))\n",
      "Predicting 201904 for product 0.7255251051546857 (122/149))\n",
      "Predicting 201904 for product 0.8561425784003897 (123/149))\n",
      "Predicting 201904 for product 0.904124507347791 (124/149))\n",
      "Predicting 201904 for product 0.9227841463828915 (125/149))\n",
      "Predicting 201904 for product 0.9414437854179921 (126/149))\n",
      "Predicting 201904 for product 0.9947570398039937 (127/149))\n",
      "Predicting 201904 for product 1.005419690681194 (128/149))\n",
      "Predicting 201904 for product 1.0240793297162945 (129/149))\n",
      "Predicting 201904 for product 1.040073306032095 (130/149))\n",
      "Predicting 201904 for product 1.106714874014597 (131/149))\n",
      "Predicting 201904 for product 1.1227088503303975 (132/149))\n",
      "Predicting 201904 for product 1.2426636726989009 (133/149))\n",
      "Predicting 201904 for product 1.2453293354182011 (134/149))\n",
      "Predicting 201904 for product 1.2479949981375011 (135/149))\n",
      "Predicting 201904 for product 1.2693202998919018 (136/149))\n",
      "Predicting 201904 for product 1.2879799389270024 (137/149))\n",
      "Predicting 201904 for product 1.346624518751604 (138/149))\n",
      "Predicting 201904 for product 1.3492901814709042 (139/149))\n",
      "Predicting 201904 for product 1.3519558441902042 (140/149))\n",
      "Predicting 201904 for product 1.3706154832253048 (141/149))\n",
      "Predicting 201904 for product 1.3786124713832049 (142/149))\n",
      "Predicting 201904 for product 1.4026034358569057 (143/149))\n",
      "Predicting 201904 for product 1.413266086734106 (144/149))\n",
      "Predicting 201904 for product 1.4239287376113063 (145/149))\n",
      "Predicting 201904 for product 1.450585364804307 (146/149))\n",
      "Predicting 201904 for product 1.4532510275236072 (147/149))\n",
      "Predicting 201904 for product 1.4852389801552082 (148/149))\n",
      "Predicting 201904 for product 1.4985672937517085 (149/149))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:32:36,060] Trial 49 finished with value: 1.3661632335849687 and parameters: {'n_layers': 2, 'n_neurons': 346, 'embudo_1': 0.9666514992740858, 'embudo_2': 1.889025121971658, 'embudo_3': 1.6195431920626777, 'dropout_threshold': 0.35425266305342207}. Best is trial 19 with value: 1.3476995419660092.\n",
      "[I 2023-11-25 22:32:36,061] A new study created in memory with name: lstm_10_BO_2.343365459926705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "Best trial for cluster0.7677485281160644:\n",
      "n_layers: 2\n",
      "n_neurons: 327\n",
      "embudo_1: 0.9043218762847649\n",
      "embudo_2: 1.9404980919506725\n",
      "embudo_3: 1.8639902851229038\n",
      "dropout_threshold: 0.3625466315744853\n",
      "Finished training for cluster 0.7677485281160644\n",
      "Training for cluster 2.343365459926705\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 272 neurons, 0.9268232059968521 embudo_1, 1.6993752657874217 embudo_2, and 2.9103981797535434 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_255 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_256 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_257 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.0356 - val_loss: 0.1606\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0183 - val_loss: 0.1533\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0177 - val_loss: 0.1530\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0173 - val_loss: 0.1512\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0171 - val_loss: 0.1490\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0169 - val_loss: 0.1476\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0165 - val_loss: 0.1450\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0166 - val_loss: 0.1437\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.1440\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0156 - val_loss: 0.1372\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.1331\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.1315\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0148 - val_loss: 0.1290\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.1280\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.1259\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.1194\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.1133\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.1028\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.1032\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.0933\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0775\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0861\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0129 - val_loss: 0.0752\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0691\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0130 - val_loss: 0.0788\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0674\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0685\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0653\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0746\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0652\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0582\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0129 - val_loss: 0.0669\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0654\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0589\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0506\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0655\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0114 - val_loss: 0.0634\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0117 - val_loss: 0.0510\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0548\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0419\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0620\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0111 - val_loss: 0.0554\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0112 - val_loss: 0.0517\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0118 - val_loss: 0.0481\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0098 - val_loss: 0.0421\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0414\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0411\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0104 - val_loss: 0.0409\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0393\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - val_loss: 0.0360\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0365\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0436\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0336\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0363\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0089 - val_loss: 0.0387\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0110 - val_loss: 0.0325\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0334\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0308\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0326\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0306\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0303\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0281\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0283\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0300\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0098 - val_loss: 0.0338\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0092 - val_loss: 0.0321\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0231\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0101 - val_loss: 0.0229\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0267\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0232\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0293\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0229\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - val_loss: 0.0218\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0206\n",
      "Epoch 75/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0251\n",
      "Epoch 76/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0104 - val_loss: 0.0249\n",
      "Epoch 77/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - val_loss: 0.0221\n",
      "Epoch 78/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0091 - val_loss: 0.0253\n",
      "Epoch 79/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0091 - val_loss: 0.0226\n",
      "Epoch 80/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0302\n",
      "Epoch 81/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0284\n",
      "Epoch 82/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0092 - val_loss: 0.0255\n",
      "Epoch 83/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0090 - val_loss: 0.0267\n",
      "Epoch 84/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0091 - val_loss: 0.0253\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:33:01,443] Trial 0 finished with value: 2.3049142867825085 and parameters: {'n_layers': 3, 'n_neurons': 272, 'embudo_1': 0.9268232059968521, 'embudo_2': 1.6993752657874217, 'embudo_3': 2.9103981797535434, 'dropout_threshold': 0.464307509397329}. Best is trial 0 with value: 2.3049142867825085.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 127 neurons, 1.1759290858166906 embudo_1, 1.79789236839814 embudo_2, and 1.0790640143814674 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_258 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_259 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 6ms/step - loss: 0.0346 - val_loss: 0.1522\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0183 - val_loss: 0.1463\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0176 - val_loss: 0.1412\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0167 - val_loss: 0.1329\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0158 - val_loss: 0.1261\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0160 - val_loss: 0.1162\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0148 - val_loss: 0.1110\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0144 - val_loss: 0.1054\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0144 - val_loss: 0.1016\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0130 - val_loss: 0.0716\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.0969\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0732\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0907\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0125 - val_loss: 0.0911\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0750\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0109 - val_loss: 0.0608\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0584\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0136 - val_loss: 0.0826\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0770\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0100 - val_loss: 0.0660\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0108 - val_loss: 0.0529\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - val_loss: 0.0706\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0114 - val_loss: 0.0525\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0126 - val_loss: 0.0750\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0107 - val_loss: 0.0805\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0117 - val_loss: 0.0877\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0111 - val_loss: 0.0840\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0768\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0136 - val_loss: 0.0843\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0125 - val_loss: 0.0796\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0114 - val_loss: 0.0738\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0121 - val_loss: 0.0811\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0112 - val_loss: 0.0765\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:33:11,532] Trial 1 finished with value: 2.316250412895822 and parameters: {'n_layers': 2, 'n_neurons': 127, 'embudo_1': 1.1759290858166906, 'embudo_2': 1.79789236839814, 'embudo_3': 1.0790640143814674, 'dropout_threshold': 0.5144613477371351}. Best is trial 0 with value: 2.3049142867825085.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 250 neurons, 1.2336980408531468 embudo_1, 1.3489845405453575 embudo_2, and 0.8859087108600233 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_260 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_261 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_262 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_263 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 3s 10ms/step - loss: 0.0402 - val_loss: 0.1601\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0185 - val_loss: 0.1574\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0181 - val_loss: 0.1554\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0177 - val_loss: 0.1549\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0177 - val_loss: 0.1514\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0173 - val_loss: 0.1501\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.1476\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0162 - val_loss: 0.1450\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0159 - val_loss: 0.1421\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0158 - val_loss: 0.1406\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0156 - val_loss: 0.1357\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0154 - val_loss: 0.1345\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.1346\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.1310\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.1317\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.1308\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.1305\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.1303\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0148 - val_loss: 0.1277\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.1277\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.1269\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.1251\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.1253\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.1218\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.1208\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.1185\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.1190\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.1135\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.1111\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.1081\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.1032\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0156 - val_loss: 0.1028\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.0912\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0849\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0836\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0137 - val_loss: 0.0827\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0719\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0705\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0648\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0558\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0808\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0532\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0443\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0420\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0499\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0452\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0407\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0360\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0425\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0422\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0390\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0399\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0351\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0319\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0308\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0093 - val_loss: 0.0303\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0309\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0309\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0311\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0318\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0297\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0287\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0330\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0281\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0286\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0306\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0292\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0498\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0280\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0119 - val_loss: 0.0271\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.0337\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0259\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0087 - val_loss: 0.0265\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0323\n",
      "Epoch 75/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0287\n",
      "Epoch 76/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0366\n",
      "Epoch 77/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0325\n",
      "Epoch 78/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0352\n",
      "Epoch 79/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0339\n",
      "Epoch 80/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0299\n",
      "Epoch 81/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0297\n",
      "Epoch 82/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0085 - val_loss: 0.0268\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:33:41,542] Trial 2 finished with value: 2.3043183604259245 and parameters: {'n_layers': 4, 'n_neurons': 250, 'embudo_1': 1.2336980408531468, 'embudo_2': 1.3489845405453575, 'embudo_3': 0.8859087108600233, 'dropout_threshold': 0.632565937983213}. Best is trial 2 with value: 2.3043183604259245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 301 neurons, 1.4066463369082651 embudo_1, 2.3904169241360047 embudo_2, and 0.8036383787715523 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_264 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 1s 5ms/step - loss: 0.0222 - val_loss: 0.1148\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.1136\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0131 - val_loss: 0.0879\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0730\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.0924\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0124 - val_loss: 0.0727\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0584\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0567\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0625\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0563\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0116 - val_loss: 0.0604\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0656\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0110 - val_loss: 0.0620\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0112 - val_loss: 0.0699\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0107 - val_loss: 0.0627\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0541\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0572\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0105 - val_loss: 0.0752\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0648\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.0568\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0591\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0103 - val_loss: 0.0768\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0702\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0114 - val_loss: 0.0639\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.0458\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0106 - val_loss: 0.0595\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.0493\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.0658\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0100 - val_loss: 0.0691\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.0597\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.0538\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.0474\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.0540\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 0.0470\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0109 - val_loss: 0.0636\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:33:49,348] Trial 3 finished with value: 2.337135987340442 and parameters: {'n_layers': 1, 'n_neurons': 301, 'embudo_1': 1.4066463369082651, 'embudo_2': 2.3904169241360047, 'embudo_3': 0.8036383787715523, 'dropout_threshold': 0.31510896294953655}. Best is trial 2 with value: 2.3043183604259245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 438 neurons, 1.1388772820917237 embudo_1, 2.0395075491797474 embudo_2, and 3.3106877233139897 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_265 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 1s 7ms/step - loss: 0.0267 - val_loss: 0.1099\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0756\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.0781\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0766\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0125 - val_loss: 0.0712\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0673\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.0779\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0110 - val_loss: 0.0500\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0786\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0601\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.0618\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0668\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0642\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0636\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0562\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.0585\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.0825\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.0709\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:33:54,629] Trial 4 finished with value: 2.3092073319773334 and parameters: {'n_layers': 1, 'n_neurons': 438, 'embudo_1': 1.1388772820917237, 'embudo_2': 2.0395075491797474, 'embudo_3': 3.3106877233139897, 'dropout_threshold': 0.13690272129607342}. Best is trial 2 with value: 2.3043183604259245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 86 neurons, 1.3501311905713682 embudo_1, 0.8316671112794376 embudo_2, and 1.9407153985205805 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_266 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 1s 4ms/step - loss: 0.0226 - val_loss: 0.1271\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.1058\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0131 - val_loss: 0.0959\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.0718\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0807\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0696\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0683\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0721\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0599\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0615\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0112 - val_loss: 0.0580\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.0595\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0111 - val_loss: 0.0583\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0112 - val_loss: 0.0622\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0112 - val_loss: 0.0715\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 0.0559\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0107 - val_loss: 0.0629\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0631\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0642\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0104 - val_loss: 0.0558\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0511\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.0553\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 0.0626\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0553\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 0.0512\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0101 - val_loss: 0.0530\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.0496\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.0545\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0498\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.0566\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0103 - val_loss: 0.0535\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.0464\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.0444\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0097 - val_loss: 0.0423\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.0489\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.0437\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.0462\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.0442\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.0421\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 0.0534\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.0424\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 0.0460\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0095 - val_loss: 0.0482\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0095 - val_loss: 0.0459\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 0.0354\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 0.0374\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 0.0412\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0095 - val_loss: 0.0350\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 0.0346\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.0354\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 0.0478\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.0358\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0094 - val_loss: 0.0440\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.0228\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.0382\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.0380\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.0408\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0094 - val_loss: 0.0383\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.0355\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0094 - val_loss: 0.0336\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0090 - val_loss: 0.0346\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0091 - val_loss: 0.0332\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - val_loss: 0.0387\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0095 - val_loss: 0.0324\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:34:05,571] Trial 5 finished with value: 2.3618056273170778 and parameters: {'n_layers': 1, 'n_neurons': 86, 'embudo_1': 1.3501311905713682, 'embudo_2': 0.8316671112794376, 'embudo_3': 1.9407153985205805, 'dropout_threshold': 0.65566251232445}. Best is trial 2 with value: 2.3043183604259245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 164 neurons, 1.4710827789785532 embudo_1, 1.3528661557327357 embudo_2, and 1.4260627198129412 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_267 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_268 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_269 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 3s 13ms/step - loss: 0.0359 - val_loss: 0.1518\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0171 - val_loss: 0.1478\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0165 - val_loss: 0.1435\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.1368\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0155 - val_loss: 0.1315\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0149 - val_loss: 0.1184\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.1131\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0828\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0801\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.0988\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0987\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0921\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0922\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0857\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0856\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0703\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0868\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0114 - val_loss: 0.0795\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0800\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0729\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0106 - val_loss: 0.0580\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0106 - val_loss: 0.0632\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0114 - val_loss: 0.0752\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0114 - val_loss: 0.0594\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0108 - val_loss: 0.0727\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0107 - val_loss: 0.0677\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0586\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0115 - val_loss: 0.0798\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0114 - val_loss: 0.0801\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0848\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0798\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:34:17,971] Trial 6 finished with value: 2.3360373071687577 and parameters: {'n_layers': 3, 'n_neurons': 164, 'embudo_1': 1.4710827789785532, 'embudo_2': 1.3528661557327357, 'embudo_3': 1.4260627198129412, 'dropout_threshold': 0.19602096507876826}. Best is trial 2 with value: 2.3043183604259245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 413 neurons, 0.900732721033167 embudo_1, 1.9690903490511633 embudo_2, and 1.5081107817240382 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_270 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 1s 5ms/step - loss: 0.0222 - val_loss: 0.1035\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0131 - val_loss: 0.0794\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0695\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0527\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.0631\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0619\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.0794\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0604\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0694\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0116 - val_loss: 0.0763\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0731\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0537\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0726\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0631\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:34:22,707] Trial 7 finished with value: 2.3125398872891783 and parameters: {'n_layers': 1, 'n_neurons': 413, 'embudo_1': 0.900732721033167, 'embudo_2': 1.9690903490511633, 'embudo_3': 1.5081107817240382, 'dropout_threshold': 0.20005095703462333}. Best is trial 2 with value: 2.3043183604259245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 410 neurons, 1.2292236740917297 embudo_1, 2.135063355437702 embudo_2, and 2.7640559352067005 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_271 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_272 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_273 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_274 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 3s 10ms/step - loss: 0.0346 - val_loss: 0.1588\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0174 - val_loss: 0.1533\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0169 - val_loss: 0.1490\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0165 - val_loss: 0.1421\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0160 - val_loss: 0.1411\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0156 - val_loss: 0.1337\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.1307\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0159 - val_loss: 0.1188\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.1288\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0783\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0156 - val_loss: 0.1242\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.1109\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0131 - val_loss: 0.1015\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0191 - val_loss: 0.1198\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.1127\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.1081\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0131 - val_loss: 0.1031\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0132 - val_loss: 0.1048\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0918\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0948\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:34:34,204] Trial 8 finished with value: 2.3455900795472275 and parameters: {'n_layers': 4, 'n_neurons': 410, 'embudo_1': 1.2292236740917297, 'embudo_2': 2.135063355437702, 'embudo_3': 2.7640559352067005, 'dropout_threshold': 0.30303794592023103}. Best is trial 2 with value: 2.3043183604259245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 42 neurons, 1.407896898325186 embudo_1, 1.1365942889324026 embudo_2, and 0.9280214382114739 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_275 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_276 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_277 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 3s 8ms/step - loss: 0.0664 - val_loss: 0.1504\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0252 - val_loss: 0.1578\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0211 - val_loss: 0.1587\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0200 - val_loss: 0.1600\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0190 - val_loss: 0.1608\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0189 - val_loss: 0.1619\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0187 - val_loss: 0.1618\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0182 - val_loss: 0.1630\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0182 - val_loss: 0.1627\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0179 - val_loss: 0.1624\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0176 - val_loss: 0.1615\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:34:41,569] Trial 9 finished with value: 2.6336115598331373 and parameters: {'n_layers': 3, 'n_neurons': 42, 'embudo_1': 1.407896898325186, 'embudo_2': 1.1365942889324026, 'embudo_3': 0.9280214382114739, 'dropout_threshold': 0.6053907334951087}. Best is trial 2 with value: 2.3043183604259245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.3333333333333333, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.36363636363636365, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(160.33333333333334, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(43.72727272727273, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.19999999999999998, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05454545454545454, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5666666666666667, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15454545454545454, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.9333333333333332, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2545454545454545, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.18666666666666668, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05090909090909091, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 201 neurons, 1.2872605270769053 embudo_1, 1.4001205636524334 embudo_2, and 2.1011381151291233 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_278 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_279 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_280 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_281 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 3s 10ms/step - loss: 0.0431 - val_loss: 0.1604\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0185 - val_loss: 0.1587\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0181 - val_loss: 0.1580\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0181 - val_loss: 0.1563\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0178 - val_loss: 0.1557\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0175 - val_loss: 0.1533\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0171 - val_loss: 0.1521\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.1473\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0158 - val_loss: 0.1453\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0158 - val_loss: 0.1400\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0155 - val_loss: 0.1386\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0155 - val_loss: 0.1367\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.1342\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.1322\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.1319\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0148 - val_loss: 0.1309\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.1325\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.1285\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.1288\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.1277\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0148 - val_loss: 0.1255\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.1239\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.1224\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.1218\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.1185\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.1169\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.1156\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.1142\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.1114\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.1116\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.1044\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.1034\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.1008\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0992\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0955\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0928\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0131 - val_loss: 0.0902\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0873\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0878\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0827\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0800\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0813\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0740\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0708\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0664\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0588\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0618\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0582\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0556\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0545\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0482\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0413\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0403\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0427\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0399\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0423\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0087 - val_loss: 0.0367\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0363\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0396\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0380\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0386\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0377\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.0368\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0420\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0388\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0349\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0095 - val_loss: 0.0347\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0370\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0339\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0361\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0345\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0322\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0332\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0330\n",
      "Epoch 75/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0284\n",
      "Epoch 76/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0093 - val_loss: 0.0308\n",
      "Epoch 77/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0295\n",
      "Epoch 78/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0298\n",
      "Epoch 79/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0295\n",
      "Epoch 80/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0092 - val_loss: 0.0290\n",
      "Epoch 81/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0306\n",
      "Epoch 82/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0339\n",
      "Epoch 83/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0094 - val_loss: 0.0340\n",
      "Epoch 84/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0300\n",
      "Epoch 85/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0092 - val_loss: 0.0288\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:35:13,067] Trial 10 finished with value: 2.3080650859052705 and parameters: {'n_layers': 4, 'n_neurons': 201, 'embudo_1': 1.2872605270769053, 'embudo_2': 1.4001205636524334, 'embudo_3': 2.1011381151291233, 'dropout_threshold': 0.5358935615411051}. Best is trial 2 with value: 2.3043183604259245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.36363636363636365, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(43.72727272727273, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05454545454545454, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15454545454545454, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2545454545454545, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05090909090909091, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 292 neurons, 1.0285162537877717 embudo_1, 1.611530988184325 embudo_2, and 2.6070179904719577 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_282 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_283 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_284 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_285 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 3s 10ms/step - loss: 0.0387 - val_loss: 0.1589\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0180 - val_loss: 0.1579\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0176 - val_loss: 0.1554\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0173 - val_loss: 0.1519\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0170 - val_loss: 0.1496\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0166 - val_loss: 0.1469\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.1439\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.1419\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.1354\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.1346\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0151 - val_loss: 0.1329\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.1315\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.1271\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.1220\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.1150\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0865\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0203 - val_loss: 0.1483\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0154 - val_loss: 0.1045\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0966\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0981\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0913\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.1013\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.0893\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0809\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.0817\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0115 - val_loss: 0.0709\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0136 - val_loss: 0.0782\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0119 - val_loss: 0.0733\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0681\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0786\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0736\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0688\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0119 - val_loss: 0.0703\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0829\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0723\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0118 - val_loss: 0.0733\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0827\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0723\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0113 - val_loss: 0.0698\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:35:30,374] Trial 11 finished with value: 2.6336115598331373 and parameters: {'n_layers': 4, 'n_neurons': 292, 'embudo_1': 1.0285162537877717, 'embudo_2': 1.611530988184325, 'embudo_3': 2.6070179904719577, 'dropout_threshold': 0.4445641094962106}. Best is trial 2 with value: 2.3043183604259245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.3333333333333333, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(40.083333333333336, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.049999999999999996, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14166666666666666, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2333333333333333, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04666666666666667, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 253 neurons, 1.0930229634444228 embudo_1, 1.6193092195499887 embudo_2, and 3.293213727963035 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_286 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_287 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_288 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 3s 14ms/step - loss: 0.0360 - val_loss: 0.1586\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0180 - val_loss: 0.1552\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.1504\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0173 - val_loss: 0.1497\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0168 - val_loss: 0.1454\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.1457\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.1409\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0159 - val_loss: 0.1378\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - val_loss: 0.1369\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.1276\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.1136\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0911\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0864\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0786\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0115 - val_loss: 0.0977\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.1003\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0125 - val_loss: 0.0749\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0149 - val_loss: 0.0968\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0894\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0816\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0731\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0171 - val_loss: 0.1118\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0969\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0851\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0848\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0785\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0854\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0126 - val_loss: 0.0873\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0780\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0831\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0825\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:35:42,859] Trial 12 finished with value: 2.6336115598331373 and parameters: {'n_layers': 3, 'n_neurons': 253, 'embudo_1': 1.0930229634444228, 'embudo_2': 1.6193092195499887, 'embudo_3': 3.293213727963035, 'dropout_threshold': 0.44844032415850177}. Best is trial 2 with value: 2.3043183604259245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.3076923076923077, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(37.0, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04615384615384615, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13076923076923078, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.21538461538461537, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04307692307692308, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 347 neurons, 1.0098101190623776 embudo_1, 1.3793747045950662 embudo_2, and 0.7004865966830394 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_289 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_290 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 7ms/step - loss: 0.0256 - val_loss: 0.1435\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0167 - val_loss: 0.1403\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0163 - val_loss: 0.1265\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0145 - val_loss: 0.1025\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - val_loss: 0.0758\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0126 - val_loss: 0.1319\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0136 - val_loss: 0.0751\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0148 - val_loss: 0.0737\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0127 - val_loss: 0.0935\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - val_loss: 0.0786\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0134 - val_loss: 0.0810\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0119 - val_loss: 0.0937\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0942\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - val_loss: 0.0785\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0125 - val_loss: 0.0844\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0107 - val_loss: 0.0642\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0143 - val_loss: 0.0927\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0739\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0126 - val_loss: 0.1035\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0118 - val_loss: 0.0804\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0123 - val_loss: 0.0783\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0134 - val_loss: 0.0745\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0119 - val_loss: 0.0710\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0128 - val_loss: 0.0688\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0106 - val_loss: 0.0692\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0137 - val_loss: 0.1030\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:35:51,811] Trial 13 finished with value: 2.3542444184860947 and parameters: {'n_layers': 2, 'n_neurons': 347, 'embudo_1': 1.0098101190623776, 'embudo_2': 1.3793747045950662, 'embudo_3': 0.7004865966830394, 'dropout_threshold': 0.5767909730441508}. Best is trial 2 with value: 2.3043183604259245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2857142857142857, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(34.357142857142854, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04285714285714286, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12142857142857143, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.19999999999999998, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 220 neurons, 1.2420551463459542 embudo_1, 1.7419161543699893 embudo_2, and 2.388161301216385 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_291 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_292 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_293 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_294 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 3s 11ms/step - loss: 0.0425 - val_loss: 0.1631\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0189 - val_loss: 0.1620\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0187 - val_loss: 0.1597\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0181 - val_loss: 0.1588\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0180 - val_loss: 0.1576\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0177 - val_loss: 0.1582\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0176 - val_loss: 0.1559\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0173 - val_loss: 0.1524\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0168 - val_loss: 0.1490\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0161 - val_loss: 0.1463\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.1414\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0156 - val_loss: 0.1401\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0154 - val_loss: 0.1362\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0158 - val_loss: 0.1350\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.1340\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.1328\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.1313\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.1332\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.1314\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.1300\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.1298\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0148 - val_loss: 0.1294\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.1288\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.1275\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.1269\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.1254\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0140 - val_loss: 0.1248\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.1218\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.1212\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.1197\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.1196\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.1135\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.1134\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.1088\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.1067\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.1020\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0137 - val_loss: 0.1029\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.1010\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0974\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0988\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0136 - val_loss: 0.0931\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0878\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0844\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0760\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0713\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.0750\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0674\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.0643\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.0636\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0586\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0508\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0485\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0436\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0455\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0434\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0440\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0132 - val_loss: 0.0510\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0471\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.0455\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0109 - val_loss: 0.0415\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0376\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0384\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0399\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0382\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0354\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0093 - val_loss: 0.0332\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0346\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0385\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0334\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0352\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0360\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0339\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0334\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0338\n",
      "Epoch 75/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0330\n",
      "Epoch 76/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0337\n",
      "Epoch 77/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0345\n",
      "Epoch 78/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0088 - val_loss: 0.0314\n",
      "Epoch 79/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0296\n",
      "Epoch 80/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0322\n",
      "Epoch 81/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0375\n",
      "Epoch 82/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0319\n",
      "Epoch 83/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0383\n",
      "Epoch 84/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0360\n",
      "Epoch 85/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0404\n",
      "Epoch 86/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0373\n",
      "Epoch 87/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0092 - val_loss: 0.0352\n",
      "Epoch 88/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0359\n",
      "Epoch 89/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0368\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:36:24,893] Trial 14 finished with value: 2.3159874962698312 and parameters: {'n_layers': 4, 'n_neurons': 220, 'embudo_1': 1.2420551463459542, 'embudo_2': 1.7419161543699893, 'embudo_3': 2.388161301216385, 'dropout_threshold': 0.62095079631754}. Best is trial 2 with value: 2.3043183604259245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.26666666666666666, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(32.06666666666667, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11333333333333333, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.18666666666666665, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.037333333333333336, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 348 neurons, 0.9129862032667905 embudo_1, 1.1600777476590647 embudo_2, and 3.0100120057646755 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_295 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_296 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_297 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 9ms/step - loss: 0.0322 - val_loss: 0.1565\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0180 - val_loss: 0.1539\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.1516\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0172 - val_loss: 0.1485\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0169 - val_loss: 0.1479\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.1446\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.1409\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0158 - val_loss: 0.1386\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0158 - val_loss: 0.1342\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0156 - val_loss: 0.1316\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.1266\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0147 - val_loss: 0.1178\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.1117\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0986\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0948\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.1335\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0800\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0798\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0158 - val_loss: 0.1054\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0734\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0934\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0708\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0111 - val_loss: 0.0724\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0685\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0891\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0807\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0667\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0112 - val_loss: 0.0731\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0521\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0593\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0598\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0622\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0577\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0422\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0618\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0591\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0452\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0488\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0478\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0445\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0103 - val_loss: 0.0380\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0104 - val_loss: 0.0309\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0380\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0400\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0367\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0112 - val_loss: 0.0421\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0429\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0385\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0400\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0408\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - val_loss: 0.0313\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0331\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:36:42,653] Trial 15 finished with value: 2.3081495244536145 and parameters: {'n_layers': 3, 'n_neurons': 348, 'embudo_1': 0.9129862032667905, 'embudo_2': 1.1600777476590647, 'embudo_3': 3.0100120057646755, 'dropout_threshold': 0.4952906925057348}. Best is trial 2 with value: 2.3043183604259245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.25, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(30.0625, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0375, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10625, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.175, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.035, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 477 neurons, 1.1305391809851244 embudo_1, 1.5923294928856713 embudo_2, and 1.8168406440454037 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_298 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_299 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 7ms/step - loss: 0.0270 - val_loss: 0.1521\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0172 - val_loss: 0.1394\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0166 - val_loss: 0.1339\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - val_loss: 0.1195\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0157 - val_loss: 0.1057\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0126 - val_loss: 0.0605\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0142 - val_loss: 0.0951\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0126 - val_loss: 0.0744\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0143 - val_loss: 0.0971\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0127 - val_loss: 0.0861\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0143 - val_loss: 0.0938\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0905\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.0858\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0107 - val_loss: 0.0786\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0152 - val_loss: 0.1058\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0122 - val_loss: 0.0831\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:36:49,466] Trial 16 finished with value: 2.3324560241591903 and parameters: {'n_layers': 2, 'n_neurons': 477, 'embudo_1': 1.1305391809851244, 'embudo_2': 1.5923294928856713, 'embudo_3': 1.8168406440454037, 'dropout_threshold': 0.572227733413083}. Best is trial 2 with value: 2.3043183604259245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.23529411764705882, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(28.294117647058822, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03529411764705882, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.16470588235294117, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03294117647058824, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 323 neurons, 1.2828256332676038 embudo_1, 1.8173072068542793 embudo_2, and 2.254789694808154 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_300 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_301 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_302 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_303 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 3s 10ms/step - loss: 0.0361 - val_loss: 0.1617\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0178 - val_loss: 0.1561\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0174 - val_loss: 0.1531\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0171 - val_loss: 0.1481\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0168 - val_loss: 0.1441\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.1408\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0159 - val_loss: 0.1336\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.1321\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.1240\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.0920\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.1114\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.1074\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0148 - val_loss: 0.1113\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0906\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0119 - val_loss: 0.1063\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0118 - val_loss: 0.0779\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0296 - val_loss: 0.1168\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.1069\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0953\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0131 - val_loss: 0.0932\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.1035\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0160 - val_loss: 0.1143\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.1066\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.1003\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0965\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0946\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:37:03,042] Trial 17 finished with value: 2.315198020468437 and parameters: {'n_layers': 4, 'n_neurons': 323, 'embudo_1': 1.2828256332676038, 'embudo_2': 1.8173072068542793, 'embudo_3': 2.254789694808154, 'dropout_threshold': 0.41808899129242294}. Best is trial 2 with value: 2.3043183604259245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2222222222222222, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(26.72222222222222, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03333333333333333, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09444444444444444, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15555555555555556, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.031111111111111114, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 235 neurons, 1.20013325060173 embudo_1, 2.261759038493119 embudo_2, and 2.5513053414936797 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_304 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_305 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_306 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.0392 - val_loss: 0.1576\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0189 - val_loss: 0.1569\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0186 - val_loss: 0.1554\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0180 - val_loss: 0.1554\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0179 - val_loss: 0.1548\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0176 - val_loss: 0.1543\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0175 - val_loss: 0.1525\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0173 - val_loss: 0.1524\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0170 - val_loss: 0.1508\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0172 - val_loss: 0.1482\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0165 - val_loss: 0.1462\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.1429\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.1406\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0157 - val_loss: 0.1383\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0154 - val_loss: 0.1358\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.1341\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.1315\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.1324\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.1298\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0157 - val_loss: 0.1300\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.1263\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.1181\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.1135\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0151 - val_loss: 0.1120\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.1050\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.1032\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0844\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0843\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0751\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0157 - val_loss: 0.0845\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0850\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0785\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0124 - val_loss: 0.0819\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0121 - val_loss: 0.0760\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0816\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0130 - val_loss: 0.0651\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0592\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0118 - val_loss: 0.0538\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0507\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0520\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0451\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0123 - val_loss: 0.0455\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0481\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0452\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0494\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.0619\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0114 - val_loss: 0.0492\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0419\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0418\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0415\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0329\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0330\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0306\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0464\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0081 - val_loss: 0.0298\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0356\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0109 - val_loss: 0.0334\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0135 - val_loss: 0.0341\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0371\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0405\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0339\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0119 - val_loss: 0.0456\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0405\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0417\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0405\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:37:23,700] Trial 18 finished with value: 2.3026451237466206 and parameters: {'n_layers': 3, 'n_neurons': 235, 'embudo_1': 1.20013325060173, 'embudo_2': 2.261759038493119, 'embudo_3': 2.5513053414936797, 'dropout_threshold': 0.6597607010916747}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.21052631578947367, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(25.31578947368421, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.031578947368421054, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08947368421052632, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14736842105263157, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02947368421052632, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 153 neurons, 1.19704795825668 embudo_1, 2.472511680515964 embudo_2, and 2.4138393035913843 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_307 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_308 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_309 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_310 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 4s 11ms/step - loss: 0.0462 - val_loss: 0.1594\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0193 - val_loss: 0.1616\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0189 - val_loss: 0.1622\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0185 - val_loss: 0.1618\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0183 - val_loss: 0.1624\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0184 - val_loss: 0.1613\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0182 - val_loss: 0.1611\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0181 - val_loss: 0.1608\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0174 - val_loss: 0.1590\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0172 - val_loss: 0.1557\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0170 - val_loss: 0.1516\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0166 - val_loss: 0.1493\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0164 - val_loss: 0.1455\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0160 - val_loss: 0.1448\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0160 - val_loss: 0.1420\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0159 - val_loss: 0.1402\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.1400\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.1371\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.1363\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0158 - val_loss: 0.1325\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.1314\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.1327\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.1314\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.1314\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0148 - val_loss: 0.1348\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.1305\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.1338\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0151 - val_loss: 0.1280\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.1293\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.1285\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0151 - val_loss: 0.1275\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0142 - val_loss: 0.1267\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.1249\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.1249\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.1248\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.1241\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.1251\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.1230\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0155 - val_loss: 0.1224\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.1219\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.1215\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.1220\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.1208\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0145 - val_loss: 0.1194\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0145 - val_loss: 0.1191\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.1200\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0132 - val_loss: 0.1180\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.1183\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.1172\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.1154\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.1146\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.1136\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.1125\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0131 - val_loss: 0.1130\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.1104\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.1114\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.1093\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.1085\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0132 - val_loss: 0.1105\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.1067\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.1066\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.1066\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.1052\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0131 - val_loss: 0.1063\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.1042\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.1050\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.1051\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0132 - val_loss: 0.1004\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.1001\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0977\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0989\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0959\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0962\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0132 - val_loss: 0.0953\n",
      "Epoch 75/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0930\n",
      "Epoch 76/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0915\n",
      "Epoch 77/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0132 - val_loss: 0.0967\n",
      "Epoch 78/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0913\n",
      "Epoch 79/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0866\n",
      "Epoch 80/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0876\n",
      "Epoch 81/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0849\n",
      "Epoch 82/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.0854\n",
      "Epoch 83/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0833\n",
      "Epoch 84/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0836\n",
      "Epoch 85/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0122 - val_loss: 0.0819\n",
      "Epoch 86/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0827\n",
      "Epoch 87/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0793\n",
      "Epoch 88/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0807\n",
      "Epoch 89/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.0799\n",
      "Epoch 90/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0132 - val_loss: 0.0786\n",
      "Epoch 91/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0766\n",
      "Epoch 92/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0756\n",
      "Epoch 93/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0742\n",
      "Epoch 94/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0118 - val_loss: 0.0740\n",
      "Epoch 95/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0733\n",
      "Epoch 96/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0728\n",
      "Epoch 97/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0708\n",
      "Epoch 98/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0717\n",
      "Epoch 99/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0115 - val_loss: 0.0686\n",
      "Epoch 100/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0681\n",
      "Epoch 101/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.0671\n",
      "Epoch 102/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0672\n",
      "Epoch 103/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0667\n",
      "Epoch 104/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0672\n",
      "Epoch 105/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0654\n",
      "Epoch 106/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0641\n",
      "Epoch 107/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.0642\n",
      "Epoch 108/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0626\n",
      "Epoch 109/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0619\n",
      "Epoch 110/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0602\n",
      "Epoch 111/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0601\n",
      "Epoch 112/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0601\n",
      "Epoch 113/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0585\n",
      "Epoch 114/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0574\n",
      "Epoch 115/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0574\n",
      "Epoch 116/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0568\n",
      "Epoch 117/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0554\n",
      "Epoch 118/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0109 - val_loss: 0.0547\n",
      "Epoch 119/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0543\n",
      "Epoch 120/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0106 - val_loss: 0.0539\n",
      "Epoch 121/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0530\n",
      "Epoch 122/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0525\n",
      "Epoch 123/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0091 - val_loss: 0.0512\n",
      "Epoch 124/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0501\n",
      "Epoch 125/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.0520\n",
      "Epoch 126/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0512\n",
      "Epoch 127/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0493\n",
      "Epoch 128/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0489\n",
      "Epoch 129/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0506\n",
      "Epoch 130/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0486\n",
      "Epoch 131/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.0472\n",
      "Epoch 132/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0463\n",
      "Epoch 133/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0456\n",
      "Epoch 134/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0447\n",
      "Epoch 135/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0445\n",
      "Epoch 136/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0431\n",
      "Epoch 137/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0440\n",
      "Epoch 138/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0086 - val_loss: 0.0439\n",
      "Epoch 139/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0427\n",
      "Epoch 140/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0430\n",
      "Epoch 141/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0403\n",
      "Epoch 142/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0409\n",
      "Epoch 143/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0406\n",
      "Epoch 144/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0093 - val_loss: 0.0390\n",
      "Epoch 145/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0085 - val_loss: 0.0381\n",
      "Epoch 146/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0084 - val_loss: 0.0384\n",
      "Epoch 147/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0369\n",
      "Epoch 148/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0391\n",
      "Epoch 149/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0378\n",
      "Epoch 150/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0375\n",
      "Epoch 151/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0095 - val_loss: 0.0352\n",
      "Epoch 152/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0353\n",
      "Epoch 153/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0357\n",
      "Epoch 154/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0359\n",
      "Epoch 155/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0390\n",
      "Epoch 156/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0362\n",
      "Epoch 157/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0351\n",
      "Epoch 158/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0356\n",
      "Epoch 159/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0090 - val_loss: 0.0348\n",
      "Epoch 160/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0118 - val_loss: 0.0370\n",
      "Epoch 161/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0360\n",
      "Epoch 162/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0355\n",
      "Epoch 163/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0360\n",
      "Epoch 164/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0352\n",
      "Epoch 165/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0351\n",
      "Epoch 166/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0354\n",
      "Epoch 167/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0338\n",
      "Epoch 168/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0356\n",
      "Epoch 169/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0342\n",
      "Epoch 170/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0346\n",
      "Epoch 171/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0356\n",
      "Epoch 172/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0344\n",
      "Epoch 173/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0349\n",
      "Epoch 174/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0103 - val_loss: 0.0353\n",
      "Epoch 175/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0341\n",
      "Epoch 176/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0079 - val_loss: 0.0343\n",
      "Epoch 177/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0331\n",
      "Epoch 178/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0096 - val_loss: 0.0331\n",
      "Epoch 179/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0321\n",
      "Epoch 180/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0340\n",
      "Epoch 181/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0336\n",
      "Epoch 182/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0338\n",
      "Epoch 183/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0321\n",
      "Epoch 184/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0094 - val_loss: 0.0326\n",
      "Epoch 185/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0327\n",
      "Epoch 186/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0331\n",
      "Epoch 187/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0336\n",
      "Epoch 188/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0331\n",
      "Epoch 189/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0318\n",
      "Epoch 190/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.0326\n",
      "Epoch 191/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0118 - val_loss: 0.0347\n",
      "Epoch 192/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0355\n",
      "Epoch 193/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0393\n",
      "Epoch 194/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0342\n",
      "Epoch 195/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0069 - val_loss: 0.0335\n",
      "Epoch 196/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0341\n",
      "Epoch 197/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0336\n",
      "Epoch 198/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0334\n",
      "Epoch 199/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0092 - val_loss: 0.0327\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:38:32,068] Trial 19 finished with value: 2.3201049712844832 and parameters: {'n_layers': 4, 'n_neurons': 153, 'embudo_1': 1.19704795825668, 'embudo_2': 2.472511680515964, 'embudo_3': 2.4138393035913843, 'dropout_threshold': 0.656384263144529}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(24.05, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08499999999999999, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13999999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.028000000000000004, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 208 neurons, 1.2890229528696557 embudo_1, 2.2579191091185615 embudo_2, and 1.2214344313342904 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_311 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_312 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_313 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 9ms/step - loss: 0.0363 - val_loss: 0.1569\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0183 - val_loss: 0.1550\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0178 - val_loss: 0.1527\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0173 - val_loss: 0.1489\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0173 - val_loss: 0.1451\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0162 - val_loss: 0.1429\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.1390\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.1333\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0154 - val_loss: 0.1265\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.1227\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0989\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.1231\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0154 - val_loss: 0.1241\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0141 - val_loss: 0.0909\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0972\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0130 - val_loss: 0.1068\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0922\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0193 - val_loss: 0.1177\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.1074\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0903\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - val_loss: 0.0869\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0930\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0927\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0810\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0833\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0110 - val_loss: 0.0693\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0795\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0751\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0690\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0748\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0788\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0718\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0116 - val_loss: 0.0650\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0760\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0123 - val_loss: 0.0684\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0122 - val_loss: 0.0670\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0699\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0599\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0114 - val_loss: 0.0573\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0451\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0647\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0092 - val_loss: 0.0532\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0591\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0523\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0127 - val_loss: 0.0482\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - val_loss: 0.0419\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.0512\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0116 - val_loss: 0.0454\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0441\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0579\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0596\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0563\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0566\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0111 - val_loss: 0.0532\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0129 - val_loss: 0.0496\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0410\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0438\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0106 - val_loss: 0.0444\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0444\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0101 - val_loss: 0.0381\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0435\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.0399\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0352\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0338\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0347\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0394\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0477\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0356\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0331\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0301\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0318\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - val_loss: 0.0355\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0377\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0109 - val_loss: 0.0368\n",
      "Epoch 75/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0334\n",
      "Epoch 76/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - val_loss: 0.0309\n",
      "Epoch 77/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - val_loss: 0.0321\n",
      "Epoch 78/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0310\n",
      "Epoch 79/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0316\n",
      "Epoch 80/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0311\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:38:56,512] Trial 20 finished with value: 2.313391374544794 and parameters: {'n_layers': 3, 'n_neurons': 208, 'embudo_1': 1.2890229528696557, 'embudo_2': 2.2579191091185615, 'embudo_3': 1.2214344313342904, 'dropout_threshold': 0.5831619833671252}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(24.05, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08499999999999999, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13999999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.028000000000000004, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 254 neurons, 1.0724113465737757 embudo_1, 2.296888065125444 embudo_2, and 2.844434160457161 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_314 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_315 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_316 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 3s 8ms/step - loss: 0.0356 - val_loss: 0.1572\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0191 - val_loss: 0.1564\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0183 - val_loss: 0.1559\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0180 - val_loss: 0.1562\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0178 - val_loss: 0.1542\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.1533\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0172 - val_loss: 0.1496\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0170 - val_loss: 0.1466\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.1465\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.1444\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.1389\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0159 - val_loss: 0.1386\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0156 - val_loss: 0.1349\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0150 - val_loss: 0.1338\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0155 - val_loss: 0.1317\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0154 - val_loss: 0.1304\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.1293\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.1277\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.1255\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.1246\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.1258\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.1206\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.1166\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.1109\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.1115\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0140 - val_loss: 0.1113\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.1067\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.1097\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0115 - val_loss: 0.0991\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0872\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0857\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0748\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0723\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - val_loss: 0.0740\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0681\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0657\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0648\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0612\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0107 - val_loss: 0.0499\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - val_loss: 0.0478\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0127 - val_loss: 0.0470\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0473\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0459\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0482\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0111 - val_loss: 0.0472\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0462\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0113 - val_loss: 0.0504\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0485\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0373\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0459\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0377\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0360\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0296\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0304\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0122 - val_loss: 0.0351\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0096 - val_loss: 0.0369\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0107 - val_loss: 0.0427\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0365\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0434\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0115 - val_loss: 0.0422\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0429\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0418\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0368\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:39:17,339] Trial 21 finished with value: 2.3039374883305825 and parameters: {'n_layers': 3, 'n_neurons': 254, 'embudo_1': 1.0724113465737757, 'embudo_2': 2.296888065125444, 'embudo_3': 2.844434160457161, 'dropout_threshold': 0.6558295132954692}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.19047619047619047, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(22.904761904761905, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02857142857142857, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08095238095238096, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13333333333333333, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02666666666666667, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 246 neurons, 1.164789448278051 embudo_1, 2.2834958089730186 embudo_2, and 2.5828669759429213 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_317 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_318 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 7ms/step - loss: 0.0332 - val_loss: 0.1547\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0192 - val_loss: 0.1510\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0184 - val_loss: 0.1498\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0180 - val_loss: 0.1508\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0174 - val_loss: 0.1485\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0172 - val_loss: 0.1419\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0168 - val_loss: 0.1422\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0157 - val_loss: 0.1359\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0143 - val_loss: 0.1145\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0148 - val_loss: 0.1230\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0133 - val_loss: 0.1052\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0153 - val_loss: 0.1206\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.1121\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - val_loss: 0.1045\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0118 - val_loss: 0.1004\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - val_loss: 0.1078\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0149 - val_loss: 0.1225\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0136 - val_loss: 0.1115\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0127 - val_loss: 0.1017\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - val_loss: 0.0974\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0135 - val_loss: 0.0903\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0125 - val_loss: 0.0883\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0135 - val_loss: 0.0969\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0123 - val_loss: 0.0916\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0136 - val_loss: 0.0932\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0114 - val_loss: 0.0859\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0776\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0151 - val_loss: 0.1179\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0142 - val_loss: 0.1130\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0122 - val_loss: 0.1025\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0114 - val_loss: 0.0971\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0117 - val_loss: 0.0912\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0140 - val_loss: 0.0979\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0113 - val_loss: 0.0901\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0139 - val_loss: 0.0925\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0119 - val_loss: 0.0819\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0126 - val_loss: 0.0787\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:39:28,026] Trial 22 finished with value: 2.6336115598331373 and parameters: {'n_layers': 2, 'n_neurons': 246, 'embudo_1': 1.164789448278051, 'embudo_2': 2.2834958089730186, 'embudo_3': 2.5828669759429213, 'dropout_threshold': 0.6591894974313793}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.18181818181818182, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(21.863636363636363, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02727272727272727, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07727272727272727, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12727272727272726, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.025454545454545455, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 178 neurons, 1.1013898264114814 embudo_1, 2.484342699784369 embudo_2, and 1.7273634162329072 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_319 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_320 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_321 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.0394 - val_loss: 0.1552\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0185 - val_loss: 0.1532\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0179 - val_loss: 0.1533\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0178 - val_loss: 0.1523\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0173 - val_loss: 0.1500\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0171 - val_loss: 0.1477\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0170 - val_loss: 0.1459\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.1446\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.1436\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.1418\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - val_loss: 0.1399\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0159 - val_loss: 0.1374\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0151 - val_loss: 0.1340\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0156 - val_loss: 0.1322\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0149 - val_loss: 0.1311\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0150 - val_loss: 0.1283\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.1198\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0859\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.1104\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.1058\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0133 - val_loss: 0.0862\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0126 - val_loss: 0.0957\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0902\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.1002\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - val_loss: 0.0934\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0789\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0731\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0594\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0241 - val_loss: 0.0976\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0126 - val_loss: 0.0949\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0883\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0128 - val_loss: 0.0813\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0780\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0105 - val_loss: 0.0773\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0716\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0712\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0720\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0722\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:39:42,238] Trial 23 finished with value: 2.351559251974611 and parameters: {'n_layers': 3, 'n_neurons': 178, 'embudo_1': 1.1013898264114814, 'embudo_2': 2.484342699784369, 'embudo_3': 1.7273634162329072, 'dropout_threshold': 0.5504886965957101}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.17391304347826086, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20.91304347826087, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02608695652173913, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07391304347826087, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.1217391304347826, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.024347826086956525, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 111 neurons, 1.2189637753078313 embudo_1, 2.2312458903606784 embudo_2, and 2.035466866147086 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_322 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_323 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_324 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.0418 - val_loss: 0.1581\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0197 - val_loss: 0.1589\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0190 - val_loss: 0.1578\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0183 - val_loss: 0.1596\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0180 - val_loss: 0.1580\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0181 - val_loss: 0.1571\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0178 - val_loss: 0.1566\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0176 - val_loss: 0.1558\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0172 - val_loss: 0.1549\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0169 - val_loss: 0.1515\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.1506\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0159 - val_loss: 0.1450\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.1412\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.1405\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0157 - val_loss: 0.1376\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.1291\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0147 - val_loss: 0.1233\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0136 - val_loss: 0.1083\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - val_loss: 0.0914\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.0979\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0153 - val_loss: 0.1029\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0963\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0928\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.0865\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0995\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0932\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.0982\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0130 - val_loss: 0.0929\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0432 - val_loss: 0.1210\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.1184\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.1157\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.1139\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.1108\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0121 - val_loss: 0.1084\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:39:54,963] Trial 24 finished with value: 2.6336115598331373 and parameters: {'n_layers': 3, 'n_neurons': 111, 'embudo_1': 1.2189637753078313, 'embudo_2': 2.2312458903606784, 'embudo_3': 2.035466866147086, 'dropout_threshold': 0.6158966139924937}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.16666666666666666, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20.041666666666668, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.024999999999999998, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07083333333333333, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11666666666666665, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.023333333333333334, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 236 neurons, 1.0730107850637092 embudo_1, 1.9816107331014954 embudo_2, and 3.0602527146542156 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_325 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_326 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_327 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_328 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 3s 11ms/step - loss: 0.0418 - val_loss: 0.1611\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0187 - val_loss: 0.1588\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0182 - val_loss: 0.1578\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0178 - val_loss: 0.1560\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.1550\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0174 - val_loss: 0.1505\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0174 - val_loss: 0.1510\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0167 - val_loss: 0.1476\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.1462\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0159 - val_loss: 0.1408\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0156 - val_loss: 0.1386\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.1368\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.1368\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0149 - val_loss: 0.1330\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0159 - val_loss: 0.1303\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.1301\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.1305\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0148 - val_loss: 0.1299\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.1273\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.1274\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.1238\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.1202\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 1s 11ms/step - loss: 0.0135 - val_loss: 0.1160\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.1068\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0883\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.0975\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0871\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0129 - val_loss: 0.0947\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0118 - val_loss: 0.0781\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.0912\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0793\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0722\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0675\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0115 - val_loss: 0.0756\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0714\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0668\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0579\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0591\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0118 - val_loss: 0.0574\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0559\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0571\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0535\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0518\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0456\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0477\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0115 - val_loss: 0.0407\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0420\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0369\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0403\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0370\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0392\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0390\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0417\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0092 - val_loss: 0.0336\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0372\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0423\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0094 - val_loss: 0.0360\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0086 - val_loss: 0.0366\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0119 - val_loss: 0.0295\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0092 - val_loss: 0.0292\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.0409\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0309\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0293\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0288\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0275\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0247\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0280\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0090 - val_loss: 0.0285\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0095 - val_loss: 0.0322\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0319\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0271\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0327\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0282\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0270\n",
      "Epoch 75/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0282\n",
      "Epoch 76/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0266\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:40:24,304] Trial 25 finished with value: 2.303841227935003 and parameters: {'n_layers': 4, 'n_neurons': 236, 'embudo_1': 1.0730107850637092, 'embudo_2': 1.9816107331014954, 'embudo_3': 3.0602527146542156, 'dropout_threshold': 0.5433868649568594}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.16, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(19.24, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.024, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.068, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11199999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.022400000000000003, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 370 neurons, 1.0490967504559585 embudo_1, 1.9978257298125666 embudo_2, and 3.4722576494293267 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_329 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_330 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 6ms/step - loss: 0.0288 - val_loss: 0.1500\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0178 - val_loss: 0.1464\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0174 - val_loss: 0.1389\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0162 - val_loss: 0.1291\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0154 - val_loss: 0.1196\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0153 - val_loss: 0.1036\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0143 - val_loss: 0.0967\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0124 - val_loss: 0.0844\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0139 - val_loss: 0.1036\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0128 - val_loss: 0.0790\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0132 - val_loss: 0.0817\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0130 - val_loss: 0.0691\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0138 - val_loss: 0.0882\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.0811\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0141 - val_loss: 0.0993\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - val_loss: 0.1022\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0124 - val_loss: 0.0775\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0754\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0111 - val_loss: 0.0729\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.0748\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0113 - val_loss: 0.0630\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0110 - val_loss: 0.0730\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0133 - val_loss: 0.0763\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0114 - val_loss: 0.0727\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0109 - val_loss: 0.0693\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0119 - val_loss: 0.0823\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0737\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0121 - val_loss: 0.0733\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0098 - val_loss: 0.0646\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0103 - val_loss: 0.0439\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0110 - val_loss: 0.0528\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0135 - val_loss: 0.0802\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0656\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0655\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0127 - val_loss: 0.0608\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0115 - val_loss: 0.0731\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0646\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0114 - val_loss: 0.0702\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0121 - val_loss: 0.0655\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0108 - val_loss: 0.0594\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:40:35,875] Trial 26 finished with value: 2.3104902652362433 and parameters: {'n_layers': 2, 'n_neurons': 370, 'embudo_1': 1.0490967504559585, 'embudo_2': 1.9978257298125666, 'embudo_3': 3.4722576494293267, 'dropout_threshold': 0.5438751175853469}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15384615384615385, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(18.5, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.023076923076923075, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06538461538461539, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10769230769230768, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02153846153846154, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 224 neurons, 1.0728104120248285 embudo_1, 2.139588938751057 embudo_2, and 3.082836690094239 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_331 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_332 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_333 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.0376 - val_loss: 0.1588\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0192 - val_loss: 0.1561\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0181 - val_loss: 0.1553\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0177 - val_loss: 0.1540\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0179 - val_loss: 0.1521\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0173 - val_loss: 0.1500\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0173 - val_loss: 0.1490\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0170 - val_loss: 0.1455\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0167 - val_loss: 0.1422\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0159 - val_loss: 0.1384\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0157 - val_loss: 0.1308\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.1203\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.1106\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0159 - val_loss: 0.1101\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0155 - val_loss: 0.1094\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.1019\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0929\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0139 - val_loss: 0.0842\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0964\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0771\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0805\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0770\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0687\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0831\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0849\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0711\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0751\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0641\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0530\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0704\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - val_loss: 0.0729\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0124 - val_loss: 0.0759\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0139 - val_loss: 0.0753\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0666\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0590\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0627\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0105 - val_loss: 0.0562\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0647\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0547\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:40:49,983] Trial 27 finished with value: 2.363804209172856 and parameters: {'n_layers': 3, 'n_neurons': 224, 'embudo_1': 1.0728104120248285, 'embudo_2': 2.139588938751057, 'embudo_3': 3.082836690094239, 'dropout_threshold': 0.5936874026111487}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14814814814814814, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(17.814814814814813, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.022222222222222223, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06296296296296296, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.1037037037037037, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.020740740740740744, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 288 neurons, 0.9906280574578119 embudo_1, 2.361938776649498 embudo_2, and 2.7880979659417076 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_334 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_335 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_336 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_337 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 4s 11ms/step - loss: 0.0368 - val_loss: 0.1613\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0183 - val_loss: 0.1594\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0180 - val_loss: 0.1549\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0176 - val_loss: 0.1549\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0175 - val_loss: 0.1511\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0170 - val_loss: 0.1505\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0167 - val_loss: 0.1470\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0164 - val_loss: 0.1441\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.1404\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.1388\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.1345\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.1325\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0151 - val_loss: 0.1319\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0148 - val_loss: 0.1311\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0151 - val_loss: 0.1275\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.1185\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0142 - val_loss: 0.1147\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.1004\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0991\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0698\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0131 - val_loss: 0.0526\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0184 - val_loss: 0.1306\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.1153\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0985\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0608\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0426 - val_loss: 0.0907\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0827\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0866\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0848\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0726\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0743\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:41:05,366] Trial 28 finished with value: 2.305062188631917 and parameters: {'n_layers': 4, 'n_neurons': 288, 'embudo_1': 0.9906280574578119, 'embudo_2': 2.361938776649498, 'embudo_3': 2.7880979659417076, 'dropout_threshold': 0.5127940441170188}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14285714285714285, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(17.178571428571427, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02142857142857143, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.060714285714285714, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 272 neurons, 1.1297107697937976 embudo_1, 1.8979214009519 embudo_2, and 2.9351456590307032 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_338 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_339 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_340 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.0358 - val_loss: 0.1602\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0187 - val_loss: 0.1575\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0182 - val_loss: 0.1555\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0178 - val_loss: 0.1543\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.1549\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.1519\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0173 - val_loss: 0.1523\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0169 - val_loss: 0.1506\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.1483\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.1457\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.1403\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0157 - val_loss: 0.1387\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0152 - val_loss: 0.1357\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0156 - val_loss: 0.1399\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0154 - val_loss: 0.1327\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0153 - val_loss: 0.1302\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.1298\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.1289\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.1265\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0140 - val_loss: 0.1242\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.1183\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.1154\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.1153\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - val_loss: 0.1064\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.0982\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0909\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0804\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0739\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0713\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0598\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0128 - val_loss: 0.0651\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0611\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0548\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0587\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0550\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0458\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0544\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0114 - val_loss: 0.0479\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0570\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0518\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0379\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0334\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0350\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0333\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0286\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0316\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - val_loss: 0.0290\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0124 - val_loss: 0.0314\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - val_loss: 0.0419\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0429\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0406\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0101 - val_loss: 0.0407\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0365\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0103 - val_loss: 0.0287\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.0307\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:41:23,592] Trial 29 finished with value: 2.3030292186635264 and parameters: {'n_layers': 3, 'n_neurons': 272, 'embudo_1': 1.1297107697937976, 'embudo_2': 1.8979214009519, 'embudo_3': 2.9351456590307032, 'dropout_threshold': 0.608392454127823}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13793103448275862, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(16.586206896551722, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.020689655172413793, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05862068965517241, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09655172413793103, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.019310344827586208, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 325 neurons, 0.9672129370419074 embudo_1, 1.9157548161560858 embudo_2, and 3.085113410308802 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_341 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_342 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 7ms/step - loss: 0.0296 - val_loss: 0.1506\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0179 - val_loss: 0.1415\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0169 - val_loss: 0.1369\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0157 - val_loss: 0.1272\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0155 - val_loss: 0.1082\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0133 - val_loss: 0.0889\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - val_loss: 0.0783\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0119 - val_loss: 0.0550\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0157 - val_loss: 0.0927\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.0916\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0849\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0126 - val_loss: 0.0932\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0148 - val_loss: 0.0999\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0129 - val_loss: 0.0876\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0126 - val_loss: 0.0755\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0121 - val_loss: 0.0873\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0111 - val_loss: 0.0736\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0121 - val_loss: 0.0700\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:41:31,359] Trial 30 finished with value: 2.309406948861235 and parameters: {'n_layers': 2, 'n_neurons': 325, 'embudo_1': 0.9672129370419074, 'embudo_2': 1.9157548161560858, 'embudo_3': 3.085113410308802, 'dropout_threshold': 0.4859319726084145}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13793103448275862, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(16.586206896551722, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.020689655172413793, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05862068965517241, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09655172413793103, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.019310344827586208, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 278 neurons, 1.0706101813993616 embudo_1, 2.118679325945031 embudo_2, and 2.8543466768034897 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_343 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_344 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_345 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.0336 - val_loss: 0.1585\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0186 - val_loss: 0.1570\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0182 - val_loss: 0.1561\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0181 - val_loss: 0.1557\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0177 - val_loss: 0.1532\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.1511\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0172 - val_loss: 0.1508\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0171 - val_loss: 0.1487\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0165 - val_loss: 0.1462\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0159 - val_loss: 0.1416\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.1362\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.1336\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.1257\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.1141\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.1163\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.1013\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0978\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0920\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0988\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0818\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0849\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0755\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0781\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0125 - val_loss: 0.0635\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0742\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0680\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0610\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0608\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0114 - val_loss: 0.0403\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0509\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0644\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0126 - val_loss: 0.0556\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0789\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0117 - val_loss: 0.0724\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0725\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0623\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0544\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0523\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0525\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:41:45,591] Trial 31 finished with value: 2.3058757111370407 and parameters: {'n_layers': 3, 'n_neurons': 278, 'embudo_1': 1.0706101813993616, 'embudo_2': 2.118679325945031, 'embudo_3': 2.8543466768034897, 'dropout_threshold': 0.6138221335318583}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13333333333333333, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(16.033333333333335, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.056666666666666664, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333332, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.018666666666666668, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 184 neurons, 1.1195982524160537 embudo_1, 1.8749112543302797 embudo_2, and 2.930915593173613 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_346 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_347 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_348 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 3s 9ms/step - loss: 0.0406 - val_loss: 0.1585\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0192 - val_loss: 0.1579\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0184 - val_loss: 0.1566\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0178 - val_loss: 0.1556\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0179 - val_loss: 0.1562\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0178 - val_loss: 0.1548\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.1538\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.1523\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0171 - val_loss: 0.1528\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0170 - val_loss: 0.1498\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.1492\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.1480\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.1435\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0158 - val_loss: 0.1422\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0157 - val_loss: 0.1382\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.1364\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.1331\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.1312\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.1339\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.1304\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.1295\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.1283\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.1277\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.1260\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.1244\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.1215\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.1211\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.1194\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.1143\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.1112\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.1102\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.1060\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.1033\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.1041\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0951\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0927\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.0944\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0929\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0857\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0926\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0872\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0837\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0773\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0761\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0683\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0746\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0664\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0682\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0618\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0615\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0125 - val_loss: 0.0549\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0605\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0107 - val_loss: 0.0650\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0618\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0611\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0582\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0558\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0479\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0490\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0460\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0498\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0106 - val_loss: 0.0499\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - val_loss: 0.0454\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0445\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0413\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0424\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0395\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0103 - val_loss: 0.0390\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0389\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0375\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0353\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - val_loss: 0.0363\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0365\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0387\n",
      "Epoch 75/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0411\n",
      "Epoch 76/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0378\n",
      "Epoch 77/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0387\n",
      "Epoch 78/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0409\n",
      "Epoch 79/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0099 - val_loss: 0.0389\n",
      "Epoch 80/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0358\n",
      "Epoch 81/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0335\n",
      "Epoch 82/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0332\n",
      "Epoch 83/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0334\n",
      "Epoch 84/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0308\n",
      "Epoch 85/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0325\n",
      "Epoch 86/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0323\n",
      "Epoch 87/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0298\n",
      "Epoch 88/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0274\n",
      "Epoch 89/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0277\n",
      "Epoch 90/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0266\n",
      "Epoch 91/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0084 - val_loss: 0.0256\n",
      "Epoch 92/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0281\n",
      "Epoch 93/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0317\n",
      "Epoch 94/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.0290\n",
      "Epoch 95/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0083 - val_loss: 0.0302\n",
      "Epoch 96/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0243\n",
      "Epoch 97/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0241\n",
      "Epoch 98/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0234\n",
      "Epoch 99/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0239\n",
      "Epoch 100/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0090 - val_loss: 0.0210\n",
      "Epoch 101/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0225\n",
      "Epoch 102/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0231\n",
      "Epoch 103/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0240\n",
      "Epoch 104/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0094 - val_loss: 0.0243\n",
      "Epoch 105/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0225\n",
      "Epoch 106/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0220\n",
      "Epoch 107/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0267\n",
      "Epoch 108/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0230\n",
      "Epoch 109/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0198\n",
      "Epoch 110/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0091 - val_loss: 0.0196\n",
      "Epoch 111/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0088 - val_loss: 0.0194\n",
      "Epoch 112/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0091 - val_loss: 0.0195\n",
      "Epoch 113/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0089 - val_loss: 0.0165\n",
      "Epoch 114/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0183\n",
      "Epoch 115/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0185\n",
      "Epoch 116/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0092 - val_loss: 0.0180\n",
      "Epoch 117/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0153\n",
      "Epoch 118/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0092 - val_loss: 0.0154\n",
      "Epoch 119/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0090 - val_loss: 0.0167\n",
      "Epoch 120/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0159\n",
      "Epoch 121/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0090 - val_loss: 0.0159\n",
      "Epoch 122/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.0135\n",
      "Epoch 123/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0175\n",
      "Epoch 124/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0082 - val_loss: 0.0143\n",
      "Epoch 125/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0193\n",
      "Epoch 126/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - val_loss: 0.0230\n",
      "Epoch 127/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0090 - val_loss: 0.0190\n",
      "Epoch 128/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0097 - val_loss: 0.0163\n",
      "Epoch 129/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - val_loss: 0.0198\n",
      "Epoch 130/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - val_loss: 0.0179\n",
      "Epoch 131/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - val_loss: 0.0193\n",
      "Epoch 132/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0091 - val_loss: 0.0266\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:42:24,996] Trial 32 finished with value: 2.3134416888185294 and parameters: {'n_layers': 3, 'n_neurons': 184, 'embudo_1': 1.1195982524160537, 'embudo_2': 1.8749112543302797, 'embudo_3': 2.930915593173613, 'dropout_threshold': 0.5639115215355779}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12903225806451613, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(15.516129032258064, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01935483870967742, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.054838709677419356, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09032258064516129, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01806451612903226, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 235 neurons, 1.1763163298774235 embudo_1, 2.0512691407192643 embudo_2, and 2.6850996303690122 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_349 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_350 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_351 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.0368 - val_loss: 0.1576\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0190 - val_loss: 0.1570\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0183 - val_loss: 0.1577\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0181 - val_loss: 0.1557\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.1550\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0174 - val_loss: 0.1540\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0173 - val_loss: 0.1516\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0169 - val_loss: 0.1503\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0169 - val_loss: 0.1487\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0164 - val_loss: 0.1490\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0159 - val_loss: 0.1433\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.1394\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0157 - val_loss: 0.1373\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0158 - val_loss: 0.1339\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.1342\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0151 - val_loss: 0.1310\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.1296\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.1291\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.1283\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.1268\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0146 - val_loss: 0.1263\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.1231\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.1224\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.1206\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0144 - val_loss: 0.1175\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.1165\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.1149\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.1121\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.1038\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.1024\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.1073\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.0961\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0944\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0876\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.0892\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0818\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0811\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0673\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0658\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0118 - val_loss: 0.0690\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0723\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0703\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - val_loss: 0.0679\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0676\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0572\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0115 - val_loss: 0.0582\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0114 - val_loss: 0.0491\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0116 - val_loss: 0.0460\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0115 - val_loss: 0.0511\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0426\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0429\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0407\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0378\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0363\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0440\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0379\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0370\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0096 - val_loss: 0.0357\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0388\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0455\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0390\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0358\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0110 - val_loss: 0.0315\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0103 - val_loss: 0.0395\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.0421\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0443\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - val_loss: 0.0309\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0338\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0342\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - val_loss: 0.0305\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0315\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0372\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0370\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0111 - val_loss: 0.0400\n",
      "Epoch 75/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0353\n",
      "Epoch 76/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0286\n",
      "Epoch 77/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0291\n",
      "Epoch 78/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0099 - val_loss: 0.0268\n",
      "Epoch 79/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0253\n",
      "Epoch 80/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0247\n",
      "Epoch 81/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0269\n",
      "Epoch 82/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0274\n",
      "Epoch 83/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0096 - val_loss: 0.0269\n",
      "Epoch 84/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0258\n",
      "Epoch 85/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0092 - val_loss: 0.0264\n",
      "Epoch 86/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0252\n",
      "Epoch 87/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0276\n",
      "Epoch 88/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0091 - val_loss: 0.0292\n",
      "Epoch 89/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0103 - val_loss: 0.0286\n",
      "Epoch 90/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0117 - val_loss: 0.0315\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:42:52,046] Trial 33 finished with value: 2.3046863942142406 and parameters: {'n_layers': 3, 'n_neurons': 235, 'embudo_1': 1.1763163298774235, 'embudo_2': 2.0512691407192643, 'embudo_3': 2.6850996303690122, 'dropout_threshold': 0.6273349862937552}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.125, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(15.03125, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01875, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.053125, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0875, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0175, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 264 neurons, 1.148941697380857 embudo_1, 2.202787433665619 embudo_2, and 2.89173675038467 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_352 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_353 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_354 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 3s 9ms/step - loss: 0.0356 - val_loss: 0.1542\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0182 - val_loss: 0.1533\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0177 - val_loss: 0.1520\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0174 - val_loss: 0.1500\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0174 - val_loss: 0.1485\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0170 - val_loss: 0.1484\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0168 - val_loss: 0.1462\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.1422\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0157 - val_loss: 0.1408\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0156 - val_loss: 0.1329\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.1295\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.1221\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.1169\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.1178\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0995\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0816\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.0765\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0142 - val_loss: 0.0940\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0762\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0791\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0811\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0549\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.0877\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0729\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0710\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - val_loss: 0.0582\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0559\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0709\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0608\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0783\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0670\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - val_loss: 0.0536\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0472\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0526\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0459\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0575\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0494\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0548\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0452\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0424\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0437\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0508\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0405\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0406\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0099 - val_loss: 0.0361\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0397\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0411\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0112 - val_loss: 0.0367\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0340\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0292\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0335\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0106 - val_loss: 0.0356\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0329\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0109 - val_loss: 0.0359\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - val_loss: 0.0366\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0310\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0325\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0323\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0092 - val_loss: 0.0274\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0338\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0303\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0317\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0275\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0299\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.0266\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0268\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0236\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0232\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0244\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0252\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0096 - val_loss: 0.0237\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0255\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.0258\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0243\n",
      "Epoch 75/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0090 - val_loss: 0.0234\n",
      "Epoch 76/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0246\n",
      "Epoch 77/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0092 - val_loss: 0.0254\n",
      "Epoch 78/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0282\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:43:17,463] Trial 34 finished with value: 2.306376546816093 and parameters: {'n_layers': 3, 'n_neurons': 264, 'embudo_1': 1.148941697380857, 'embudo_2': 2.202787433665619, 'embudo_3': 2.89173675038467, 'dropout_threshold': 0.5279184349662255}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12121212121212122, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(14.575757575757576, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01818181818181818, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.051515151515151514, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08484848484848484, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01696969696969697, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 129 neurons, 1.0484929192664527 embudo_1, 2.342028582795797 embudo_2, and 2.5682292302902985 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_355 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_356 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_357 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_358 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 3s 11ms/step - loss: 0.0469 - val_loss: 0.1615\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0193 - val_loss: 0.1632\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0189 - val_loss: 0.1623\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0187 - val_loss: 0.1629\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0185 - val_loss: 0.1638\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0184 - val_loss: 0.1629\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0181 - val_loss: 0.1617\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0180 - val_loss: 0.1607\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.1597\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0171 - val_loss: 0.1559\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0169 - val_loss: 0.1518\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0168 - val_loss: 0.1491\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.1479\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0164 - val_loss: 0.1432\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.1411\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0156 - val_loss: 0.1391\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.1362\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0151 - val_loss: 0.1329\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0156 - val_loss: 0.1307\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.1300\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.1300\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.1298\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.1309\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.1298\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0151 - val_loss: 0.1261\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.1256\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0145 - val_loss: 0.1244\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.1235\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.1220\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.1210\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.1197\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.1194\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.1181\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.1159\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.1165\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.1148\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.1136\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.1109\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.1111\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.1105\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.1090\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.1084\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.1066\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.1083\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0136 - val_loss: 0.1197\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.1021\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.1010\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.0990\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0977\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0959\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0964\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0948\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0145 - val_loss: 0.0939\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0917\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0928\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0914\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0131 - val_loss: 0.0891\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.0916\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0877\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0868\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0855\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.0832\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0824\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.0807\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0810\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0803\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0810\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0786\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0782\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0115 - val_loss: 0.0759\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0749\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0728\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0761\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0734\n",
      "Epoch 75/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0728\n",
      "Epoch 76/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0740\n",
      "Epoch 77/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0694\n",
      "Epoch 78/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0805\n",
      "Epoch 79/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0708\n",
      "Epoch 80/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0670\n",
      "Epoch 81/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0680\n",
      "Epoch 82/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0120 - val_loss: 0.0653\n",
      "Epoch 83/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0660\n",
      "Epoch 84/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.0621\n",
      "Epoch 85/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0118 - val_loss: 0.0633\n",
      "Epoch 86/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0610\n",
      "Epoch 87/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0622\n",
      "Epoch 88/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0118 - val_loss: 0.0605\n",
      "Epoch 89/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0114 - val_loss: 0.0595\n",
      "Epoch 90/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0576\n",
      "Epoch 91/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0575\n",
      "Epoch 92/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0564\n",
      "Epoch 93/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0535\n",
      "Epoch 94/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0541\n",
      "Epoch 95/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0520\n",
      "Epoch 96/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0522\n",
      "Epoch 97/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0090 - val_loss: 0.0513\n",
      "Epoch 98/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0089 - val_loss: 0.0510\n",
      "Epoch 99/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0114 - val_loss: 0.0504\n",
      "Epoch 100/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0493\n",
      "Epoch 101/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0132 - val_loss: 0.0500\n",
      "Epoch 102/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0481\n",
      "Epoch 103/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0474\n",
      "Epoch 104/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0496\n",
      "Epoch 105/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0478\n",
      "Epoch 106/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0478\n",
      "Epoch 107/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0089 - val_loss: 0.0463\n",
      "Epoch 108/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0124 - val_loss: 0.0449\n",
      "Epoch 109/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0450\n",
      "Epoch 110/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0436\n",
      "Epoch 111/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - val_loss: 0.0445\n",
      "Epoch 112/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0094 - val_loss: 0.0435\n",
      "Epoch 113/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0450\n",
      "Epoch 114/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0450\n",
      "Epoch 115/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0091 - val_loss: 0.0439\n",
      "Epoch 116/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0439\n",
      "Epoch 117/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0415\n",
      "Epoch 118/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0094 - val_loss: 0.0418\n",
      "Epoch 119/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0114 - val_loss: 0.0412\n",
      "Epoch 120/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0417\n",
      "Epoch 121/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0406\n",
      "Epoch 122/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0414\n",
      "Epoch 123/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0397\n",
      "Epoch 124/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0404\n",
      "Epoch 125/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0389\n",
      "Epoch 126/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.0394\n",
      "Epoch 127/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0385\n",
      "Epoch 128/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0394\n",
      "Epoch 129/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0377\n",
      "Epoch 130/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0085 - val_loss: 0.0370\n",
      "Epoch 131/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0361\n",
      "Epoch 132/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0356\n",
      "Epoch 133/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0360\n",
      "Epoch 134/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0355\n",
      "Epoch 135/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0364\n",
      "Epoch 136/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0352\n",
      "Epoch 137/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0349\n",
      "Epoch 138/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0338\n",
      "Epoch 139/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0337\n",
      "Epoch 140/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0336\n",
      "Epoch 141/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0092 - val_loss: 0.0330\n",
      "Epoch 142/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0324\n",
      "Epoch 143/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0321\n",
      "Epoch 144/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0090 - val_loss: 0.0307\n",
      "Epoch 145/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0306\n",
      "Epoch 146/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0093 - val_loss: 0.0301\n",
      "Epoch 147/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0299\n",
      "Epoch 148/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.0297\n",
      "Epoch 149/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0092 - val_loss: 0.0293\n",
      "Epoch 150/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0303\n",
      "Epoch 151/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0300\n",
      "Epoch 152/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0305\n",
      "Epoch 153/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0316\n",
      "Epoch 154/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0301\n",
      "Epoch 155/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0301\n",
      "Epoch 156/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0290\n",
      "Epoch 157/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0092 - val_loss: 0.0286\n",
      "Epoch 158/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0088 - val_loss: 0.0294\n",
      "Epoch 159/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0095 - val_loss: 0.0277\n",
      "Epoch 160/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0090 - val_loss: 0.0288\n",
      "Epoch 161/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0287\n",
      "Epoch 162/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0293\n",
      "Epoch 163/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0283\n",
      "Epoch 164/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0091 - val_loss: 0.0281\n",
      "Epoch 165/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0091 - val_loss: 0.0278\n",
      "Epoch 166/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0274\n",
      "Epoch 167/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0270\n",
      "Epoch 168/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0282\n",
      "Epoch 169/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0278\n",
      "Epoch 170/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0088 - val_loss: 0.0275\n",
      "Epoch 171/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0295\n",
      "Epoch 172/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0276\n",
      "Epoch 173/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0277\n",
      "Epoch 174/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0272\n",
      "Epoch 175/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0086 - val_loss: 0.0274\n",
      "Epoch 176/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0277\n",
      "Epoch 177/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0267\n",
      "Epoch 178/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0268\n",
      "Epoch 179/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0276\n",
      "Epoch 180/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0095 - val_loss: 0.0264\n",
      "Epoch 181/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0095 - val_loss: 0.0251\n",
      "Epoch 182/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0251\n",
      "Epoch 183/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0241\n",
      "Epoch 184/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0403\n",
      "Epoch 185/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0291\n",
      "Epoch 186/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0090 - val_loss: 0.0249\n",
      "Epoch 187/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0244\n",
      "Epoch 188/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0247\n",
      "Epoch 189/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0094 - val_loss: 0.0254\n",
      "Epoch 190/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0085 - val_loss: 0.0250\n",
      "Epoch 191/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0244\n",
      "Epoch 192/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0227\n",
      "Epoch 193/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0243\n",
      "Epoch 194/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0236\n",
      "Epoch 195/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0243\n",
      "Epoch 196/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0093 - val_loss: 0.0234\n",
      "Epoch 197/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0090 - val_loss: 0.0238\n",
      "Epoch 198/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0251\n",
      "Epoch 199/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0092 - val_loss: 0.0245\n",
      "Epoch 200/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0239\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:44:23,430] Trial 35 finished with value: 2.304879510165142 and parameters: {'n_layers': 4, 'n_neurons': 129, 'embudo_1': 1.0484929192664527, 'embudo_2': 2.342028582795797, 'embudo_3': 2.5682292302902985, 'dropout_threshold': 0.5927221497324728}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11764705882352941, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(14.147058823529411, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01764705882352941, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.049999999999999996, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08235294117647059, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01647058823529412, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 312 neurons, 1.1099933178377805 embudo_1, 2.041254457480421 embudo_2, and 3.130523441629641 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_359 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_360 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 6ms/step - loss: 0.0320 - val_loss: 0.1533\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0189 - val_loss: 0.1506\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0182 - val_loss: 0.1496\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0174 - val_loss: 0.1451\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0166 - val_loss: 0.1363\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0153 - val_loss: 0.1284\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0156 - val_loss: 0.1231\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0148 - val_loss: 0.1186\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0159 - val_loss: 0.1074\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - val_loss: 0.1037\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0136 - val_loss: 0.1103\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0134 - val_loss: 0.0942\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0149 - val_loss: 0.1006\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0144 - val_loss: 0.0977\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0830\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - val_loss: 0.0887\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0148 - val_loss: 0.1090\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0128 - val_loss: 0.0973\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0122 - val_loss: 0.0856\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0817\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0143 - val_loss: 0.0891\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.0938\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0878\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0126 - val_loss: 0.0795\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0130 - val_loss: 0.0853\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0115 - val_loss: 0.0898\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0128 - val_loss: 0.0675\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0114 - val_loss: 0.0781\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0114 - val_loss: 0.0742\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0130 - val_loss: 0.0694\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0133 - val_loss: 0.0870\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0116 - val_loss: 0.0820\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - val_loss: 0.0606\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0122 - val_loss: 0.0750\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0109 - val_loss: 0.0809\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0125 - val_loss: 0.0712\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0116 - val_loss: 0.0663\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0121 - val_loss: 0.0688\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0658\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0113 - val_loss: 0.0791\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0106 - val_loss: 0.0630\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - val_loss: 0.0586\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0112 - val_loss: 0.0620\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - val_loss: 0.0685\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0118 - val_loss: 0.0795\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0113 - val_loss: 0.0691\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0129 - val_loss: 0.0617\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0610\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0136 - val_loss: 0.0786\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0109 - val_loss: 0.0660\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0109 - val_loss: 0.0609\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0113 - val_loss: 0.0766\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:44:37,856] Trial 36 finished with value: 2.6336115598331373 and parameters: {'n_layers': 2, 'n_neurons': 312, 'embudo_1': 1.1099933178377805, 'embudo_2': 2.041254457480421, 'embudo_3': 3.130523441629641, 'dropout_threshold': 0.6578965355439613}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11428571428571428, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(13.742857142857142, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.017142857142857144, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04857142857142857, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.016, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 269 neurons, 1.1549426636926263 embudo_1, 1.8969934499736187 embudo_2, and 2.796433016910029 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_361 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_362 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_363 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_364 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 3s 11ms/step - loss: 0.0413 - val_loss: 0.1609\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0187 - val_loss: 0.1613\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0186 - val_loss: 0.1613\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0181 - val_loss: 0.1590\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0180 - val_loss: 0.1583\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0180 - val_loss: 0.1576\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0175 - val_loss: 0.1549\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0171 - val_loss: 0.1532\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0168 - val_loss: 0.1476\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0161 - val_loss: 0.1452\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.1415\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0158 - val_loss: 0.1391\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0156 - val_loss: 0.1360\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0161 - val_loss: 0.1340\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.1349\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.1313\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.1319\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.1302\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0148 - val_loss: 0.1291\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0142 - val_loss: 0.1269\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.1233\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0162 - val_loss: 0.1150\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.1034\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.1027\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0918\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.0809\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.0792\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0676\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0799\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0555\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0571\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0529\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0508\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0119 - val_loss: 0.0420\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0425\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0363\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.0417\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0478\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0118 - val_loss: 0.0497\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0468\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0425\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0436\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0346\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0530\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.0606\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0612\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0132 - val_loss: 0.0512\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0560\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0497\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0493\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0478\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0432\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0434\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:44:59,814] Trial 37 finished with value: 2.323041458467073 and parameters: {'n_layers': 4, 'n_neurons': 269, 'embudo_1': 1.1549426636926263, 'embudo_2': 1.8969934499736187, 'embudo_3': 2.796433016910029, 'dropout_threshold': 0.6240465963148276}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.1111111111111111, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(13.36111111111111, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.016666666666666666, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04722222222222222, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07777777777777778, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015555555555555557, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 192 neurons, 0.96395562958945 embudo_1, 2.164479820587857 embudo_2, and 2.9655647627014083 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_365 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_366 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_367 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 9ms/step - loss: 0.0385 - val_loss: 0.1573\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0195 - val_loss: 0.1551\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0183 - val_loss: 0.1548\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0178 - val_loss: 0.1540\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0176 - val_loss: 0.1528\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0174 - val_loss: 0.1505\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.1512\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0170 - val_loss: 0.1483\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.1468\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.1418\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0165 - val_loss: 0.1398\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0157 - val_loss: 0.1337\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.1285\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.1185\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.1083\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0146 - val_loss: 0.1042\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.1081\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.1067\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0227 - val_loss: 0.1243\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.1192\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.1055\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.1058\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0998\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0130 - val_loss: 0.0852\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0882\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0778\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0730\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0810\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0797\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0738\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0667\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0798\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0710\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0109 - val_loss: 0.0566\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.0771\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0121 - val_loss: 0.0761\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0609\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0626\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0705\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0090 - val_loss: 0.0738\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0701\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0660\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0711\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0691\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:45:15,636] Trial 38 finished with value: 2.341353237055178 and parameters: {'n_layers': 3, 'n_neurons': 192, 'embudo_1': 0.96395562958945, 'embudo_2': 2.164479820587857, 'embudo_3': 2.9655647627014083, 'dropout_threshold': 0.556443393327107}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10810810810810811, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(13.0, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.016216216216216217, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04594594594594594, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07567567567567567, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015135135135135137, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 383 neurons, 1.0934937715560331 embudo_1, 2.2766533032409955 embudo_2, and 2.698110647548696 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_368 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_369 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 6ms/step - loss: 0.0309 - val_loss: 0.1532\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0183 - val_loss: 0.1486\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0171 - val_loss: 0.1413\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0170 - val_loss: 0.1338\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0154 - val_loss: 0.1219\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0151 - val_loss: 0.1254\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0154 - val_loss: 0.1186\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0149 - val_loss: 0.1100\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0129 - val_loss: 0.0913\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0144 - val_loss: 0.0896\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0131 - val_loss: 0.0945\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0130 - val_loss: 0.0766\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.1013\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0117 - val_loss: 0.0669\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0131 - val_loss: 0.0819\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0161 - val_loss: 0.1137\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0126 - val_loss: 0.1059\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0129 - val_loss: 0.0978\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0118 - val_loss: 0.0807\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0142 - val_loss: 0.1002\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0116 - val_loss: 0.0846\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0857\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0130 - val_loss: 0.1050\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0857\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:45:23,982] Trial 39 finished with value: 2.491150566760697 and parameters: {'n_layers': 2, 'n_neurons': 383, 'embudo_1': 1.0934937715560331, 'embudo_2': 2.2766533032409955, 'embudo_3': 2.698110647548696, 'dropout_threshold': 0.6337929518965424}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10526315789473684, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.657894736842104, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015789473684210527, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04473684210526316, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07368421052631578, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01473684210526316, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 155 neurons, 1.1465739482445851 embudo_1, 2.3799827325110994 embudo_2, and 3.3201177379378053 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_370 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_371 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_372 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 9ms/step - loss: 0.0412 - val_loss: 0.1607\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0193 - val_loss: 0.1580\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0184 - val_loss: 0.1571\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0180 - val_loss: 0.1570\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0178 - val_loss: 0.1568\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0178 - val_loss: 0.1568\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0177 - val_loss: 0.1563\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0174 - val_loss: 0.1536\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0173 - val_loss: 0.1528\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0171 - val_loss: 0.1529\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0170 - val_loss: 0.1498\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.1485\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.1457\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - val_loss: 0.1436\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0156 - val_loss: 0.1404\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0154 - val_loss: 0.1378\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.1356\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.1316\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0148 - val_loss: 0.1310\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.1318\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.1299\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.1294\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.1298\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.1274\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.1265\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.1257\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0143 - val_loss: 0.1242\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.1223\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.1203\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.1180\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0138 - val_loss: 0.1154\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.1166\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.1130\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.1103\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.1117\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0140 - val_loss: 0.1058\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0128 - val_loss: 0.1031\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0143 - val_loss: 0.1024\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.1020\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.1052\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0984\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0982\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0933\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0929\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0114 - val_loss: 0.0873\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0835\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0811\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0796\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0114 - val_loss: 0.0724\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0718\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0126 - val_loss: 0.0719\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0761\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0682\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0666\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0633\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0096 - val_loss: 0.0584\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0513\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0519\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0549\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0487\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0098 - val_loss: 0.0543\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0529\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0442\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0084 - val_loss: 0.0475\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0483\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0456\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - val_loss: 0.0435\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0567\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0488\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0486\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0117 - val_loss: 0.0496\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0474\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0452\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0480\n",
      "Epoch 75/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0478\n",
      "Epoch 76/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0105 - val_loss: 0.0444\n",
      "Epoch 77/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0619\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:45:47,521] Trial 40 finished with value: 2.359410613215011 and parameters: {'n_layers': 3, 'n_neurons': 155, 'embudo_1': 1.1465739482445851, 'embudo_2': 2.3799827325110994, 'embudo_3': 3.3201177379378053, 'dropout_threshold': 0.5199328227161847}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10526315789473684, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.657894736842104, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015789473684210527, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04473684210526316, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07368421052631578, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01473684210526316, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 244 neurons, 1.179266046566357 embudo_1, 2.0626472577550836 embudo_2, and 1.0151129756170778 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_373 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_374 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_375 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_376 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 4s 10ms/step - loss: 0.0415 - val_loss: 0.1607\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0187 - val_loss: 0.1566\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0181 - val_loss: 0.1565\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0180 - val_loss: 0.1544\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0173 - val_loss: 0.1537\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0174 - val_loss: 0.1534\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0171 - val_loss: 0.1498\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0165 - val_loss: 0.1484\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0160 - val_loss: 0.1437\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.1411\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.1381\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.1359\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0151 - val_loss: 0.1340\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0148 - val_loss: 0.1312\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0156 - val_loss: 0.1306\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.1295\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0145 - val_loss: 0.1294\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.1285\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.1269\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.1271\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0158 - val_loss: 0.1242\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.1247\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.1208\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.1199\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0148 - val_loss: 0.1169\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0136 - val_loss: 0.1135\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.1101\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.1104\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.1004\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0131 - val_loss: 0.0915\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.0931\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0136 - val_loss: 0.0863\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.1078\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.0824\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0826\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0727\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0162 - val_loss: 0.0748\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0119 - val_loss: 0.0748\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.0782\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0784\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0712\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0647\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0590\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0547\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0095 - val_loss: 0.0571\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0492\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0455\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0117 - val_loss: 0.0435\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0118 - val_loss: 0.0384\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0364\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0550\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0809\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0373\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0420\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0387\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0345\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0368\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0334\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0329\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0342\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0092 - val_loss: 0.0301\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0315\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0301\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0273\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0337\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0362\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0317\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0274\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0312\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0286\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0433\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0368\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0304\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0298\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:46:16,458] Trial 41 finished with value: 2.304571311953925 and parameters: {'n_layers': 4, 'n_neurons': 244, 'embudo_1': 1.179266046566357, 'embudo_2': 2.0626472577550836, 'embudo_3': 1.0151129756170778, 'dropout_threshold': 0.6322866379184198}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10256410256410256, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.333333333333334, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015384615384615384, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04358974358974359, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07179487179487179, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01435897435897436, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 213 neurons, 1.1953810308996524 embudo_1, 1.9566603115393217 embudo_2, and 2.381960462520429 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_377 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_378 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_379 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_380 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 3s 10ms/step - loss: 0.0433 - val_loss: 0.1622\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0187 - val_loss: 0.1603\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0184 - val_loss: 0.1585\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0183 - val_loss: 0.1597\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0182 - val_loss: 0.1589\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0177 - val_loss: 0.1577\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0175 - val_loss: 0.1557\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0170 - val_loss: 0.1533\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0167 - val_loss: 0.1500\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0162 - val_loss: 0.1465\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0161 - val_loss: 0.1430\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0159 - val_loss: 0.1410\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.1390\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.1372\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.1332\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.1319\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.1310\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.1316\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.1319\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.1317\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.1296\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.1289\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0142 - val_loss: 0.1287\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.1283\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.1265\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.1253\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.1250\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.1237\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.1217\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0132 - val_loss: 0.1203\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.1203\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.1169\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0131 - val_loss: 0.1144\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.1122\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.1105\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.1084\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.1073\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.1055\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.1049\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.1056\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.1003\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0959\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0946\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0910\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0137 - val_loss: 0.0880\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0830\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0828\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.0805\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0771\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0119 - val_loss: 0.0785\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0791\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0131 - val_loss: 0.0765\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0727\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0727\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0688\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0634\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0643\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0118 - val_loss: 0.0656\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0596\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0570\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0118 - val_loss: 0.0550\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0568\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0532\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.0523\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0495\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0483\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0513\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0445\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0426\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0425\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0406\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0401\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0384\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0377\n",
      "Epoch 75/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0357\n",
      "Epoch 76/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0365\n",
      "Epoch 77/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0417\n",
      "Epoch 78/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0375\n",
      "Epoch 79/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0108 - val_loss: 0.0342\n",
      "Epoch 80/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0343\n",
      "Epoch 81/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0347\n",
      "Epoch 82/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0341\n",
      "Epoch 83/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0324\n",
      "Epoch 84/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0327\n",
      "Epoch 85/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0320\n",
      "Epoch 86/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0341\n",
      "Epoch 87/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0304\n",
      "Epoch 88/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0315\n",
      "Epoch 89/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0325\n",
      "Epoch 90/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0305\n",
      "Epoch 91/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0095 - val_loss: 0.0298\n",
      "Epoch 92/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0089 - val_loss: 0.0318\n",
      "Epoch 93/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0313\n",
      "Epoch 94/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0303\n",
      "Epoch 95/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0290\n",
      "Epoch 96/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0272\n",
      "Epoch 97/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0095 - val_loss: 0.0287\n",
      "Epoch 98/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0272\n",
      "Epoch 99/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0323\n",
      "Epoch 100/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0091 - val_loss: 0.0351\n",
      "Epoch 101/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0325\n",
      "Epoch 102/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0344\n",
      "Epoch 103/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0119 - val_loss: 0.0305\n",
      "Epoch 104/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0308\n",
      "Epoch 105/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0324\n",
      "Epoch 106/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0320\n",
      "Epoch 107/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0277\n",
      "Epoch 108/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0265\n",
      "Epoch 109/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0092 - val_loss: 0.0284\n",
      "Epoch 110/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0079 - val_loss: 0.0257\n",
      "Epoch 111/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0083 - val_loss: 0.0233\n",
      "Epoch 112/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0261\n",
      "Epoch 113/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0269\n",
      "Epoch 114/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0295\n",
      "Epoch 115/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0088 - val_loss: 0.0313\n",
      "Epoch 116/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0269\n",
      "Epoch 117/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0270\n",
      "Epoch 118/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0272\n",
      "Epoch 119/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0287\n",
      "Epoch 120/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0275\n",
      "Epoch 121/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0119 - val_loss: 0.0263\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:46:59,868] Trial 42 finished with value: 2.3046146140162995 and parameters: {'n_layers': 4, 'n_neurons': 213, 'embudo_1': 1.1953810308996524, 'embudo_2': 1.9566603115393217, 'embudo_3': 2.381960462520429, 'dropout_threshold': 0.6002837065432067}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.1, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.025, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.042499999999999996, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06999999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.014000000000000002, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 292 neurons, 1.1178036963782516 embudo_1, 1.8205886013562398 embudo_2, and 2.5228129475926058 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_381 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_382 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_383 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_384 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 3s 10ms/step - loss: 0.0394 - val_loss: 0.1604\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0187 - val_loss: 0.1604\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0186 - val_loss: 0.1586\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0181 - val_loss: 0.1576\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0178 - val_loss: 0.1584\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0179 - val_loss: 0.1554\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0175 - val_loss: 0.1553\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0171 - val_loss: 0.1504\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0164 - val_loss: 0.1465\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0162 - val_loss: 0.1442\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0161 - val_loss: 0.1402\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0156 - val_loss: 0.1380\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.1354\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.1345\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0156 - val_loss: 0.1323\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0160 - val_loss: 0.1317\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.1296\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.1295\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.1277\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0151 - val_loss: 0.1273\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.1282\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0145 - val_loss: 0.1242\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.1220\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.1241\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0136 - val_loss: 0.1190\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.1251\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.1120\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.1136\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0158 - val_loss: 0.1070\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.1070\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0132 - val_loss: 0.1012\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0140 - val_loss: 0.0996\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0974\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0897\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0898\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0820\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0804\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0736\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0699\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0665\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0609\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0526\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0460\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0797\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0496\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0427\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0426\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0421\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0408\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0388\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0455\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0462\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0468\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0450\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0375\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0354\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0406\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0386\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0364\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0347\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0091 - val_loss: 0.0318\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0341\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0354\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0310\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0311\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0306\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0343\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0337\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0309\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.0310\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0079 - val_loss: 0.0288\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0366\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0330\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0114 - val_loss: 0.0323\n",
      "Epoch 75/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0309\n",
      "Epoch 76/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0306\n",
      "Epoch 77/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0294\n",
      "Epoch 78/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0279\n",
      "Epoch 79/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0095 - val_loss: 0.0299\n",
      "Epoch 80/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0286\n",
      "Epoch 81/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0088 - val_loss: 0.0252\n",
      "Epoch 82/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0238\n",
      "Epoch 83/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0229\n",
      "Epoch 84/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0218\n",
      "Epoch 85/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0267\n",
      "Epoch 86/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0237\n",
      "Epoch 87/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0268\n",
      "Epoch 88/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0255\n",
      "Epoch 89/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0224\n",
      "Epoch 90/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0248\n",
      "Epoch 91/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0093 - val_loss: 0.0239\n",
      "Epoch 92/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0212\n",
      "Epoch 93/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0215\n",
      "Epoch 94/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0207\n",
      "Epoch 95/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0214\n",
      "Epoch 96/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0223\n",
      "Epoch 97/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0236\n",
      "Epoch 98/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0232\n",
      "Epoch 99/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0206\n",
      "Epoch 100/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - val_loss: 0.0236\n",
      "Epoch 101/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - val_loss: 0.0227\n",
      "Epoch 102/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0225\n",
      "Epoch 103/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0191\n",
      "Epoch 104/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0093 - val_loss: 0.0206\n",
      "Epoch 105/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0204\n",
      "Epoch 106/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0222\n",
      "Epoch 107/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0095 - val_loss: 0.0174\n",
      "Epoch 108/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0085 - val_loss: 0.0151\n",
      "Epoch 109/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0147\n",
      "Epoch 110/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0190\n",
      "Epoch 111/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0189\n",
      "Epoch 112/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0094 - val_loss: 0.0196\n",
      "Epoch 113/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0196\n",
      "Epoch 114/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0092 - val_loss: 0.0183\n",
      "Epoch 115/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0149\n",
      "Epoch 116/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0174\n",
      "Epoch 117/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0090 - val_loss: 0.0179\n",
      "Epoch 118/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0177\n",
      "Epoch 119/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0094 - val_loss: 0.0155\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:47:42,181] Trial 43 finished with value: 2.3067904470486593 and parameters: {'n_layers': 4, 'n_neurons': 292, 'embudo_1': 1.1178036963782516, 'embudo_2': 1.8205886013562398, 'embudo_3': 2.5228129475926058, 'dropout_threshold': 0.6379731428522184}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0975609756097561, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(11.731707317073171, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.014634146341463414, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.041463414634146344, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06829268292682926, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013658536585365855, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 251 neurons, 1.1385060321898495 embudo_1, 1.6964837373902977 embudo_2, and 3.1820474149975717 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_385 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_386 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_387 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_388 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 4s 11ms/step - loss: 0.0405 - val_loss: 0.1635\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0189 - val_loss: 0.1615\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0185 - val_loss: 0.1600\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0180 - val_loss: 0.1584\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0180 - val_loss: 0.1561\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0178 - val_loss: 0.1559\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0176 - val_loss: 0.1544\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0172 - val_loss: 0.1534\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0168 - val_loss: 0.1518\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0162 - val_loss: 0.1471\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.1414\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0158 - val_loss: 0.1380\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0159 - val_loss: 0.1352\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.1343\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.1325\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.1309\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.1308\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0158 - val_loss: 0.1302\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0145 - val_loss: 0.1289\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.1276\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.1242\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.1222\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.1169\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.1147\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.1161\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.1120\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.1128\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.1054\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.1006\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0971\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0948\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0886\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.0986\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0853\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0864\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0783\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0720\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0638\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0753\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0643\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0122 - val_loss: 0.0698\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0568\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0598\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0510\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0487\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0461\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0488\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.0439\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0461\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0417\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0388\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0374\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0381\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0439\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0477\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0423\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0468\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0471\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0352\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0441\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0381\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0398\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0343\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0314\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0330\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0388\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0343\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0382\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0393\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0372\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0388\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0095 - val_loss: 0.0325\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0352\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0399\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:48:10,866] Trial 44 finished with value: 2.3185112979270586 and parameters: {'n_layers': 4, 'n_neurons': 251, 'embudo_1': 1.1385060321898495, 'embudo_2': 1.6964837373902977, 'embudo_3': 3.1820474149975717, 'dropout_threshold': 0.5870409284812731}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09523809523809523, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(11.452380952380953, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.014285714285714285, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04047619047619048, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06666666666666667, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013333333333333334, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 306 neurons, 1.2374342365932236 embudo_1, 2.190515572309505 embudo_2, and 0.8579993876457186 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_389 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 1s 4ms/step - loss: 0.0222 - val_loss: 0.1097\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0925\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0832\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.0737\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 0.0707\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0782\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0559\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0126 - val_loss: 0.0739\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0694\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0602\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.0654\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.0636\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 0.0730\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0655\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0110 - val_loss: 0.0605\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.0710\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0108 - val_loss: 0.0654\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:48:15,982] Trial 45 finished with value: 2.3408457256368607 and parameters: {'n_layers': 1, 'n_neurons': 306, 'embudo_1': 1.2374342365932236, 'embudo_2': 2.190515572309505, 'embudo_3': 0.8579993876457186, 'dropout_threshold': 0.5646783979351593}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09302325581395349, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(11.186046511627907, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013953488372093023, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03953488372093023, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06511627906976744, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013023255813953489, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 268 neurons, 1.0692052951594517 embudo_1, 2.076145840987394 embudo_2, and 1.1739587790460595 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_390 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_391 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_392 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.0351 - val_loss: 0.1539\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0179 - val_loss: 0.1509\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.1490\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0172 - val_loss: 0.1468\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.1439\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.1430\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.1396\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0157 - val_loss: 0.1366\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0153 - val_loss: 0.1346\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.1367\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.1328\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.1298\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.1277\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.1228\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.1169\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0996\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0148 - val_loss: 0.1026\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0834\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0919\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0813\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0835\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0795\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0741\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0667\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0787\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0129 - val_loss: 0.0740\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0633\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0665\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0681\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0115 - val_loss: 0.0685\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0590\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0634\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0513\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0467\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0114 - val_loss: 0.0467\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0517\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0580\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0490\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0380\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0406\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0389\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0377\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0104 - val_loss: 0.0404\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0375\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0114 - val_loss: 0.0437\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0395\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0453\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0115 - val_loss: 0.0379\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0364\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0343\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0319\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0358\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0334\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0105 - val_loss: 0.0325\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0339\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0365\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0336\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.0347\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0327\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0299\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0429\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0294\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0273\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0303\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0221\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0261\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0302\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0251\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0244\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0206\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0208\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0194\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0249\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0115 - val_loss: 0.0253\n",
      "Epoch 75/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0306\n",
      "Epoch 76/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0297\n",
      "Epoch 77/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0099 - val_loss: 0.0290\n",
      "Epoch 78/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0124 - val_loss: 0.0202\n",
      "Epoch 79/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0233\n",
      "Epoch 80/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0218\n",
      "Epoch 81/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0105 - val_loss: 0.0200\n",
      "Epoch 82/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0174\n",
      "Epoch 83/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0227\n",
      "Epoch 84/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0090 - val_loss: 0.0205\n",
      "Epoch 85/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0216\n",
      "Epoch 86/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - val_loss: 0.0166\n",
      "Epoch 87/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0089 - val_loss: 0.0181\n",
      "Epoch 88/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0101 - val_loss: 0.0126\n",
      "Epoch 89/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - val_loss: 0.0224\n",
      "Epoch 90/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0159\n",
      "Epoch 91/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0092 - val_loss: 0.0198\n",
      "Epoch 92/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - val_loss: 0.0183\n",
      "Epoch 93/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0194\n",
      "Epoch 94/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0082 - val_loss: 0.0209\n",
      "Epoch 95/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0228\n",
      "Epoch 96/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0096 - val_loss: 0.0186\n",
      "Epoch 97/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0194\n",
      "Epoch 98/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - val_loss: 0.0188\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:48:45,425] Trial 46 finished with value: 2.323293099212627 and parameters: {'n_layers': 3, 'n_neurons': 268, 'embudo_1': 1.0692052951594517, 'embudo_2': 2.076145840987394, 'embudo_3': 1.1739587790460595, 'dropout_threshold': 0.6051531175372952}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09090909090909091, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(10.931818181818182, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013636363636363636, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.038636363636363635, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06363636363636363, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.012727272727272728, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 227 neurons, 1.2005162263530942 embudo_1, 1.9586808604462123 embudo_2, and 2.7029805842518506 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_393 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_394 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_395 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_396 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 4s 11ms/step - loss: 0.0404 - val_loss: 0.1630\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0191 - val_loss: 0.1614\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0184 - val_loss: 0.1606\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0183 - val_loss: 0.1597\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0182 - val_loss: 0.1601\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0179 - val_loss: 0.1592\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0177 - val_loss: 0.1562\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0174 - val_loss: 0.1553\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0169 - val_loss: 0.1515\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.1473\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.1436\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0159 - val_loss: 0.1464\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0161 - val_loss: 0.1403\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.1401\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.1324\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.1310\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.1308\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.1290\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0151 - val_loss: 0.1293\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.1258\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0151 - val_loss: 0.1239\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.1230\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.1205\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.1200\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.1177\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.1154\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0154 - val_loss: 0.1133\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.1111\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0144 - val_loss: 0.1114\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.1109\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.1075\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0132 - val_loss: 0.1068\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.1020\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.1017\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.1026\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0131 - val_loss: 0.1004\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0118 - val_loss: 0.0991\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0928\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0893\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0886\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.0864\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.0833\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0136 - val_loss: 0.0797\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0107 - val_loss: 0.0716\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0744\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0727\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.0703\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0718\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0904\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0119 - val_loss: 0.0674\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0617\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.0610\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0118 - val_loss: 0.0524\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0511\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.0485\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0577\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0729\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0650\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0507\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0617\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0671\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0695\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0699\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0549\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0519\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:49:11,550] Trial 47 finished with value: 2.6336115598331373 and parameters: {'n_layers': 4, 'n_neurons': 227, 'embudo_1': 1.2005162263530942, 'embudo_2': 1.9586808604462123, 'embudo_3': 2.7029805842518506, 'dropout_threshold': 0.6432801924900535}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08888888888888889, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(10.688888888888888, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013333333333333332, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03777777777777778, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06222222222222222, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.012444444444444445, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 202 neurons, 1.0268607993163599 embudo_1, 2.110324482002835 embudo_2, and 2.9148749560851117 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_397 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_398 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_399 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.0374 - val_loss: 0.1613\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0193 - val_loss: 0.1583\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0186 - val_loss: 0.1587\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0181 - val_loss: 0.1583\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0181 - val_loss: 0.1575\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0179 - val_loss: 0.1563\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.1545\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0173 - val_loss: 0.1512\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0169 - val_loss: 0.1500\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0173 - val_loss: 0.1488\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0165 - val_loss: 0.1425\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0156 - val_loss: 0.1420\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0155 - val_loss: 0.1374\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.1353\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0157 - val_loss: 0.1286\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0148 - val_loss: 0.1291\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.1228\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.1225\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0155 - val_loss: 0.1189\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.1173\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.1147\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0139 - val_loss: 0.1153\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.1108\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.1082\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.1012\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.1048\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.1000\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0136 - val_loss: 0.0955\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.1116\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.1029\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0924\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0911\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0713\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0089 - val_loss: 0.0596\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0755\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0183 - val_loss: 0.0827\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.0935\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0909\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0873\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0831\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0791\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0748\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0128 - val_loss: 0.0776\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0743\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:49:26,823] Trial 48 finished with value: 2.6336115598331373 and parameters: {'n_layers': 3, 'n_neurons': 202, 'embudo_1': 1.0268607993163599, 'embudo_2': 2.110324482002835, 'embudo_3': 2.9148749560851117, 'dropout_threshold': 0.6595498815647964}. Best is trial 18 with value: 2.3026451237466206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n",
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08695652173913043, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(10.456521739130435, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013043478260869565, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03695652173913044, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0608695652173913, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.012173913043478262, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 34 neurons, 1.1254786272566903 embudo_1, 2.0108832038734876 embudo_2, and 2.2281967794869946 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_400 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_401 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_402 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_403 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 4s 10ms/step - loss: 0.0666 - val_loss: 0.1576\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0227 - val_loss: 0.1647\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0204 - val_loss: 0.1688\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0196 - val_loss: 0.1691\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0191 - val_loss: 0.1708\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0190 - val_loss: 0.1714\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0189 - val_loss: 0.1697\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0189 - val_loss: 0.1692\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0187 - val_loss: 0.1707\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0187 - val_loss: 0.1703\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0186 - val_loss: 0.1707\n",
      "Predicting 201904 for product -1.50563459089948 (1/73))\n",
      "Predicting 201904 for product -1.4949719400222794 (2/73))\n",
      "Predicting 201904 for product -0.6446255325655545 (3/73))\n",
      "Predicting 201904 for product -0.5060110711619504 (4/73))\n",
      "Predicting 201904 for product -0.4606948049338491 (5/73))\n",
      "Predicting 201904 for product -0.4127128759864477 (6/73))\n",
      "Predicting 201904 for product -0.39138757423204706 (7/73))\n",
      "Predicting 201904 for product -0.3380743198460455 (8/73))\n",
      "Predicting 201904 for product -0.3274116689688452 (9/73))\n",
      "Predicting 201904 for product -0.29275805361794416 (10/73))\n",
      "Predicting 201904 for product -0.2740984145828436 (11/73))\n",
      "Predicting 201904 for product -0.25810443826704316 (12/73))\n",
      "Predicting 201904 for product -0.24477612467054277 (13/73))\n",
      "Predicting 201904 for product -0.2421104619512427 (14/73))\n",
      "Predicting 201904 for product -0.2154538347582419 (15/73))\n",
      "Predicting 201904 for product -0.18613154484594105 (16/73))\n",
      "Predicting 201904 for product -0.1034960005476386 (17/73))\n",
      "Predicting 201904 for product -0.06084539703883735 (18/73))\n",
      "Predicting 201904 for product -0.03152310712653649 (19/73))\n",
      "Predicting 201904 for product 0.07776906436476672 (20/73))\n",
      "Predicting 201904 for product 0.08576605252266696 (21/73))\n",
      "Predicting 201904 for product 0.1044256915577675 (22/73))\n",
      "Predicting 201904 for product 0.13374798147006836 (23/73))\n",
      "Predicting 201904 for product 0.15773894594376908 (24/73))\n",
      "Predicting 201904 for product 0.1657359341016693 (25/73))\n",
      "Predicting 201904 for product 0.3016847327859733 (26/73))\n",
      "Predicting 201904 for product 0.3549979871719749 (27/73))\n",
      "Predicting 201904 for product 0.39498292796147605 (28/73))\n",
      "Predicting 201904 for product 0.4456305196281775 (29/73))\n",
      "Predicting 201904 for product 0.49361244857557895 (30/73))\n",
      "Predicting 201904 for product 0.49627811129487903 (31/73))\n",
      "Predicting 201904 for product 0.5415943775229803 (32/73))\n",
      "Predicting 201904 for product 0.6722118507686842 (33/73))\n",
      "Predicting 201904 for product 0.6882058270844846 (34/73))\n",
      "Predicting 201904 for product 0.7121967915581854 (35/73))\n",
      "Predicting 201904 for product 0.7148624542774854 (36/73))\n",
      "Predicting 201904 for product 0.8454799275231892 (37/73))\n",
      "Predicting 201904 for product 0.8774678801547903 (38/73))\n",
      "Predicting 201904 for product 0.8827992055933904 (39/73))\n",
      "Predicting 201904 for product 0.9067901700670911 (40/73))\n",
      "Predicting 201904 for product 0.9254498091021917 (41/73))\n",
      "Predicting 201904 for product 0.936112459979392 (42/73))\n",
      "Predicting 201904 for product 0.9627690871723927 (43/73))\n",
      "Predicting 201904 for product 0.9681004126109929 (44/73))\n",
      "Predicting 201904 for product 0.9867600516460935 (45/73))\n",
      "Predicting 201904 for product 0.9920913770846936 (46/73))\n",
      "Predicting 201904 for product 1.0160823415583944 (47/73))\n",
      "Predicting 201904 for product 1.0320763178741947 (48/73))\n",
      "Predicting 201904 for product 1.0480702941899953 (49/73))\n",
      "Predicting 201904 for product 1.0827239095408963 (50/73))\n",
      "Predicting 201904 for product 1.1573624656812984 (51/73))\n",
      "Predicting 201904 for product 1.181353430154999 (52/73))\n",
      "Predicting 201904 for product 1.2053443946287 (53/73))\n",
      "Predicting 201904 for product 1.2133413827866002 (54/73))\n",
      "Predicting 201904 for product 1.311970903400703 (55/73))\n",
      "Predicting 201904 for product 1.3626184950674045 (56/73))\n",
      "Predicting 201904 for product 1.4345913884885066 (57/73))\n",
      "Predicting 201904 for product 1.442588376646407 (58/73))\n",
      "Predicting 201904 for product 1.4692450038394076 (59/73))\n",
      "Predicting 201904 for product 1.482573317435908 (60/73))\n",
      "Predicting 201904 for product 1.4905703055938082 (61/73))\n",
      "Predicting 201904 for product 1.5358865718219097 (62/73))\n",
      "Predicting 201904 for product 1.5438835599798097 (63/73))\n",
      "Predicting 201904 for product 1.55454621085701 (64/73))\n",
      "Predicting 201904 for product 1.583868500769311 (65/73))\n",
      "Predicting 201904 for product 1.586534163488611 (66/73))\n",
      "Predicting 201904 for product 1.5891998262079112 (67/73))\n",
      "Predicting 201904 for product 1.5945311516465113 (68/73))\n",
      "Predicting 201904 for product 1.6158564534009119 (69/73))\n",
      "Predicting 201904 for product 1.6238534415588122 (70/73))\n",
      "Predicting 201904 for product 1.6291847669974122 (71/73))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:49:35,753] Trial 49 finished with value: 2.6336115598331373 and parameters: {'n_layers': 4, 'n_neurons': 34, 'embudo_1': 1.1254786272566903, 'embudo_2': 2.0108832038734876, 'embudo_3': 2.2281967794869946, 'dropout_threshold': 0.6053835502481812}. Best is trial 18 with value: 2.3026451237466206.\n",
      "[I 2023-11-25 22:49:35,754] A new study created in memory with name: lstm_10_BO_1.5555569940213847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6318504297167125 (72/73))\n",
      "Predicting 201904 for product 1.6345160924360125 (73/73))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "Best trial for cluster2.343365459926705:\n",
      "n_layers: 3\n",
      "n_neurons: 235\n",
      "embudo_1: 1.20013325060173\n",
      "embudo_2: 2.261759038493119\n",
      "embudo_3: 2.5513053414936797\n",
      "dropout_threshold: 0.6597607010916747\n",
      "Finished training for cluster 2.343365459926705\n",
      "Training for cluster 1.5555569940213847\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 308 neurons, 1.3013974395521934 embudo_1, 2.210097618703375 embudo_2, and 3.0174157779070008 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_404 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_405 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_406 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_407 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 3s 8ms/step - loss: 0.0885 - val_loss: 0.0021\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0914 - val_loss: 0.0021\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0337 - val_loss: 0.0020\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0344 - val_loss: 0.0016\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0314 - val_loss: 0.0013\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0318 - val_loss: 0.0013\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0227 - val_loss: 0.0021\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0267 - val_loss: 0.0024\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0260 - val_loss: 0.0022\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0264 - val_loss: 0.0017\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0236 - val_loss: 0.0019\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0243 - val_loss: 7.7889e-04\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0218 - val_loss: 0.0015\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0215 - val_loss: 0.0014\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0244 - val_loss: 0.0011\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0226 - val_loss: 0.0019\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0233 - val_loss: 0.0029\n",
      "Epoch 18/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0210 - val_loss: 0.0038\n",
      "Epoch 19/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0207 - val_loss: 0.0037\n",
      "Epoch 20/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0190 - val_loss: 9.6286e-04\n",
      "Epoch 21/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0216 - val_loss: 0.0028\n",
      "Epoch 22/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0212 - val_loss: 0.0018\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:49:57,626] Trial 0 finished with value: 8.997037993765232 and parameters: {'n_layers': 4, 'n_neurons': 308, 'embudo_1': 1.3013974395521934, 'embudo_2': 2.210097618703375, 'embudo_3': 3.0174157779070008, 'dropout_threshold': 0.20751666592540416}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 260 neurons, 1.0979123168128435 embudo_1, 1.087566077993646 embudo_2, and 2.2895936842536067 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_408 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_409 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_410 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_411 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 3s 8ms/step - loss: 0.0924 - val_loss: 0.0059\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0753 - val_loss: 8.7633e-04\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0686 - val_loss: 0.0022\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0541 - val_loss: 0.0022\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0860 - val_loss: 0.0013\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0461 - val_loss: 8.5703e-04\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0302 - val_loss: 8.2406e-04\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0335 - val_loss: 0.0016\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0356 - val_loss: 0.0020\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0324 - val_loss: 0.0011\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0270 - val_loss: 0.0012\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0298 - val_loss: 0.0015\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0349 - val_loss: 0.0023\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0310 - val_loss: 0.0018\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0321 - val_loss: 0.0022\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0023\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0270 - val_loss: 0.0010\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:50:16,005] Trial 1 finished with value: 8.997037993765232 and parameters: {'n_layers': 4, 'n_neurons': 260, 'embudo_1': 1.0979123168128435, 'embudo_2': 1.087566077993646, 'embudo_3': 2.2895936842536067, 'dropout_threshold': 0.569421433869813}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 236 neurons, 1.455112327021689 embudo_1, 0.8594194516487068 embudo_2, and 0.8299623041166441 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_412 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 1s 3ms/step - loss: 0.0382 - val_loss: 0.0027\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0067\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0036\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0029\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0050\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0063\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0056\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0035\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0118\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0060\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0134\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:50:24,292] Trial 2 finished with value: 8.997037993765232 and parameters: {'n_layers': 1, 'n_neurons': 236, 'embudo_1': 1.455112327021689, 'embudo_2': 0.8594194516487068, 'embudo_3': 0.8299623041166441, 'dropout_threshold': 0.19382122250522604}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 270 neurons, 1.1312531815747695 embudo_1, 1.2057757254293984 embudo_2, and 2.418640905286896 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_413 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_414 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 2s 5ms/step - loss: 0.0772 - val_loss: 0.0053\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0355 - val_loss: 0.0037\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0413 - val_loss: 0.0023\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0018\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0398 - val_loss: 0.0026\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0016\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0016\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0021\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0240 - val_loss: 0.0025\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 0.0034\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0015\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0030\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.0031\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0027\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0026\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0030\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0268 - val_loss: 0.0028\n",
      "Epoch 18/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 0.0018\n",
      "Epoch 19/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0246 - val_loss: 0.0023\n",
      "Epoch 20/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0249 - val_loss: 0.0022\n",
      "Epoch 21/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0228 - val_loss: 0.0024\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:50:38,633] Trial 3 finished with value: 8.997037993765232 and parameters: {'n_layers': 2, 'n_neurons': 270, 'embudo_1': 1.1312531815747695, 'embudo_2': 1.2057757254293984, 'embudo_3': 2.418640905286896, 'dropout_threshold': 0.6035059659949602}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 260 neurons, 1.0971342664962274 embudo_1, 1.9287674765224387 embudo_2, and 3.4158659916272 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_415 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_416 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 2s 5ms/step - loss: 0.0724 - val_loss: 0.0018\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0308 - val_loss: 0.0019\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 0.0028\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0021\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0021\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0017\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0248 - val_loss: 0.0020\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0269 - val_loss: 0.0019\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0025\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0246 - val_loss: 0.0025\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0025\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0015\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.0021\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 0.0018\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0211 - val_loss: 0.0023\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0229 - val_loss: 0.0030\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0021\n",
      "Epoch 18/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0223 - val_loss: 0.0026\n",
      "Epoch 19/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0201 - val_loss: 0.0033\n",
      "Epoch 20/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0221 - val_loss: 0.0024\n",
      "Epoch 21/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0232 - val_loss: 0.0025\n",
      "Epoch 22/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0205 - val_loss: 0.0031\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:50:53,088] Trial 4 finished with value: 8.997037993765232 and parameters: {'n_layers': 2, 'n_neurons': 260, 'embudo_1': 1.0971342664962274, 'embudo_2': 1.9287674765224387, 'embudo_3': 3.4158659916272, 'dropout_threshold': 0.37909097991328433}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 247 neurons, 1.4646259484006272 embudo_1, 1.570207963121668 embudo_2, and 1.635014085217686 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_417 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_418 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_419 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_420 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 4s 8ms/step - loss: 0.0878 - val_loss: 0.0016\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0700 - val_loss: 0.0012\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0393 - val_loss: 0.0035\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0299 - val_loss: 0.0032\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0350 - val_loss: 0.0042\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0287 - val_loss: 0.0020\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0280 - val_loss: 0.0016\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0231 - val_loss: 0.0019\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0238 - val_loss: 0.0028\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0241 - val_loss: 0.0022\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0294 - val_loss: 0.0041\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0254 - val_loss: 0.0053\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:51:08,522] Trial 5 finished with value: 8.997037993765232 and parameters: {'n_layers': 4, 'n_neurons': 247, 'embudo_1': 1.4646259484006272, 'embudo_2': 1.570207963121668, 'embudo_3': 1.635014085217686, 'dropout_threshold': 0.11612154835646421}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 132 neurons, 1.1892170834324225 embudo_1, 0.912244554920141 embudo_2, and 2.8727628539067895 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_421 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_422 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_423 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 3s 6ms/step - loss: 0.0897 - val_loss: 0.0057\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0411 - val_loss: 0.0015\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0265 - val_loss: 0.0057\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0330 - val_loss: 0.0018\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0021\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0241 - val_loss: 0.0037\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0262 - val_loss: 0.0023\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0234 - val_loss: 0.0016\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0236 - val_loss: 0.0013\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0245 - val_loss: 0.0022\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0220 - val_loss: 0.0032\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0272 - val_loss: 9.9267e-04\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0237 - val_loss: 0.0052\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0205 - val_loss: 0.0075\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0188 - val_loss: 0.0019\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0229 - val_loss: 0.0021\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0232 - val_loss: 0.0016\n",
      "Epoch 18/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0208 - val_loss: 0.0020\n",
      "Epoch 19/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0237 - val_loss: 0.0025\n",
      "Epoch 20/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0230 - val_loss: 0.0027\n",
      "Epoch 21/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0243 - val_loss: 0.0028\n",
      "Epoch 22/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0184 - val_loss: 0.0025\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:51:26,552] Trial 6 finished with value: 8.997037993765232 and parameters: {'n_layers': 3, 'n_neurons': 132, 'embudo_1': 1.1892170834324225, 'embudo_2': 0.912244554920141, 'embudo_3': 2.8727628539067895, 'dropout_threshold': 0.15865054674155027}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 47 neurons, 1.0012071655935546 embudo_1, 1.3998012265141715 embudo_2, and 2.122993586307618 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_424 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_425 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 2s 5ms/step - loss: 0.0910 - val_loss: 0.0082\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0433 - val_loss: 0.0045\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0322 - val_loss: 0.0022\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0312 - val_loss: 0.0019\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0031\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0312 - val_loss: 0.0033\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0035\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0021\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0235 - val_loss: 0.0028\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0019\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0019\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0028\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0021\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 0.0026\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:51:37,752] Trial 7 finished with value: 8.997037993765232 and parameters: {'n_layers': 2, 'n_neurons': 47, 'embudo_1': 1.0012071655935546, 'embudo_2': 1.3998012265141715, 'embudo_3': 2.122993586307618, 'dropout_threshold': 0.21331524699568705}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 185 neurons, 1.0748636438661614 embudo_1, 2.307099528615397 embudo_2, and 2.320169727246459 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_426 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 1s 3ms/step - loss: 0.0483 - val_loss: 0.0016\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0041\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0042\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0038\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0028\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0024\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0047\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0032\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0045\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0022\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0052\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:51:45,531] Trial 8 finished with value: 8.997037993765232 and parameters: {'n_layers': 1, 'n_neurons': 185, 'embudo_1': 1.0748636438661614, 'embudo_2': 2.307099528615397, 'embudo_3': 2.320169727246459, 'dropout_threshold': 0.46720256545138716}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 399 neurons, 1.0715209013091758 embudo_1, 1.6659546279511628 embudo_2, and 2.644185689850689 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_427 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_428 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_429 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 3s 7ms/step - loss: 0.0872 - val_loss: 0.0022\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0557 - val_loss: 0.0014\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0301 - val_loss: 0.0022\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0371 - val_loss: 0.0015\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0366 - val_loss: 0.0024\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0020\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0303 - val_loss: 0.0011\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0292 - val_loss: 0.0032\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0294 - val_loss: 0.0018\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0275 - val_loss: 0.0025\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0291 - val_loss: 0.0016\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0254 - val_loss: 0.0024\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0030\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0258 - val_loss: 0.0037\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0241 - val_loss: 0.0021\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0234 - val_loss: 0.0045\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0249 - val_loss: 0.0032\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:52:01,690] Trial 9 finished with value: 8.997037993765232 and parameters: {'n_layers': 3, 'n_neurons': 399, 'embudo_1': 1.0715209013091758, 'embudo_2': 1.6659546279511628, 'embudo_3': 2.644185689850689, 'dropout_threshold': 0.4478283008022014}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.3333333333333333, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.36363636363636365, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(160.33333333333334, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(43.72727272727273, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.19999999999999998, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05454545454545454, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5666666666666667, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15454545454545454, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.9333333333333332, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2545454545454545, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.18666666666666668, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05090909090909091, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 483 neurons, 1.3265733213552489 embudo_1, 2.374931570179166 embudo_2, and 3.447399102122538 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_430 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_431 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_432 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_433 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 4s 8ms/step - loss: 0.0858 - val_loss: 0.0014\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0787 - val_loss: 0.0022\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0311 - val_loss: 0.0017\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0301 - val_loss: 0.0024\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0035\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0429 - val_loss: 0.0010\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0236 - val_loss: 0.0024\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0253 - val_loss: 0.0018\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0262 - val_loss: 0.0022\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0239 - val_loss: 0.0039\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0203 - val_loss: 0.0025\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0253 - val_loss: 0.0031\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0225 - val_loss: 0.0025\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0246 - val_loss: 0.0039\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0240 - val_loss: 0.0019\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0220 - val_loss: 0.0041\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:52:20,377] Trial 10 finished with value: 8.997037993765232 and parameters: {'n_layers': 4, 'n_neurons': 483, 'embudo_1': 1.3265733213552489, 'embudo_2': 2.374931570179166, 'embudo_3': 3.447399102122538, 'dropout_threshold': 0.26734760414369363}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.36363636363636365, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(43.72727272727273, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05454545454545454, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15454545454545454, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2545454545454545, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05090909090909091, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 345 neurons, 1.2826581443809841 embudo_1, 2.1018115453018797 embudo_2, and 2.8525683286522194 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_434 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_435 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_436 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_437 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 3s 8ms/step - loss: 0.0918 - val_loss: 0.0038\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0756 - val_loss: 0.0012\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0715 - val_loss: 0.0014\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0660 - val_loss: 0.0017\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0571 - val_loss: 0.0016\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0484 - val_loss: 0.0013\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0381 - val_loss: 0.0021\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0320 - val_loss: 0.0013\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0313 - val_loss: 0.0028\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0305 - val_loss: 0.0016\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0396 - val_loss: 0.0024\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0013\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:52:35,617] Trial 11 finished with value: 8.997037993765232 and parameters: {'n_layers': 4, 'n_neurons': 345, 'embudo_1': 1.2826581443809841, 'embudo_2': 2.1018115453018797, 'embudo_3': 2.8525683286522194, 'dropout_threshold': 0.6054619858633874}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.3333333333333333, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(40.083333333333336, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.049999999999999996, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14166666666666666, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2333333333333333, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04666666666666667, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 359 neurons, 0.937083792552485 embudo_1, 2.488853487536232 embudo_2, and 1.6430729591246485 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_438 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_439 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_440 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 3s 6ms/step - loss: 0.0846 - val_loss: 0.0031\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0479 - val_loss: 0.0048\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0274 - val_loss: 0.0040\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0383 - val_loss: 0.0027\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0272 - val_loss: 0.0018\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0030\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0250 - val_loss: 0.0022\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0250 - val_loss: 0.0024\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0262 - val_loss: 0.0049\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0249 - val_loss: 0.0033\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0234 - val_loss: 0.0016\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0324 - val_loss: 0.0014\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0214 - val_loss: 0.0019\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0210 - val_loss: 0.0038\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0213 - val_loss: 0.0034\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0227 - val_loss: 0.0025\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0209 - val_loss: 0.0010\n",
      "Epoch 18/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0219 - val_loss: 0.0016\n",
      "Epoch 19/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0224 - val_loss: 0.0028\n",
      "Epoch 20/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0229 - val_loss: 0.0051\n",
      "Epoch 21/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0206 - val_loss: 0.0030\n",
      "Epoch 22/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0217 - val_loss: 0.0010\n",
      "Epoch 23/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0222 - val_loss: 0.0023\n",
      "Epoch 24/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0195 - val_loss: 0.0027\n",
      "Epoch 25/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0207 - val_loss: 0.0012\n",
      "Epoch 26/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0195 - val_loss: 9.7601e-04\n",
      "Epoch 27/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0196 - val_loss: 0.0029\n",
      "Epoch 28/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0196 - val_loss: 0.0017\n",
      "Epoch 29/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0203 - val_loss: 0.0018\n",
      "Epoch 30/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0208 - val_loss: 0.0026\n",
      "Epoch 31/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0191 - val_loss: 0.0039\n",
      "Epoch 32/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0205 - val_loss: 0.0020\n",
      "Epoch 33/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0192 - val_loss: 0.0014\n",
      "Epoch 34/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0174 - val_loss: 0.0052\n",
      "Epoch 35/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0182 - val_loss: 0.0027\n",
      "Epoch 36/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0184 - val_loss: 0.0034\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:53:02,428] Trial 12 finished with value: 8.997037993765232 and parameters: {'n_layers': 3, 'n_neurons': 359, 'embudo_1': 0.937083792552485, 'embudo_2': 2.488853487536232, 'embudo_3': 1.6430729591246485, 'dropout_threshold': 0.29970185381736664}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.3076923076923077, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(37.0, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04615384615384615, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13076923076923078, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.21538461538461537, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04307692307692308, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 319 neurons, 1.2318863130753055 embudo_1, 1.974659642595263 embudo_2, and 2.9472556430445356 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_441 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_442 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_443 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_444 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 3s 8ms/step - loss: 0.0934 - val_loss: 0.0040\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0770 - val_loss: 0.0012\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0723 - val_loss: 0.0013\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0686 - val_loss: 0.0014\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0636 - val_loss: 0.0016\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0563 - val_loss: 0.0011\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0478 - val_loss: 0.0030\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0368 - val_loss: 0.0033\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0365 - val_loss: 0.0021\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0299 - val_loss: 9.5360e-04\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0309 - val_loss: 0.0015\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0260 - val_loss: 0.0028\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0258 - val_loss: 0.0022\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0253 - val_loss: 8.0801e-04\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0251 - val_loss: 0.0019\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0252 - val_loss: 0.0019\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0274 - val_loss: 0.0010\n",
      "Epoch 18/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0236 - val_loss: 0.0011\n",
      "Epoch 19/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0236 - val_loss: 0.0011\n",
      "Epoch 20/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0211 - val_loss: 0.0014\n",
      "Epoch 21/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0246 - val_loss: 9.4854e-04\n",
      "Epoch 22/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0227 - val_loss: 0.0011\n",
      "Epoch 23/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0237 - val_loss: 0.0013\n",
      "Epoch 24/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0244 - val_loss: 9.2674e-04\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:53:25,356] Trial 13 finished with value: 8.997037993765232 and parameters: {'n_layers': 4, 'n_neurons': 319, 'embudo_1': 1.2318863130753055, 'embudo_2': 1.974659642595263, 'embudo_3': 2.9472556430445356, 'dropout_threshold': 0.6501796374936624}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2857142857142857, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(34.357142857142854, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04285714285714286, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12142857142857143, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.19999999999999998, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 444 neurons, 1.3365602012189983 embudo_1, 1.16114152351643 embudo_2, and 3.1119521753475783 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_445 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_446 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_447 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_448 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 4s 8ms/step - loss: 0.0902 - val_loss: 0.0032\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0742 - val_loss: 0.0019\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0684 - val_loss: 0.0011\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0534 - val_loss: 0.0015\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0840 - val_loss: 0.0035\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0380 - val_loss: 0.0031\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0378 - val_loss: 0.0016\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0313 - val_loss: 0.0039\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0343 - val_loss: 0.0018\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0308 - val_loss: 0.0016\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0280 - val_loss: 0.0013\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0011\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0273 - val_loss: 0.0027\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0270 - val_loss: 0.0020\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0228 - val_loss: 0.0030\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0251 - val_loss: 0.0012\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0212 - val_loss: 0.0010\n",
      "Epoch 18/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0230 - val_loss: 0.0021\n",
      "Epoch 19/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0233 - val_loss: 0.0015\n",
      "Epoch 20/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0026\n",
      "Epoch 21/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0201 - val_loss: 0.0020\n",
      "Epoch 22/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0222 - val_loss: 0.0020\n",
      "Epoch 23/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0207 - val_loss: 0.0023\n",
      "Epoch 24/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0212 - val_loss: 0.0027\n",
      "Epoch 25/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0212 - val_loss: 0.0017\n",
      "Epoch 26/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0218 - val_loss: 0.0050\n",
      "Epoch 27/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0201 - val_loss: 0.0024\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:53:50,958] Trial 14 finished with value: 8.997037993765232 and parameters: {'n_layers': 4, 'n_neurons': 444, 'embudo_1': 1.3365602012189983, 'embudo_2': 1.16114152351643, 'embudo_3': 3.1119521753475783, 'dropout_threshold': 0.5222619515680151}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.26666666666666666, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(32.06666666666667, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11333333333333333, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.18666666666666665, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.037333333333333336, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 144 neurons, 1.1872861718455727 embudo_1, 1.7341501990826265 embudo_2, and 2.465223579590905 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_449 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_450 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_451 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 3s 6ms/step - loss: 0.0918 - val_loss: 0.0044\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0747 - val_loss: 0.0029\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0469 - val_loss: 8.2648e-04\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0396 - val_loss: 0.0014\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0325 - val_loss: 0.0021\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0332 - val_loss: 0.0017\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0324 - val_loss: 0.0013\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0302 - val_loss: 0.0016\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0272 - val_loss: 0.0013\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0268 - val_loss: 0.0019\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0261 - val_loss: 0.0036\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0260 - val_loss: 0.0020\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0230 - val_loss: 0.0014\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:54:04,597] Trial 15 finished with value: 8.997037993765232 and parameters: {'n_layers': 3, 'n_neurons': 144, 'embudo_1': 1.1872861718455727, 'embudo_2': 1.7341501990826265, 'embudo_3': 2.465223579590905, 'dropout_threshold': 0.3436569729349231}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.25, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(30.0625, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0375, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10625, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.175, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.035, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 301 neurons, 0.9208170288257793 embudo_1, 2.1883189177309847 embudo_2, and 1.9208732149346746 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_452 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_453 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_454 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_455 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 3s 8ms/step - loss: 0.0879 - val_loss: 0.0044\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0731 - val_loss: 7.9856e-04\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0369 - val_loss: 0.0012\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0548 - val_loss: 0.0017\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0394 - val_loss: 0.0037\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0282 - val_loss: 5.7719e-04\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0254 - val_loss: 5.9699e-04\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0305 - val_loss: 0.0021\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0035\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0290 - val_loss: 0.0013\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0246 - val_loss: 0.0017\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0248 - val_loss: 8.0680e-04\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0259 - val_loss: 0.0014\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0232 - val_loss: 0.0025\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0222 - val_loss: 0.0033\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0271 - val_loss: 0.0023\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:54:22,600] Trial 16 finished with value: 8.997037993765232 and parameters: {'n_layers': 4, 'n_neurons': 301, 'embudo_1': 0.9208170288257793, 'embudo_2': 2.1883189177309847, 'embudo_3': 1.9208732149346746, 'dropout_threshold': 0.2943024302463849}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.23529411764705882, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(28.294117647058822, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03529411764705882, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.16470588235294117, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03294117647058824, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 395 neurons, 1.0154199766415024 embudo_1, 1.9328594324483084 embudo_2, and 3.1703972100537445 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_456 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_457 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_458 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 3s 6ms/step - loss: 0.0865 - val_loss: 0.0031\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0711 - val_loss: 0.0021\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0441 - val_loss: 9.0804e-04\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0347 - val_loss: 0.0019\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0341 - val_loss: 0.0026\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0284 - val_loss: 8.9611e-04\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0361 - val_loss: 0.0016\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0285 - val_loss: 0.0017\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0281 - val_loss: 0.0029\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0262 - val_loss: 0.0017\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0245 - val_loss: 0.0017\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0245 - val_loss: 0.0012\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0229 - val_loss: 0.0013\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0244 - val_loss: 0.0029\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0227 - val_loss: 0.0017\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0229 - val_loss: 0.0039\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:54:38,410] Trial 17 finished with value: 8.997037993765232 and parameters: {'n_layers': 3, 'n_neurons': 395, 'embudo_1': 1.0154199766415024, 'embudo_2': 1.9328594324483084, 'embudo_3': 3.1703972100537445, 'dropout_threshold': 0.46149173675488153}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2222222222222222, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(26.72222222222222, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03333333333333333, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09444444444444444, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15555555555555556, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.031111111111111114, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 62 neurons, 1.3814882834696554 embudo_1, 1.4928357819739564 embudo_2, and 2.663534021389552 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_459 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_460 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_461 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_462 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 3s 8ms/step - loss: 0.1014 - val_loss: 0.0040\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0576 - val_loss: 0.0025\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0360 - val_loss: 0.0045\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0328 - val_loss: 0.0019\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0331 - val_loss: 0.0020\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0315 - val_loss: 0.0021\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0333 - val_loss: 0.0014\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0301 - val_loss: 0.0016\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0294 - val_loss: 0.0016\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0238 - val_loss: 0.0028\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0244 - val_loss: 0.0027\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0242 - val_loss: 0.0021\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0290 - val_loss: 0.0055\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0195 - val_loss: 0.0012\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0199 - val_loss: 0.0143\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0234 - val_loss: 0.0032\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0197 - val_loss: 0.0030\n",
      "Epoch 18/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0198 - val_loss: 0.0018\n",
      "Epoch 19/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0220 - val_loss: 0.0041\n",
      "Epoch 20/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0241 - val_loss: 0.0024\n",
      "Epoch 21/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0190 - val_loss: 0.0022\n",
      "Epoch 22/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0205 - val_loss: 0.0044\n",
      "Epoch 23/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0243 - val_loss: 0.0031\n",
      "Epoch 24/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0202 - val_loss: 0.0044\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:55:00,201] Trial 18 finished with value: 8.997037993765232 and parameters: {'n_layers': 4, 'n_neurons': 62, 'embudo_1': 1.3814882834696554, 'embudo_2': 1.4928357819739564, 'embudo_3': 2.663534021389552, 'dropout_threshold': 0.11573532960327701}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.21052631578947367, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(25.31578947368421, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.031578947368421054, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08947368421052632, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14736842105263157, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02947368421052632, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 195 neurons, 1.2512834925924639 embudo_1, 1.7705850663408595 embudo_2, and 3.1724094167538146 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_463 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_464 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_465 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 3s 6ms/step - loss: 0.0890 - val_loss: 0.0043\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0471 - val_loss: 0.0021\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0292 - val_loss: 0.0035\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0331 - val_loss: 0.0024\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0280 - val_loss: 0.0014\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0318 - val_loss: 0.0019\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0224 - val_loss: 0.0022\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0297 - val_loss: 0.0014\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0270 - val_loss: 0.0027\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0238 - val_loss: 0.0027\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0270 - val_loss: 0.0031\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0248 - val_loss: 0.0033\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0222 - val_loss: 0.0025\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0227 - val_loss: 0.0034\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0232 - val_loss: 0.0024\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:55:15,083] Trial 19 finished with value: 8.997037993765232 and parameters: {'n_layers': 3, 'n_neurons': 195, 'embudo_1': 1.2512834925924639, 'embudo_2': 1.7705850663408595, 'embudo_3': 3.1724094167538146, 'dropout_threshold': 0.24016587064140055}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1.0, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(120.25, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(24.05, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.425, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08499999999999999, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.7, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13999999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.028000000000000004, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 198 neurons, 1.1697674244057699 embudo_1, 1.2744506337799584 embudo_2, and 2.621293367501928 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_466 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_467 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_468 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_469 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 3s 8ms/step - loss: 0.0932 - val_loss: 0.0054\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0761 - val_loss: 0.0025\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0797 - val_loss: 0.0017\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0420 - val_loss: 0.0010\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0334 - val_loss: 0.0015\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0413 - val_loss: 0.0013\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0017\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0252 - val_loss: 7.6801e-04\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0261 - val_loss: 0.0011\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0297 - val_loss: 0.0019\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0302 - val_loss: 0.0020\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0258 - val_loss: 0.0012\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0256 - val_loss: 0.0017\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0244 - val_loss: 0.0027\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0016\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0229 - val_loss: 0.0011\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0227 - val_loss: 0.0013\n",
      "Epoch 18/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0241 - val_loss: 0.0014\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:55:34,496] Trial 20 finished with value: 8.997037993765232 and parameters: {'n_layers': 4, 'n_neurons': 198, 'embudo_1': 1.1697674244057699, 'embudo_2': 1.2744506337799584, 'embudo_3': 2.621293367501928, 'dropout_threshold': 0.354059502113769}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(24.05, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08499999999999999, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13999999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.028000000000000004, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 230 neurons, 1.4673325659859817 embudo_1, 0.8313525636527868 embudo_2, and 0.7054451921288183 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_470 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 1s 3ms/step - loss: 0.0448 - val_loss: 0.0031\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0027\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0059\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0090\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0022\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0050\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0040\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0078\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0025\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0037\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.0041\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0064\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0059\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0031\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.0040\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:55:43,433] Trial 21 finished with value: 8.997037993765232 and parameters: {'n_layers': 1, 'n_neurons': 230, 'embudo_1': 1.4673325659859817, 'embudo_2': 0.8313525636527868, 'embudo_3': 0.7054451921288183, 'dropout_threshold': 0.1904712949221989}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.19047619047619047, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(22.904761904761905, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02857142857142857, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08095238095238096, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13333333333333333, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02666666666666667, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 299 neurons, 1.4126391391287922 embudo_1, 0.9847282241129636 embudo_2, and 0.8904923158657889 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_471 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 1s 3ms/step - loss: 0.0369 - val_loss: 0.0046\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0028\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0033\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0042\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0025\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.0070\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0057\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0042\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0078\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0042\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0027\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0033\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0062\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0034\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0026\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:55:52,368] Trial 22 finished with value: 8.997037993765232 and parameters: {'n_layers': 1, 'n_neurons': 299, 'embudo_1': 1.4126391391287922, 'embudo_2': 0.9847282241129636, 'embudo_3': 0.8904923158657889, 'dropout_threshold': 0.18401819126132274}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.18181818181818182, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(21.863636363636363, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02727272727272727, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07727272727272727, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12727272727272726, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.025454545454545455, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 144 neurons, 1.4164760703993415 embudo_1, 1.1083166948922871 embudo_2, and 1.2231658464380213 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_472 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_473 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 2s 8ms/step - loss: 0.0674 - val_loss: 0.0020\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0276 - val_loss: 0.0021\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0026\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0028\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0256 - val_loss: 0.0016\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0256 - val_loss: 0.0023\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0026\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0021\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0210 - val_loss: 0.0016\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0235 - val_loss: 0.0022\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0221 - val_loss: 0.0024\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0229 - val_loss: 0.0013\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0194 - val_loss: 0.0032\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0232 - val_loss: 0.0029\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0226 - val_loss: 0.0030\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0221 - val_loss: 0.0037\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0216 - val_loss: 0.0052\n",
      "Epoch 18/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0034\n",
      "Epoch 19/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0029\n",
      "Epoch 20/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0210 - val_loss: 0.0020\n",
      "Epoch 21/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0196 - val_loss: 0.0029\n",
      "Epoch 22/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0195 - val_loss: 0.0024\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:56:07,419] Trial 23 finished with value: 8.997037993765232 and parameters: {'n_layers': 2, 'n_neurons': 144, 'embudo_1': 1.4164760703993415, 'embudo_2': 1.1083166948922871, 'embudo_3': 1.2231658464380213, 'dropout_threshold': 0.2546532330860817}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.17391304347826086, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20.91304347826087, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02608695652173913, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07391304347826087, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.1217391304347826, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.024347826086956525, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 225 neurons, 1.4921479927812946 embudo_1, 0.9893569627270377 embudo_2, and 1.8824998934015265 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_474 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 1s 3ms/step - loss: 0.0422 - val_loss: 0.0017\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0030\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0048\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0093\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0038\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0062\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0046\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0035\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0061\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.0038\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0040\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:56:15,197] Trial 24 finished with value: 8.997037993765232 and parameters: {'n_layers': 1, 'n_neurons': 225, 'embudo_1': 1.4921479927812946, 'embudo_2': 0.9893569627270377, 'embudo_3': 1.8824998934015265, 'dropout_threshold': 0.3144549174838837}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.16666666666666666, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20.041666666666668, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.024999999999999998, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07083333333333333, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11666666666666665, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.023333333333333334, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 286 neurons, 1.304661603645288 embudo_1, 0.834427917947701 embudo_2, and 2.1733458416830973 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_475 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_476 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 2s 5ms/step - loss: 0.0698 - val_loss: 0.0020\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0270 - val_loss: 0.0020\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0025\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0031\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0031\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0036\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0206 - val_loss: 0.0033\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0031\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0042\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0228 - val_loss: 0.0039\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0030\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0220 - val_loss: 0.0022\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:56:25,569] Trial 25 finished with value: 8.997037993765232 and parameters: {'n_layers': 2, 'n_neurons': 286, 'embudo_1': 1.304661603645288, 'embudo_2': 0.834427917947701, 'embudo_3': 2.1733458416830973, 'dropout_threshold': 0.22371146464685088}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.16, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(19.24, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.024, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.068, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11199999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.022400000000000003, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 349 neurons, 1.369247522100911 embudo_1, 1.3361660452285147 embudo_2, and 1.2134506157019926 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_477 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_478 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_479 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 3s 6ms/step - loss: 0.0806 - val_loss: 0.0030\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0361 - val_loss: 0.0015\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0322 - val_loss: 0.0034\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0302 - val_loss: 0.0038\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0239 - val_loss: 0.0014\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0223 - val_loss: 0.0025\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0226 - val_loss: 0.0026\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0211 - val_loss: 0.0016\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0229 - val_loss: 0.0017\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0226 - val_loss: 0.0026\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0244 - val_loss: 0.0012\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0211 - val_loss: 0.0013\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0217 - val_loss: 0.0046\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0222 - val_loss: 0.0037\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0218 - val_loss: 6.9317e-04\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0213 - val_loss: 0.0045\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0210 - val_loss: 0.0033\n",
      "Epoch 18/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0212 - val_loss: 0.0013\n",
      "Epoch 19/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0206 - val_loss: 0.0024\n",
      "Epoch 20/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0207 - val_loss: 0.0044\n",
      "Epoch 21/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0212 - val_loss: 0.0017\n",
      "Epoch 22/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0216 - val_loss: 0.0040\n",
      "Epoch 23/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0210 - val_loss: 0.0019\n",
      "Epoch 24/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0201 - val_loss: 0.0031\n",
      "Epoch 25/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0205 - val_loss: 0.0046\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:56:46,102] Trial 26 finished with value: 8.997037993765232 and parameters: {'n_layers': 3, 'n_neurons': 349, 'embudo_1': 1.369247522100911, 'embudo_2': 1.3361660452285147, 'embudo_3': 1.2134506157019926, 'dropout_threshold': 0.15106617263220135}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.15384615384615385, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(18.5, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.023076923076923075, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06538461538461539, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10769230769230768, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02153846153846154, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 104 neurons, 1.259616603225762 embudo_1, 1.071239828151142 embudo_2, and 1.9171725160804522 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_480 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 1s 3ms/step - loss: 0.0511 - val_loss: 0.0024\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0020\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0022\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0025\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.0054\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0054\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0028\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0040\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0023\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0021\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0045\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0027\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:56:54,037] Trial 27 finished with value: 8.997037993765232 and parameters: {'n_layers': 1, 'n_neurons': 104, 'embudo_1': 1.259616603225762, 'embudo_2': 1.071239828151142, 'embudo_3': 1.9171725160804522, 'dropout_threshold': 0.25354904455721994}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14814814814814814, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(17.814814814814813, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.022222222222222223, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06296296296296296, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.1037037037037037, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.020740740740740744, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 392 neurons, 1.237652262447626 embudo_1, 0.8073116663286803 embudo_2, and 2.3273785229216766 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_481 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_482 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_483 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_484 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 4s 8ms/step - loss: 0.0892 - val_loss: 0.0022\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.1066 - val_loss: 0.0014\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0436 - val_loss: 0.0017\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0392 - val_loss: 0.0010\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0419 - val_loss: 9.5917e-04\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0317 - val_loss: 6.7239e-04\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0304 - val_loss: 0.0016\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0267 - val_loss: 0.0018\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0322 - val_loss: 0.0013\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0012\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0300 - val_loss: 0.0019\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0224 - val_loss: 8.2362e-04\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0240 - val_loss: 0.0015\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0242 - val_loss: 0.0015\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0236 - val_loss: 0.0020\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0271 - val_loss: 0.0021\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:57:12,137] Trial 28 finished with value: 8.997037993765232 and parameters: {'n_layers': 4, 'n_neurons': 392, 'embudo_1': 1.237652262447626, 'embudo_2': 0.8073116663286803, 'embudo_3': 2.3273785229216766, 'dropout_threshold': 0.420738057061346}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.14285714285714285, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(17.178571428571427, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02142857142857143, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.060714285714285714, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 268 neurons, 1.1419150096647548 embudo_1, 1.195738305535381 embudo_2, and 2.4503770536866045 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_485 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_486 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 2s 5ms/step - loss: 0.0690 - val_loss: 0.0042\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0313 - val_loss: 0.0018\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0027\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0025\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0307 - val_loss: 0.0031\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0022\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0038\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0029\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0025\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0232 - val_loss: 0.0026\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0045\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0251 - val_loss: 0.0028\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:57:22,677] Trial 29 finished with value: 8.997037993765232 and parameters: {'n_layers': 2, 'n_neurons': 268, 'embudo_1': 1.1419150096647548, 'embudo_2': 1.195738305535381, 'embudo_3': 2.4503770536866045, 'dropout_threshold': 0.532159889907929}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.8, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13793103448275862, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(96.2, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(16.586206896551722, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.020689655172413793, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.33999999999999997, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05862068965517241, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5599999999999999, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09655172413793103, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11200000000000002, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.019310344827586208, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 324 neurons, 1.316042715706997 embudo_1, 1.2633117853046618 embudo_2, and 2.2168201434876105 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_487 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_488 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_489 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 3s 6ms/step - loss: 0.0852 - val_loss: 0.0020\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0522 - val_loss: 0.0014\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0356 - val_loss: 0.0013\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0368 - val_loss: 0.0012\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0329 - val_loss: 6.4215e-04\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0302 - val_loss: 0.0020\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0294 - val_loss: 0.0017\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0310 - val_loss: 0.0011\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0265 - val_loss: 0.0015\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0262 - val_loss: 0.0027\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0012\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0275 - val_loss: 0.0016\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0250 - val_loss: 0.0026\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0232 - val_loss: 0.0013\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0203 - val_loss: 0.0015\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:57:37,593] Trial 30 finished with value: 8.997037993765232 and parameters: {'n_layers': 3, 'n_neurons': 324, 'embudo_1': 1.316042715706997, 'embudo_2': 1.2633117853046618, 'embudo_3': 2.2168201434876105, 'dropout_threshold': 0.4027512267092101}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13793103448275862, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(16.586206896551722, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.020689655172413793, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.05862068965517241, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09655172413793103, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.019310344827586208, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 263 neurons, 1.1238956946792875 embudo_1, 0.9907532734149024 embudo_2, and 2.5077376805630105 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_490 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_491 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 2s 5ms/step - loss: 0.0747 - val_loss: 0.0031\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0330 - val_loss: 0.0018\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0024\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0418 - val_loss: 0.0034\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0318 - val_loss: 0.0018\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0244 - val_loss: 0.0018\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0028\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0264 - val_loss: 0.0026\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0025\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0242 - val_loss: 0.0023\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0264 - val_loss: 0.0033\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0030\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0270 - val_loss: 0.0035\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0022\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0020\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:57:49,345] Trial 31 finished with value: 8.997037993765232 and parameters: {'n_layers': 2, 'n_neurons': 263, 'embudo_1': 1.1238956946792875, 'embudo_2': 0.9907532734149024, 'embudo_3': 2.5077376805630105, 'dropout_threshold': 0.5803406353784747}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.13333333333333333, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(16.033333333333335, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.02, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.056666666666666664, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333332, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.018666666666666668, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 244 neurons, 1.2219069914768301 embudo_1, 1.4374632694584992 embudo_2, and 2.297786634245793 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_492 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 2s 3ms/step - loss: 0.0441 - val_loss: 0.0017\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0028\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0033\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0024\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0052\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0212 - val_loss: 0.0020\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0065\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.0047\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0043\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.0038\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0024\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:57:57,893] Trial 32 finished with value: 8.997037993765232 and parameters: {'n_layers': 1, 'n_neurons': 244, 'embudo_1': 1.2219069914768301, 'embudo_2': 1.4374632694584992, 'embudo_3': 2.297786634245793, 'dropout_threshold': 0.6468016570661851}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12903225806451613, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(15.516129032258064, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01935483870967742, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.054838709677419356, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09032258064516129, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01806451612903226, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 220 neurons, 1.1512388947377257 embudo_1, 1.078886851326163 embudo_2, and 2.0404777905112814 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_493 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_494 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 2s 5ms/step - loss: 0.0628 - val_loss: 0.0028\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0030\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0262 - val_loss: 0.0027\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0024\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0206 - val_loss: 0.0037\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0249 - val_loss: 0.0018\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 0.0026\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0228 - val_loss: 0.0017\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0059\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0025\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0226 - val_loss: 0.0039\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0204 - val_loss: 0.0020\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0210 - val_loss: 0.0038\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0197 - val_loss: 0.0015\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0202 - val_loss: 0.0058\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0213 - val_loss: 0.0083\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0203 - val_loss: 0.0031\n",
      "Epoch 18/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0189 - val_loss: 0.0052\n",
      "Epoch 19/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0196 - val_loss: 0.0027\n",
      "Epoch 20/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0208 - val_loss: 0.0051\n",
      "Epoch 21/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0202 - val_loss: 0.0035\n",
      "Epoch 22/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0196 - val_loss: 0.0061\n",
      "Epoch 23/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0180 - val_loss: 0.0053\n",
      "Epoch 24/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0174 - val_loss: 0.0018\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:58:13,094] Trial 33 finished with value: 8.997037993765232 and parameters: {'n_layers': 2, 'n_neurons': 220, 'embudo_1': 1.1512388947377257, 'embudo_2': 1.078886851326163, 'embudo_3': 2.0404777905112814, 'dropout_threshold': 0.10030568361545272}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.125, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(15.03125, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01875, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.053125, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0875, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0175, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 273 neurons, 1.1120167141962616 embudo_1, 1.597580425646229 embudo_2, and 2.7588387168125665 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_495 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 1s 3ms/step - loss: 0.0429 - val_loss: 0.0030\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0040\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0020\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0031\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0048\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0038\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0045\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.0042\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0040\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0034\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.0017\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0035\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0087\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.0065\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0051\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0083\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.0034\n",
      "Epoch 18/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0061\n",
      "Epoch 19/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0067\n",
      "Epoch 20/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0036\n",
      "Epoch 21/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.0042\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:58:24,541] Trial 34 finished with value: 8.997037993765232 and parameters: {'n_layers': 1, 'n_neurons': 273, 'embudo_1': 1.1120167141962616, 'embudo_2': 1.597580425646229, 'embudo_3': 2.7588387168125665, 'dropout_threshold': 0.14703718731628418}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.12121212121212122, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(14.575757575757576, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01818181818181818, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.051515151515151514, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08484848484848484, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01696969696969697, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 179 neurons, 1.1917346389420516 embudo_1, 0.9299453908071336 embudo_2, and 2.980292782612704 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_496 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_497 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 2s 5ms/step - loss: 0.0660 - val_loss: 0.0020\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0016\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0247 - val_loss: 0.0025\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0021\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0260 - val_loss: 0.0025\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0193 - val_loss: 0.0028\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0223 - val_loss: 0.0027\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0194 - val_loss: 0.0038\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0028\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0241 - val_loss: 0.0024\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0025\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0042\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:58:35,196] Trial 35 finished with value: 8.997037993765232 and parameters: {'n_layers': 2, 'n_neurons': 179, 'embudo_1': 1.1917346389420516, 'embudo_2': 0.9299453908071336, 'embudo_3': 2.980292782612704, 'dropout_threshold': 0.20318145504094676}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11764705882352941, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(14.147058823529411, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01764705882352941, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.049999999999999996, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08235294117647059, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01647058823529412, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 249 neurons, 1.0980410088315158 embudo_1, 1.209981784712522 embudo_2, and 1.6702088106540605 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_498 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_499 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_500 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_501 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 4s 9ms/step - loss: 0.0910 - val_loss: 0.0033\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0753 - val_loss: 0.0012\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0713 - val_loss: 0.0010\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0652 - val_loss: 0.0012\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0479 - val_loss: 7.1572e-04\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0428 - val_loss: 0.0014\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0344 - val_loss: 0.0012\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0280 - val_loss: 0.0021\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0333 - val_loss: 0.0019\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0349 - val_loss: 0.0016\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0318 - val_loss: 0.0022\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0260 - val_loss: 0.0016\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0305 - val_loss: 0.0037\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0245 - val_loss: 0.0029\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0255 - val_loss: 0.0011\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:58:52,891] Trial 36 finished with value: 8.997037993765232 and parameters: {'n_layers': 4, 'n_neurons': 249, 'embudo_1': 1.0980410088315158, 'embudo_2': 1.209981784712522, 'embudo_3': 1.6702088106540605, 'dropout_threshold': 0.5269679619011612}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.11428571428571428, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(13.742857142857142, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.017142857142857144, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04857142857142857, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.016, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 171 neurons, 1.064976560137089 embudo_1, 1.347686487159153 embudo_2, and 3.3859698092580968 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_502 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_503 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_504 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 3s 7ms/step - loss: 0.0909 - val_loss: 0.0053\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0764 - val_loss: 0.0036\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0504 - val_loss: 8.0505e-04\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0376 - val_loss: 9.9865e-04\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0410 - val_loss: 0.0015\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0020\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0019\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0012\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0266 - val_loss: 0.0022\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0320 - val_loss: 0.0018\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0235 - val_loss: 0.0025\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0295 - val_loss: 0.0016\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0257 - val_loss: 0.0017\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:59:05,927] Trial 37 finished with value: 8.997037993765232 and parameters: {'n_layers': 3, 'n_neurons': 171, 'embudo_1': 1.064976560137089, 'embudo_2': 1.347686487159153, 'embudo_3': 3.3859698092580968, 'dropout_threshold': 0.38200100914728063}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.1111111111111111, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(13.36111111111111, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.016666666666666666, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04722222222222222, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07777777777777778, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015555555555555557, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 326 neurons, 1.2885422566766505 embudo_1, 1.4884560702483536 embudo_2, and 2.7985863265041173 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_505 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_506 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 2s 5ms/step - loss: 0.0532 - val_loss: 0.0050\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0024\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0021\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0244 - val_loss: 0.0024\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0240 - val_loss: 0.0023\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0234 - val_loss: 0.0021\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0219 - val_loss: 0.0027\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0215 - val_loss: 0.0022\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0243 - val_loss: 0.0021\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0211 - val_loss: 0.0028\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0215 - val_loss: 0.0048\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0235 - val_loss: 0.0020\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0204 - val_loss: 0.0028\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0228 - val_loss: 0.0057\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0207 - val_loss: 0.0043\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0221 - val_loss: 0.0026\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.0027\n",
      "Epoch 18/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0224 - val_loss: 0.0042\n",
      "Epoch 19/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0215 - val_loss: 0.0030\n",
      "Epoch 20/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0205 - val_loss: 0.0021\n",
      "Epoch 21/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0216 - val_loss: 0.0029\n",
      "Epoch 22/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0199 - val_loss: 0.0054\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:59:21,453] Trial 38 finished with value: 8.997037993765232 and parameters: {'n_layers': 2, 'n_neurons': 326, 'embudo_1': 1.2885422566766505, 'embudo_2': 1.4884560702483536, 'embudo_3': 2.7985863265041173, 'dropout_threshold': 0.16771785644928477}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10810810810810811, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(13.0, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.016216216216216217, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04594594594594594, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07567567567567567, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015135135135135137, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 213 neurons, 1.2146275902654804 embudo_1, 0.8905708251516272 embudo_2, and 2.111009552769817 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_507 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 1s 3ms/step - loss: 0.0411 - val_loss: 0.0043\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0031\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0057\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0027\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0050\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0044\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0030\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0071\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0101\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.0026\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0051\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0041\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0038\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0014\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0057\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0061\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0076\n",
      "Epoch 18/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0056\n",
      "Epoch 19/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0041\n",
      "Epoch 20/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0044\n",
      "Epoch 21/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0019\n",
      "Epoch 22/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0017\n",
      "Epoch 23/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0046\n",
      "Epoch 24/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0047\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:59:32,483] Trial 39 finished with value: 8.997037993765232 and parameters: {'n_layers': 1, 'n_neurons': 213, 'embudo_1': 1.2146275902654804, 'embudo_2': 0.8905708251516272, 'embudo_3': 2.111009552769817, 'dropout_threshold': 0.49362873208548513}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.6666666666666666, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10526315789473684, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(80.16666666666667, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.657894736842104, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09999999999999999, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015789473684210527, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.2833333333333333, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04473684210526316, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.4666666666666666, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07368421052631578, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09333333333333334, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01473684210526316, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 377 neurons, 1.1684631773453726 embudo_1, 1.077342225644979 embudo_2, and 2.5609340932091715 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_508 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_509 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_510 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_511 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 4s 11ms/step - loss: 0.0902 - val_loss: 0.0023\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0749 - val_loss: 0.0012\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0704 - val_loss: 0.0013\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0643 - val_loss: 0.0028\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0435 - val_loss: 0.0019\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0387 - val_loss: 8.6107e-04\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0363 - val_loss: 6.4101e-04\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0481 - val_loss: 0.0017\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0294 - val_loss: 0.0018\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0365 - val_loss: 0.0023\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0302 - val_loss: 0.0019\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0044\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0287 - val_loss: 0.0016\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0447 - val_loss: 0.0025\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0308 - val_loss: 0.0024\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0246 - val_loss: 0.0033\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0255 - val_loss: 0.0020\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 22:59:51,933] Trial 40 finished with value: 8.997037993765232 and parameters: {'n_layers': 4, 'n_neurons': 377, 'embudo_1': 1.1684631773453726, 'embudo_2': 1.077342225644979, 'embudo_3': 2.5609340932091715, 'dropout_threshold': 0.5664241915280287}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10526315789473684, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.657894736842104, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015789473684210527, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04473684210526316, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07368421052631578, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01473684210526316, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 245 neurons, 1.0497601976519895 embudo_1, 2.230437593012019 embudo_2, and 3.3902294799951735 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_512 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_513 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 2s 5ms/step - loss: 0.0783 - val_loss: 0.0041\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0374 - val_loss: 0.0021\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0025\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0317 - val_loss: 0.0024\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0016\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0027\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0260 - val_loss: 0.0018\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0022\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0020\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0238 - val_loss: 0.0034\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0026\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0277 - val_loss: 0.0015\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0247 - val_loss: 0.0016\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0258 - val_loss: 0.0022\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0236 - val_loss: 0.0024\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0231 - val_loss: 0.0021\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.0033\n",
      "Epoch 18/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0251 - val_loss: 0.0024\n",
      "Epoch 19/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0020\n",
      "Epoch 20/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0023\n",
      "Epoch 21/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0225 - val_loss: 0.0036\n",
      "Epoch 22/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0202 - val_loss: 0.0017\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 23:00:06,316] Trial 41 finished with value: 8.997037993765232 and parameters: {'n_layers': 2, 'n_neurons': 245, 'embudo_1': 1.0497601976519895, 'embudo_2': 2.230437593012019, 'embudo_3': 3.3902294799951735, 'dropout_threshold': 0.43644190717672826}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.10256410256410256, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.333333333333334, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015384615384615384, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04358974358974359, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.07179487179487179, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.01435897435897436, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 300 neurons, 1.0899396551676108 embudo_1, 2.1029222316192664 embudo_2, and 3.2894591191819824 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_514 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_515 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 2s 5ms/step - loss: 0.0744 - val_loss: 0.0047\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0391 - val_loss: 0.0033\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0024\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0025\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0320 - val_loss: 0.0028\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0025\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0023\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0021\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0024\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0217 - val_loss: 0.0018\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0243 - val_loss: 0.0025\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0025\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0237 - val_loss: 0.0023\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0029\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0020\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0242 - val_loss: 0.0019\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0033\n",
      "Epoch 18/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0234 - val_loss: 0.0023\n",
      "Epoch 19/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0023\n",
      "Epoch 20/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0221 - val_loss: 0.0024\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 23:00:20,595] Trial 42 finished with value: 8.997037993765232 and parameters: {'n_layers': 2, 'n_neurons': 300, 'embudo_1': 1.0899396551676108, 'embudo_2': 2.1029222316192664, 'embudo_3': 3.2894591191819824, 'dropout_threshold': 0.4807898920984456}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.1, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(12.025, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.015, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.042499999999999996, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06999999999999999, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.014000000000000002, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 275 neurons, 1.0439841623326749 embudo_1, 1.8421270431454597 embudo_2, and 2.891927091219577 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_516 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_517 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 2s 5ms/step - loss: 0.0761 - val_loss: 0.0022\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0282 - val_loss: 0.0035\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0037\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0017\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0235 - val_loss: 0.0018\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0021\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 0.0030\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0019\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0185 - val_loss: 0.0024\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0040\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 0.0022\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0038\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0230 - val_loss: 0.0018\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0066\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 23:00:31,778] Trial 43 finished with value: 8.997037993765232 and parameters: {'n_layers': 2, 'n_neurons': 275, 'embudo_1': 1.0439841623326749, 'embudo_2': 1.8421270431454597, 'embudo_3': 2.891927091219577, 'dropout_threshold': 0.22367996998737127}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0975609756097561, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(11.731707317073171, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.014634146341463414, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.041463414634146344, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06829268292682926, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013658536585365855, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 436 neurons, 1.129730431108973 embudo_1, 1.6147122879767097 embudo_2, and 3.008162271267908 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_518 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_519 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_520 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 3s 6ms/step - loss: 0.0881 - val_loss: 0.0033\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0751 - val_loss: 0.0014\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0011\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0617 - val_loss: 7.1930e-04\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0612 - val_loss: 7.3407e-04\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0397 - val_loss: 0.0011\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0301 - val_loss: 0.0042\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0521 - val_loss: 0.0011\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0360 - val_loss: 0.0019\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0270 - val_loss: 0.0030\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0325 - val_loss: 0.0045\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0312 - val_loss: 9.6135e-04\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0300 - val_loss: 0.0034\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0012\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 23:00:46,099] Trial 44 finished with value: 8.997037993765232 and parameters: {'n_layers': 3, 'n_neurons': 436, 'embudo_1': 1.129730431108973, 'embudo_2': 1.6147122879767097, 'embudo_3': 3.008162271267908, 'dropout_threshold': 0.6189439779325311}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09523809523809523, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(11.452380952380953, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.014285714285714285, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.04047619047619048, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06666666666666667, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013333333333333334, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 3 layers, 319 neurons, 1.1080209595022183 embudo_1, 2.050441313152866 embudo_2, and 3.497420403997127 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_521 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_522 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_523 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 3s 6ms/step - loss: 0.0884 - val_loss: 0.0036\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0037\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0341 - val_loss: 5.9007e-04\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0413 - val_loss: 9.6302e-04\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 0.0358 - val_loss: 0.0025\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0337 - val_loss: 0.0012\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0298 - val_loss: 0.0017\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0262 - val_loss: 7.7867e-04\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0295 - val_loss: 0.0017\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0267 - val_loss: 8.2911e-04\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0243 - val_loss: 0.0027\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0267 - val_loss: 0.0031\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0257 - val_loss: 0.0026\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 23:00:59,373] Trial 45 finished with value: 8.997037993765232 and parameters: {'n_layers': 3, 'n_neurons': 319, 'embudo_1': 1.1080209595022183, 'embudo_2': 2.050441313152866, 'embudo_3': 3.497420403997127, 'dropout_threshold': 0.45336064711610624}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09302325581395349, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(11.186046511627907, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013953488372093023, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03953488372093023, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06511627906976744, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013023255813953489, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 289 neurons, 1.1599912124186267 embudo_1, 2.3197005936297828 embudo_2, and 2.730415600791786 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_524 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_525 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_526 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_527 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 4s 8ms/step - loss: 0.0883 - val_loss: 0.0019\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0664 - val_loss: 0.0039\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0416 - val_loss: 0.0018\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0309 - val_loss: 0.0018\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0331 - val_loss: 0.0026\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0301 - val_loss: 0.0021\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0289 - val_loss: 0.0031\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0337 - val_loss: 0.0027\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0242 - val_loss: 0.0021\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0260 - val_loss: 0.0036\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0017\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0243 - val_loss: 0.0014\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0243 - val_loss: 0.0015\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0235 - val_loss: 0.0016\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0032\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0214 - val_loss: 0.0015\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0022\n",
      "Epoch 18/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0215 - val_loss: 0.0020\n",
      "Epoch 19/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0210 - val_loss: 9.1422e-04\n",
      "Epoch 20/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0226 - val_loss: 0.0018\n",
      "Epoch 21/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0194 - val_loss: 0.0024\n",
      "Epoch 22/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0225 - val_loss: 0.0011\n",
      "Epoch 23/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0235 - val_loss: 0.0020\n",
      "Epoch 24/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0201 - val_loss: 0.0016\n",
      "Epoch 25/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0212 - val_loss: 0.0037\n",
      "Epoch 26/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0219 - val_loss: 0.0045\n",
      "Epoch 27/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0187 - val_loss: 0.0012\n",
      "Epoch 28/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0200 - val_loss: 0.0028\n",
      "Epoch 29/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0194 - val_loss: 0.0022\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 23:01:25,621] Trial 46 finished with value: 8.997037993765232 and parameters: {'n_layers': 4, 'n_neurons': 289, 'embudo_1': 1.1599912124186267, 'embudo_2': 2.3197005936297828, 'embudo_3': 2.730415600791786, 'dropout_threshold': 0.29524684712664223}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.09090909090909091, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(10.931818181818182, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013636363636363636, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.038636363636363635, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06363636363636363, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.012727272727272728, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 4 layers, 364 neurons, 1.200430400535421 embudo_1, 1.912813943671684 embudo_2, and 3.0949579798611007 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_528 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_529 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_530 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_531 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 3s 8ms/step - loss: 0.0874 - val_loss: 0.0018\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0535 - val_loss: 0.0027\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0468 - val_loss: 0.0014\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0360 - val_loss: 0.0033\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 8.2220e-04\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0260 - val_loss: 0.0027\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0261 - val_loss: 0.0021\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0304 - val_loss: 0.0020\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0277 - val_loss: 0.0012\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0273 - val_loss: 0.0024\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0227 - val_loss: 0.0021\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 0.0243 - val_loss: 0.0039\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0203 - val_loss: 0.0028\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0247 - val_loss: 9.8938e-04\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0029\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 23:01:42,371] Trial 47 finished with value: 8.997037993765232 and parameters: {'n_layers': 4, 'n_neurons': 364, 'embudo_1': 1.200430400535421, 'embudo_2': 1.912813943671684, 'embudo_3': 3.0949579798611007, 'dropout_threshold': 0.26937374330810554}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08888888888888889, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(10.688888888888888, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013333333333333332, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03777777777777778, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.06222222222222222, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.012444444444444445, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 1 layers, 169 neurons, 1.0899674133374442 embudo_1, 1.7074846925725717 embudo_2, and 2.4020483633958634 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_532 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0417 - val_loss: 0.0023\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0053\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0049\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0036\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0034\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0205 - val_loss: 0.0040\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0022\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0056\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0086\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0202 - val_loss: 0.0069\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0196 - val_loss: 0.0084\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0056\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0049\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0059\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0096\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0050\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0041\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n",
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 23:01:52,487] Trial 48 finished with value: 8.997037993765232 and parameters: {'n_layers': 1, 'n_neurons': 169, 'embudo_1': 1.0899674133374442, 'embudo_2': 1.7074846925725717, 'embudo_3': 2.4020483633958634, 'dropout_threshold': 0.4973273819915169}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.5714285714285714, 4.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08695652173913043, 4.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(1, 4)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(68.71428571428571, 481.0)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(10.456521739130435, 481.0)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'decimals': 0, 'out': None}\n",
      "()\n",
      "{'out': None}\n",
      "(20, 500)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08571428571428572, 0.6)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.013043478260869565, 0.6)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.24285714285714285, 1.7)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.03695652173913044, 1.7)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.39999999999999997, 2.8)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.0608695652173913, 2.8)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.08, 0.56)\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'out': None}\n",
      "(0.012173913043478262, 0.56)\n",
      "{'axis': None, 'out': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "Training model with 2 layers, 210 neurons, 1.0080702960222416 embudo_1, 2.494871233456238 embudo_2, and 3.285956960528645 embudo_3.\n",
      "Número de features: 29\n",
      "WARNING:tensorflow:Layer lstm_533 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_534 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "114/114 [==============================] - 2s 5ms/step - loss: 0.0781 - val_loss: 0.0056\n",
      "Epoch 2/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0274 - val_loss: 0.0056\n",
      "Epoch 3/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0314 - val_loss: 0.0024\n",
      "Epoch 4/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0019\n",
      "Epoch 5/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0022\n",
      "Epoch 6/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 0.0016\n",
      "Epoch 7/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0033\n",
      "Epoch 8/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0274 - val_loss: 0.0016\n",
      "Epoch 9/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0012\n",
      "Epoch 10/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0235 - val_loss: 0.0017\n",
      "Epoch 11/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0276 - val_loss: 0.0017\n",
      "Epoch 12/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0023\n",
      "Epoch 13/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0216 - val_loss: 0.0032\n",
      "Epoch 14/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0019\n",
      "Epoch 15/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 0.0020\n",
      "Epoch 16/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0021\n",
      "Epoch 17/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0235 - val_loss: 0.0018\n",
      "Epoch 18/200\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.0187 - val_loss: 0.0019\n",
      "Epoch 19/200\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.0221 - val_loss: 0.0030\n",
      "Predicting 201904 for product -1.4709809755485788 (1/158))\n",
      "Predicting 201904 for product -1.4549869992327784 (2/158))\n",
      "Predicting 201904 for product -1.4389930229169778 (3/158))\n",
      "Predicting 201904 for product -1.2017490408992708 (4/158))\n",
      "Predicting 201904 for product -1.137773135636069 (5/158))\n",
      "Predicting 201904 for product -1.108450845723768 (6/158))\n",
      "Predicting 201904 for product -1.0737972303728671 (7/158))\n",
      "Predicting 201904 for product -1.0258153014254656 (8/158))\n",
      "Predicting 201904 for product -0.9271857808113628 (9/158))\n",
      "Predicting 201904 for product -0.9191887926534625 (10/158))\n",
      "Predicting 201904 for product -0.8445502365130604 (11/158))\n",
      "Predicting 201904 for product -0.799233970284959 (12/158))\n",
      "Predicting 201904 for product -0.6712821597585553 (13/158))\n",
      "Predicting 201904 for product -0.6233002308111539 (14/158))\n",
      "Predicting 201904 for product -0.6179689053725538 (15/158))\n",
      "Predicting 201904 for product -0.6073062544953535 (16/158))\n",
      "Predicting 201904 for product -0.5913122781795529 (17/158))\n",
      "Predicting 201904 for product -0.5886466154602529 (18/158))\n",
      "Predicting 201904 for product -0.5353333610742513 (19/158))\n",
      "Predicting 201904 for product -0.5193393847584509 (20/158))\n",
      "Predicting 201904 for product -0.42604118958294807 (21/158))\n",
      "Predicting 201904 for product -0.40471588782854745 (22/158))\n",
      "Predicting 201904 for product -0.3753935979162466 (23/158))\n",
      "Predicting 201904 for product -0.2661014264249434 (24/158))\n",
      "Predicting 201904 for product -0.2367791365126425 (25/158))\n",
      "Predicting 201904 for product -0.22345082291614213 (26/158))\n",
      "Predicting 201904 for product -0.18346588212664094 (27/158))\n",
      "Predicting 201904 for product -0.17013756853014056 (28/158))\n",
      "Predicting 201904 for product -0.15680925493364017 (29/158))\n",
      "Predicting 201904 for product -0.1408152786178397 (30/158))\n",
      "Predicting 201904 for product -0.1301526277406394 (31/158))\n",
      "Predicting 201904 for product -0.09816467510903845 (32/158))\n",
      "Predicting 201904 for product -0.09016768695113822 (33/158))\n",
      "Predicting 201904 for product -0.08217069879323798 (34/158))\n",
      "Predicting 201904 for product -0.042185758003736806 (35/158))\n",
      "Predicting 201904 for product -0.03685443256513665 (36/158))\n",
      "Predicting 201904 for product -0.0181947935300361 (37/158))\n",
      "Predicting 201904 for product 0.03511846085596547 (38/158))\n",
      "Predicting 201904 for product 0.04044978629456562 (39/158))\n",
      "Predicting 201904 for product 0.06710641348756641 (40/158))\n",
      "Predicting 201904 for product 0.07510340164546664 (41/158))\n",
      "Predicting 201904 for product 0.09376304068056719 (42/158))\n",
      "Predicting 201904 for product 0.10709135427706758 (43/158))\n",
      "Predicting 201904 for product 0.14441063234726867 (44/158))\n",
      "Predicting 201904 for product 0.1524076205051689 (45/158))\n",
      "Predicting 201904 for product 0.1790642476981697 (46/158))\n",
      "Predicting 201904 for product 0.2297118393648712 (47/158))\n",
      "Predicting 201904 for product 0.23504316480347134 (48/158))\n",
      "Predicting 201904 for product 0.30701605822457345 (49/158))\n",
      "Predicting 201904 for product 0.31767870910177376 (50/158))\n",
      "Predicting 201904 for product 0.32834135997897407 (51/158))\n",
      "Predicting 201904 for product 0.4216395551544768 (52/158))\n",
      "Predicting 201904 for product 0.4349678687509772 (53/158))\n",
      "Predicting 201904 for product 0.44296485690887744 (54/158))\n",
      "Predicting 201904 for product 0.45096184506677767 (55/158))\n",
      "Predicting 201904 for product 0.4776184722597785 (56/158))\n",
      "Predicting 201904 for product 0.4882811231369788 (57/158))\n",
      "Predicting 201904 for product 0.5176034130492796 (58/158))\n",
      "Predicting 201904 for product 0.5362630520843802 (59/158))\n",
      "Predicting 201904 for product 0.5495913656808806 (60/158))\n",
      "Predicting 201904 for product 0.5549226911194808 (61/158))\n",
      "Predicting 201904 for product 0.562919679277381 (62/158))\n",
      "Predicting 201904 for product 0.5762479928738814 (63/158))\n",
      "Predicting 201904 for product 0.5815793183124816 (64/158))\n",
      "Predicting 201904 for product 0.5842449810317816 (65/158))\n",
      "Predicting 201904 for product 0.5922419691896819 (66/158))\n",
      "Predicting 201904 for product 0.5949076319089819 (67/158))\n",
      "Predicting 201904 for product 0.6029046200668822 (68/158))\n",
      "Predicting 201904 for product 0.6135672709440825 (69/158))\n",
      "Predicting 201904 for product 0.6162329336633825 (70/158))\n",
      "Predicting 201904 for product 0.6402238981370832 (71/158))\n",
      "Predicting 201904 for product 0.6428895608563834 (72/158))\n",
      "Predicting 201904 for product 0.6588835371721838 (73/158))\n",
      "Predicting 201904 for product 0.6908714898037848 (74/158))\n",
      "Predicting 201904 for product 0.7468504069090864 (75/158))\n",
      "Predicting 201904 for product 0.7681757086634871 (76/158))\n",
      "Predicting 201904 for product 0.7868353476985875 (77/158))\n",
      "Predicting 201904 for product 0.802829324014388 (78/158))\n",
      "Predicting 201904 for product 0.8108263121722883 (79/158))\n",
      "Predicting 201904 for product 0.8134919748915883 (80/158))\n",
      "Predicting 201904 for product 0.8748022174354901 (81/158))\n",
      "Predicting 201904 for product 0.9014588446284909 (82/158))\n",
      "Predicting 201904 for product 0.9094558327863912 (83/158))\n",
      "Predicting 201904 for product 0.938778122698692 (84/158))\n",
      "Predicting 201904 for product 0.9574377617337926 (85/158))\n",
      "Predicting 201904 for product 0.973431738049593 (86/158))\n",
      "Predicting 201904 for product 1.0134166788390941 (87/158))\n",
      "Predicting 201904 for product 1.0560672823478954 (88/158))\n",
      "Predicting 201904 for product 1.069395595944396 (89/158))\n",
      "Predicting 201904 for product 1.0907208976987965 (90/158))\n",
      "Predicting 201904 for product 1.1093805367338971 (91/158))\n",
      "Predicting 201904 for product 1.1120461994531972 (92/158))\n",
      "Predicting 201904 for product 1.1147118621724972 (93/158))\n",
      "Predicting 201904 for product 1.1173775248917972 (94/158))\n",
      "Predicting 201904 for product 1.1360371639268978 (95/158))\n",
      "Predicting 201904 for product 1.1493654775233981 (96/158))\n",
      "Predicting 201904 for product 1.1520311402426984 (97/158))\n",
      "Predicting 201904 for product 1.1600281284005984 (98/158))\n",
      "Predicting 201904 for product 1.176022104716399 (99/158))\n",
      "Predicting 201904 for product 1.1840190928742993 (100/158))\n",
      "Predicting 201904 for product 1.1866847555935993 (101/158))\n",
      "Predicting 201904 for product 1.1893504183128993 (102/158))\n",
      "Predicting 201904 for product 1.1946817437514996 (103/158))\n",
      "Predicting 201904 for product 1.2000130691900996 (104/158))\n",
      "Predicting 201904 for product 1.2346666845410008 (105/158))\n",
      "Predicting 201904 for product 1.2533263235761012 (106/158))\n",
      "Predicting 201904 for product 1.314636566120003 (107/158))\n",
      "Predicting 201904 for product 1.3279648797165036 (108/158))\n",
      "Predicting 201904 for product 1.3306305424358036 (109/158))\n",
      "Predicting 201904 for product 1.3546215069095042 (110/158))\n",
      "Predicting 201904 for product 1.3599528323481045 (111/158))\n",
      "Predicting 201904 for product 1.3652841577867045 (112/158))\n",
      "Predicting 201904 for product 1.3732811459446048 (113/158))\n",
      "Predicting 201904 for product 1.3866094595411051 (114/158))\n",
      "Predicting 201904 for product 1.3919407849797054 (115/158))\n",
      "Predicting 201904 for product 1.4079347612955058 (116/158))\n",
      "Predicting 201904 for product 1.415931749453406 (117/158))\n",
      "Predicting 201904 for product 1.418597412172706 (118/158))\n",
      "Predicting 201904 for product 1.4265944003306064 (119/158))\n",
      "Predicting 201904 for product 1.4372570512078067 (120/158))\n",
      "Predicting 201904 for product 1.445254039365707 (121/158))\n",
      "Predicting 201904 for product 1.447919702085007 (122/158))\n",
      "Predicting 201904 for product 1.4612480156815073 (123/158))\n",
      "Predicting 201904 for product 1.4639136784008075 (124/158))\n",
      "Predicting 201904 for product 1.4665793411201076 (125/158))\n",
      "Predicting 201904 for product 1.4719106665587076 (126/158))\n",
      "Predicting 201904 for product 1.4745763292780079 (127/158))\n",
      "Predicting 201904 for product 1.4879046428745082 (128/158))\n",
      "Predicting 201904 for product 1.4932359683131082 (129/158))\n",
      "Predicting 201904 for product 1.5012329564710085 (130/158))\n",
      "Predicting 201904 for product 1.5065642819096088 (131/158))\n",
      "Predicting 201904 for product 1.5145612700675088 (132/158))\n",
      "Predicting 201904 for product 1.5252239209447092 (133/158))\n",
      "Predicting 201904 for product 1.5278895836640094 (134/158))\n",
      "Predicting 201904 for product 1.5305552463833094 (135/158))\n",
      "Predicting 201904 for product 1.5332209091026094 (136/158))\n",
      "Predicting 201904 for product 1.54654922269911 (137/158))\n",
      "Predicting 201904 for product 1.55188054813771 (138/158))\n",
      "Predicting 201904 for product 1.5678745244535106 (139/158))\n",
      "Predicting 201904 for product 1.5732058498921107 (140/158))\n",
      "Predicting 201904 for product 1.5758715126114107 (141/158))\n",
      "Predicting 201904 for product 1.5971968143658113 (142/158))\n",
      "Predicting 201904 for product 1.6078594652430116 (143/158))\n",
      "Predicting 201904 for product 1.6131907906816119 (144/158))\n",
      "Predicting 201904 for product 1.618522116120212 (145/158))\n",
      "Predicting 201904 for product 1.6265191042781122 (146/158))\n",
      "Predicting 201904 for product 1.6478444060325128 (147/158))\n",
      "Predicting 201904 for product 1.6585070569097131 (148/158))\n",
      "Predicting 201904 for product 1.6611727196290134 (149/158))\n",
      "Predicting 201904 for product 1.6638383823483134 (150/158))\n",
      "Predicting 201904 for product 1.6665040450676134 (151/158))\n",
      "Predicting 201904 for product 1.6718353705062137 (152/158))\n",
      "Predicting 201904 for product 1.685163684102714 (153/158))\n",
      "Predicting 201904 for product 1.6931606722606143 (154/158))\n",
      "Predicting 201904 for product 1.6958263349799143 (155/158))\n",
      "Predicting 201904 for product 1.6984919976992143 (156/158))\n",
      "Predicting 201904 for product 1.719817299453615 (157/158))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-25 23:02:05,783] Trial 49 finished with value: 8.997037993765232 and parameters: {'n_layers': 2, 'n_neurons': 210, 'embudo_1': 1.0080702960222416, 'embudo_2': 2.494871233456238, 'embudo_3': 3.285956960528645, 'dropout_threshold': 0.352090714195068}. Best is trial 0 with value: 8.997037993765232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 201904 for product 1.7251486248922152 (158/158))\n",
      "{}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': -1, 'kind': None, 'order': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "{'axis': None, 'dtype': None, 'out': None}\n",
      "()\n",
      "Best trial for cluster1.5555569940213847:\n",
      "n_layers: 4\n",
      "n_neurons: 308\n",
      "embudo_1: 1.3013974395521934\n",
      "embudo_2: 2.210097618703375\n",
      "embudo_3: 3.0174157779070008\n",
      "dropout_threshold: 0.20751666592540416\n",
      "Finished training for cluster 1.5555569940213847\n"
     ]
    }
   ],
   "source": [
    "for cluster in train_df[\"cluster\"].unique()[1:]:\n",
    "    print(f\"Training for cluster {cluster}\")\n",
    "    bo(cluster)\n",
    "    print(f\"Finished training for cluster {cluster}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
